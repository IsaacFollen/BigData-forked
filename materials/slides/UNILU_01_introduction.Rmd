---
title: "Big Data Analytics"
subtitle: "Lecture 1: Introduction, Big Data Econometrics"
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../../style/ioslides_unilu.css
    template: ../../style/nologo_template.html
logo: ../img/logo_unilu.png
bibliography: ../references/bigdata.bib
---

```{r set-options, echo=FALSE, cache=FALSE, purl=FALSE}
options(width = 100)
library(knitr)
```

# Big Data?

------

```{r whatis, echo=FALSE, out.width = "50%", fig.align='center', purl=FALSE}
include_graphics("../img/01_what_is.jpg")
```



-----

```{r intro, echo=FALSE, out.width = "60%", fig.align='center', purl=FALSE, fig.cap="Posted on flickr by BBVAtech in 2012, by Asigra [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)"}
include_graphics("../img/01_big_data_intro.jpg")
```


## Expert Survey (UC Berkeley, 2014)

- Ask 40 experts to define *"big data"*...


## Expert Survey (UC Berkeley, 2014)

- Ask 40 experts to define *"big data"*...
- ... get 40 different definitions :)


## Expert Survey (UC Berkeley, 2014)

```{r wordcloud, echo=FALSE, out.width = "90%", fig.align='center', purl=FALSE, fig.cap="Image by Jennifer Dutcher, datascience@berkeley, source: https://datascience.berkeley.edu/what-is-big-data/"}
include_graphics("../img/01_survey_wordcloud.png")
```


## Expert Survey: Example 1

"Big Data is the result of *collecting information at its most granular level* — it’s what you get when you instrument a system and keep all of the data that your instrumentation is able to gather."

*Jon Bruner*
(Editor-at-Large, O’Reilly Media)


## Expert Survey: Example 2

"Big data is data that contains enough observations to *demand unusual handling because of its sheer size*, though what is unusual changes over time and varies from one discipline to another."

*Annette Greiner*<br>
(Lecturer, UC Berkeley School of Information)


## Expert Survey: Example 3

"[...] 'big data' will ultimately describe any data set large enough to necessitate *high-level programming skill* and *statistically defensible methodologies* in order to transform the data asset into something of value."

*Reid Bryant*<br>
(Data Scientist, Brooks Bell)


## Conclusion

```{r hard, echo=FALSE, out.width = "60%", fig.align='center', purl=FALSE}
include_graphics("../img/01_its_hard.jpeg")
```


## Conclusion

- Large amounts of data (big *N*: many obs.; big *P*: many vars.)
- Various types/formats of data
- Unusual sources
- Speed of data flow/stream
- Use programming and statistics (in a broad sense) to extract value



## 'Learn Big Data'?

```{r landscape, echo=FALSE, out.width = "90%", fig.align='center', purl=FALSE, fig.cap="'Big Data Landscape (2019)', source: http://mattturck.com"}
include_graphics("https://mattturck.com/wp-content/uploads/2019/06/2019_Matt_Turck_Big_Data_Landscape_2019_Thumb.png")
```


## Domains Affected
- How to design/set-up the machinery to handle large amounts of data? (Hardware focus, data engineering)
- How to use the existing machinery most efficiently for large amounts of data?
- How to approach the analysis of large amounts of data with econometrics?


## Focus in This Course 
- How to design/set-up the machinery to handle large amounts of data? (Hardware focus, data engineering)
- *How to use the existing machinery most efficiently for large amounts of data?*
- *How to approach the analysis of large amounts of data with econometrics?*


## Focus in This Course 
- How to design/set-up the machinery to handle large amounts of data? (Hardware focus, data engineering)
- *How to use the existing machinery most efficiently for large amounts of data?*
- *How to approach the analysis of large amounts of data with econometrics?*
    1. Compute 'usual' statistics based on large data set (many observations).


## Focus in This Course 
- How to design/set-up the machinery to handle large amounts of data? (Hardware focus, data engineering)
- *How to use the existing machinery most efficiently for large amounts of data?*
- *How to approach the analysis of large amounts of data with econometrics?*
    1. Compute 'usual' statistics based on large data set (many observations, many variables).
    2. Practical handling of large data sets for applied econometrics (gathering, storage, preparation, etc.)



<!-- # Big Data in Scientific Research -->

<!-- ## Big Data in the Sciences -->
<!-- - Mother nature always has provided the data, but... -->
<!--      - ... instruments have gotten more precise -->
<!--      - ... new measurement methods have been developed  -->
<!-- - Prominent examples: Astronomy, Genomics/Bioinformatics -->

<!-- ```{r nightsky, echo=FALSE, out.width = "45%", fig.align='center', purl=FALSE, fig.cap="Photo by Joe Parks, [(CC BY-NC 2.0)](https://creativecommons.org/licenses/by-nc/2.0/) source: https://flic.kr/p/e2umhv"} -->
<!-- include_graphics("../img/01_nightsky.jpg") -->
<!-- ``` -->



<!-- ## Big Data in the Sciences -->
<!-- *Astronomy*: SKA Radio Telescope -->

<!-- ```{r ska1, echo=FALSE, out.width = "60%", fig.align='center', purl=FALSE, fig.cap="Image by SKA Organisation, source: https://www.skatelescope.org/multimedia/image"} -->
<!-- include_graphics("../img/01_ska_closeup.jpg") -->
<!-- ``` -->

<!-- ## Big Data in the Sciences -->
<!-- *Astronomy*: SKA Radio Telescope -->

<!-- ```{r ska2, echo=FALSE, out.width = "60%", fig.align='center', purl=FALSE, fig.cap="Image by SKY Organisation, source: https://www.skatelescope.org/multimedia/image"} -->
<!-- include_graphics("../img/01_ska_data.jpg") -->
<!-- ``` -->


<!-- ## Big Data in the Social Sciences -->

<!-- ```{r prog_web, echo=FALSE, out.width = "90%", fig.align='center', purl=FALSE} -->
<!-- include_graphics("../img/01_prog_web.png") -->
<!-- ``` -->


<!-- ## Big Data in the Social Sciences -->

<!-- - *Hardware*: Diffusion of the Internet and mobile-phone networks. -->
<!-- - *Software*: Web 2.0 Technologies (APIs, JSON, Programmable Web, etc.). -->


<!-- ## Big Data in the Social Sciences -->

<!-- - *Hardware*: Diffusion of the Internet and mobile-phone networks. -->
<!-- - *Software*: Web 2.0 Technologies (APIs, JSON, Programmable Web, etc.). -->
<!--      - Backbone of social media and many prominent web services (e.g., Google Maps). -->
<!--      - Data integration across platforms and services. -->
<!--      - Exchange of data between/across applications. -->

<!-- ## Big Data in the Social Sciences/Economics -->

<!-- ```{r twitter1, echo=FALSE, out.width = "80%", fig.align='center', purl=FALSE, fig.cap='Source: @bollen_etal2011'} -->
<!-- include_graphics("../img/01_twitter_mood.png") -->
<!-- ``` -->

<!-- ## Big Data in the Social Sciences/Economics -->

<!-- ```{r twitter2, echo=FALSE, out.width = "70%", fig.align='center', purl=FALSE, fig.cap='Source: @bollen_etal2011'} -->
<!-- include_graphics("../img/01_twitter_mood2.png") -->
<!-- ``` -->

<!-- ## Big Data in the Social Sciences/Economics -->

<!-- ```{r twitter3, echo=FALSE, out.width = "80%", fig.align='center', purl=FALSE, fig.cap='Source: @ranco_etal2015'} -->
<!-- include_graphics("../img/01_twitter_stockmarket.png") -->
<!-- ``` -->




<!-- ## Big Data in the Social Sciences/Economics -->

<!-- - Often tied to web applications and digitization of economic and political processes. -->

<!-- ## Big Data in the Social Sciences/Economics -->

<!-- - Often tied to web applications and digitization of economic and political processes. -->
<!-- - *Volume* of data is substantial (but usually smaller than in the natural sciences). -->

<!-- ## Big Data in the Social Sciences/Economics -->

<!-- - Often tied to web applications and digitization of economic and political processes. -->
<!-- - *Volume* of data is substantial (but usually smaller than in natural sciences) -->
<!-- - *Variety* and *variability* often more challenging than in natural sciences. -->
<!--      - Various sources -->
<!--      - Data generation/sensors are independent from research endeavor. -->

<!-- ## Big Data in the Social Sciences/Economics -->

<!-- - Often tied to web applications and digitization of economic and political processes. -->
<!-- - *Volume* of data is substantial (but usually smaller than in natural sciences) -->
<!-- - *Variety* and *variability* often more challenging than in natural sciences. -->
<!--      - Various sources -->
<!--      - Data generation/sensors are independent from research endeavor. -->
<!-- - Questions/problems often similar to applied research in the industry. -->
<!--      - Key difference: usually no streaming applications (*velocity* not that much of an an issue). -->


# This Course

## Three Parts 

1. Big Data: Basic Concepts, Tools (Software/Hardware/Statistics)
2. Work with Big Data (Import/Clean/Prepare)
3. Visualization/Analytics

## Objectives

- Understand the *concept of Big Data* in the context of economic research.
- Understand the *technical challenges* of Big Data Analytics and how to practically deal with them.
- Know how to *apply* the relevant econometric tools, R packages and programming practices to effectively and efficiently handle large data sets.



## Schedule {.smaller}

 1. *Introduction:Introduction, Big Data Econometrics. Walkowiak (2016): Chapter 1.*
 2. Software/Tools, Efficient R Programming. Wickham (2019): Chapters 2, 3, 17,23, 24.
 3. Hardware: Computing Resources.
 4. Hardware: Distributed Systems. 
 5. Cloud Computing.
 6. Big Data Compilation and Storage. Walkowiak (2016): Chapter 5.
 7. Cleaning/Preparation. Walkowiak (2016): Chapter 3: p. 74‑118.
 8. Aggregation/Descriptives/Visualization. Walkowiak (2016): Chapter 3: p. 118‑127; Wickham et al.(2015); Schwabish (2014).
 9. Analytics I 
 9. Analytics II (screen cast)
 10. Project Presentations.
 11. Project Presentations; Q&A.
 

## Examination

- You will be working in *groups of four (or three)* and conduct a *big data analytics project*.
- You will find a *"large" data set (min. 2GB)* to work with and pose *1-2 relevant research questions*.
- You will use the techniques learned in this course to handle and analyze the large data set in order to provide *empirical answers* to your research question.
- The output of this exercise has two parts:
  1. A *technical report/"take-home exercises"*, with a detailed/structured account of the technical aspects of your project.
  2. A *short presentation* of your key findings.

## Examination: Part I

- Decentral 
‐ Group examination (all given the same grades) (60%).
- Group size: 4 (or 3) students.
- "Take‐home exercises"/research report: Application of basic concepts in R when working with big data. Conceptual questions related to the application.

<center> *Register your group here: [https://bit.ly/unilu_bda](https://bit.ly/unilu_bda)* </center>
<center> *Hand in on June 12 2022, 16:00* </center>
 
 
## Examination: Part II
- Decentral
- Group examination: presentation + code (all given the same grades) (40%)

<center> *(12 May 2022, 08:15-10:00; University of St.Gallen)* </center>

 
# Approach

---

```{r rlogo, echo=FALSE, out.width = "70%", fig.align='center', purl=FALSE}
include_graphics("../img/01_rlogo2.png")
```

---
 
```{r rstudio, echo=FALSE, out.width = "60%", fig.align='center', purl=FALSE}
include_graphics("../img/01_rstudio.png")
```


## Approaches to Big Data Analytics

1. *Statistics/econometrics and machine learning*: "Why not just take a random sample?!"
2. *Writing efficient code*: how can you generally speed up your analytics scripts? (scales with a lot of data)
3. *Use limited local computing resources more efficiently*: parallelization, virtual memory & co.
4. *Scale up and scale out*: distributed systems and cloud computing.


## R used in three ways

- A tool to analyze problems posed by large data sets.
     - For example, memory usage (in R).
- Illustration of underlying concepts (e.g. hardware/memory or parallelization).
- A practical tool for Big Data Analytics.

## Prerequisites?
  - Basic R programming skills.
  - Build on concepts taught in Data Analytics I (and more basic econometrics courses).
    - Brief review of concepts, but no additional introduction.


# Course Resources

## Literature

### Books
[Walkowiak, Simon (2016): Big Data Analytics with R. Birmingham, UK: Packt Publishing.](https://www.packtpub.com/big-data-and-business-intelligence/big-data-analytics-r)

- Available [here](https://eds.a.ebscohost.com/eds/detail/detail?vid=1&sid=c2adb69f-fdce-4f5e-89fd-c1bc6dbeef95%40sessionmgr4007&bdata=Jmxhbmc9ZGUmc2l0ZT1lZHMtbGl2ZQ%3d%3d#AN=stgal.000949061&db=cat00327a)

[Wickham, Hadley (2019): Advanced R. Second Edition, Boca Raton, FL: CRC Press](https://adv-r.hadley.nz/)



## Literature

### Journal Articles

[Wickham, Hadley and Dianne Cook and Heike Hofmann (2015): Visualizing statistical models: Removing the blindfold.Statistical Analysis and Data Mining: The ASA Data Science Journal. 8(4):203‐225.](https://onlinelibrary.wiley.com/doi/full/10.1002/sam.11271?scrollTo=references)

[Schwabish, Jonathan A. (2014): An Economistʹs Guide to Visualizing Data.Journal of Economic Perspectives. 28(1):209‐234.](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.1.209)

<center>*Lecture notes and slides will point to further reading...*</center>


## Notes, Slides, Code, et al.
- StudyNet: slides to download, links to other resources/downloads.
- [umatter.github.io/courses](https://umatter.github.io/courses.html)
- [github.com/umatter/BigData](https://github.com/umatter/BigData)
<!-- - [StudyNet/Canvas]() -->

## TODO 

- [Install R](https://stat.ethz.ch/CRAN/), [RStudio](https://www.rstudio.com/products/rstudio/download/#download)
- Set up your own [GitHub](https://github.com/)-account
- [Get familiar with Git/GitHub](https://guides.github.com/activities/hello-world/)
- Create an [AWS](https://aws.amazon.com/) and [Google Cloud Platform](https://cloud.google.com) account.

## Q&A

- General questions about the course?
- Exchange students: additional information regarding prerequisites


# Big Data Econometrics

## Two broad categories

- *Big P*: high-dimensional econometrics, "too many variables" (you know this from Data Analytics I!).
- *Big N*: "too many observations", data does not fit into memory (main focus of this course).

## A "Big P" problem
- You analyze the prepared log data from an e-commerce website.
- The aim is to come up with a useful econometric model to predict a purchase based on the user's origin (from where was she referred to the store).
- Each user origin (domain of the previously visited site) is an explanatory variable (big P...)
- Model serves the purpose of identifying potentially important sources that should be considered in future online marketing campaigns.
- Real-life data: [Google Analytics Sample](https://console.cloud.google.com/marketplace/product/obfuscated-ga360-data/obfuscated-ga360-data?_ga=2.140068147.1003332479.1645523176-998940270.1645523176&project=onlinemedia-slant)


## The data

```{r}
# import/inspect data
ga <- read.csv("../../data/ga.csv")
head(ga)

```


## Approaches based on "traditional" econometrics

- *Any ideas?*

## Better solution: lasso estimator

- Key idea: penalize complexity of the model in order to stabilize ("regularization").
- A lot of hypothesis testing is 'costly', lasso makes this cost explicit ($\lambda\sum_k{|\beta_k|}$).
- Provides an elegant way to get a sequence of candidate models (one for each $\lambda$; "regularization path").
- Procedure: first get the lasso regularization path (the candidate models sequence), then use model selection tools to choose the best model (i.e., the best $\hat{\lambda}$)

## Lasso example: estimation

```{r warning=FALSE, message=FALSE}
# load packages
library(gamlr)

# create model matrix
mm <- model.matrix(purchase~source + browser + isMobile, data = ga)
# run K-fold cross-valudation lasso
cvpurchase <- cv.gamlr(mm, ga$purchase, family="binomial")

```

## Lasso example: prediction based on "best model"

```{r}
# load packages
library(PRROC)

# use "best" model for prediction 
# (model selection based on average OSS deviance, here CV1se rule
pred <- predict(cvpurchase$gamlr, mm, type="response")

# compute tpr, fpr, plot ROC
comparison <- roc.curve(scores.class0 = pred,
                       weights.class0=ga$purchase,
                       curve=TRUE)
plot(comparison)

```



<!-- ````{r} -->
<!-- SC <- fread("https://raw.githubusercontent.com/TaddyLab/BDS/master/examples/semiconductor.csv") -->
<!-- full <- glm(FAIL ~ ., data=SC, family=binomial) -->
<!-- 1-full$deviance/full$null.deviance -->
<!-- full_sum <- summary(full) -->
<!-- full_sum$coefficients[,"Pr(>|z|)"] -->
<!-- bh <- p.adjust(full_sum$coefficients[,"Pr(>|z|)"], method = "BH") -->
<!-- bh[which(bh<0.1)][-1] # ignore intercept -->
<!-- bh_vars <- c("FAIL", names(bh[which(bh<0.1)][-1])) -->

<!-- bh_reg <- glm(FAIL ~ ., data=SC[, bh_vars, with=FALSE], family=binomial) -->

<!-- ``` -->


## *Big N* problem: least squares example

- Classical approach to estimating linear models: OLS.
- Alternative: The *Uluru* algorithm [@dhillon_2013].

## OLS as a point of reference

Recall the OLS estimator in matrix notation, given the linear model $\mathbf{y}=\mathbf{X}\beta + \epsilon$:

$\hat{\beta}_{OLS} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{y}$.

## Computational bottleneck of OLS

- $\hat{\beta}_{OLS}$ depends on $(\mathbf{X}^\intercal\mathbf{X})^{-1}$.
- Large cross-product if the number of observations is large ($X$ is of dimensions $n\times p$)
- (Large) matrix inversions are computationally demanding.
     <!-- - Computational complexity is larger than $O(n^{2})$. -->
- OLS has a $O(np^{2})$ running time.

## OLS in R

```{r}
beta_ols <- 
     function(X, y) {
          
          # compute cross products and inverse
          XXi <- solve(crossprod(X,X))
          Xy <- crossprod(X, y) 
          
          return( XXi  %*% Xy )
     }
```

## Monte Carlo study

- Parameters and pseudo data

```{r}
# set parameter values
n <- 10000000
p <- 4 

# Generate sample based on Monte Carlo
# generate a design matrix (~ our 'dataset') with four variables and 10000 observations
X <- matrix(rnorm(n*p, mean = 10), ncol = p)
# add column for intercept
X <- cbind(rep(1, n), X)

```

## Monte Carlo study

- Model and model output

```{r}
# MC model
y <- 2 + 1.5*X[,2] + 4*X[,3] - 3.5*X[,4] + 0.5*X[,5] + rnorm(n)

```


## Monte Carlo study

- Performance of OLS

```{r}
# apply the ols estimator
beta_ols(X, y)
```


## The Uluru algorithm as an alternative to OLS

Following @dhillon_2013, we compute $\hat{\beta}_{Uluru}$:

$$\hat{\beta}_{Uluru}=\hat{\beta}_{FS} + \hat{\beta}_{correct}$$, where
$$\hat{\beta}_{FS} = (\mathbf{X}_{subs}^\intercal\mathbf{X}_{subs})^{-1}\mathbf{X}_{subs}^{\intercal}\mathbf{y}_{subs}$$, and
$$\hat{\beta}_{correct}= \frac{n_{subs}}{n_{rem}} \cdot (\mathbf{X}_{subs}^\intercal\mathbf{X}_{subs})^{-1} \mathbf{X}_{rem}^{\intercal}\mathbf{R}_{rem}$$, and
$$\mathbf{R}_{rem} = \mathbf{Y}_{rem} - \mathbf{X}_{rem}  \cdot \hat{\beta}_{FS}$$.

## The Uluru algorithm as an alternative to OLS

- Key idea: Compute $(\mathbf{X}^\intercal\mathbf{X})^{-1}$ only on a sub-sample ($X_{subs}$, etc.)
- If the sample is large enough (which is the case in a Big Data context), the result is approximately the same.

## Uluru algorithm in R (simplified)

```{r}

# simple version of the Uluru algorithm
beta_uluru <-
     function(X_subs, y_subs, X_rem, y_rem) {
          
          # compute beta_fs (this is simply OLS applied to the subsample)
          XXi_subs <- solve(crossprod(X_subs, X_subs))
          Xy_subs <- crossprod(X_subs, y_subs)
          b_fs <- XXi_subs  %*% Xy_subs
          
          # compute \mathbf{R}_{rem}
          R_rem <- y_rem - X_rem %*% b_fs
          
          # compute \hat{\beta}_{correct}
          b_correct <- (nrow(X_subs)/(nrow(X_rem))) * XXi_subs %*% crossprod(X_rem, R_rem)

          # beta uluru       
          return(b_fs + b_correct)
     }

```


## Uluru algorithm in R (simplified)

Test it with the same input as above:

```{r}
# set size of subsample
n_subs <- 1000
# select subsample and remainder
n_obs <- nrow(X)
X_subs <- X[1L:n_subs,]
y_subs <- y[1L:n_subs]
X_rem <- X[(n_subs+1L):n_obs,]
y_rem <- y[(n_subs+1L):n_obs]

# apply the uluru estimator
beta_uluru(X_subs, y_subs, X_rem, y_rem)
```


## Uluru algorithm: Monte Carlo study

```{r}
# define subsamples
n_subs_sizes <- seq(from = 1000, to = 500000, by=10000)
n_runs <- length(n_subs_sizes)
# compute uluru result, stop time
mc_results <- rep(NA, n_runs)
mc_times <- rep(NA, n_runs)
for (i in 1:n_runs) {
     # set size of subsample
     n_subs <- n_subs_sizes[i]
     # select subsample and remainder
     n_obs <- nrow(X)
     X_subs <- X[1L:n_subs,]
     y_subs <- y[1L:n_subs]
     X_rem <- X[(n_subs+1L):n_obs,]
     y_rem <- y[(n_subs+1L):n_obs]
     
     mc_results[i] <- beta_uluru(X_subs, y_subs, X_rem, y_rem)[2] # the first element is the intercept
     mc_times[i] <- system.time(beta_uluru(X_subs, y_subs, X_rem, y_rem))[3]
     
}

```


## Uluru algorithm: Monte Carlo study

```{r}

# compute ols results and ols time
ols_time <- system.time(beta_ols(X, y))
ols_res <- beta_ols(X, y)[2]

```



## Uluru algorithm: Monte Carlo study

- Visualize comparison with OLS.

```{r}
# load packages
library(ggplot2)

# prepare data to plot
plotdata <- data.frame(beta1 = mc_results,
                       time_elapsed = mc_times,
                       subs_size = n_subs_sizes)
```

## Uluru algorithm: Monte Carlo study

1. Computation time.

```{r}
ggplot(plotdata, aes(x = subs_size, y = time_elapsed)) +
     geom_point(color="darkgreen") + 
     geom_hline(yintercept = ols_time[3],
                color = "red", 
                size = 1) +
     theme_minimal() +
     ylab("Time elapsed") +
     xlab("Subsample size")
```

## Uluru algorithm: Monte Carlo study

2. Precision

```{r}
ggplot(plotdata, aes(x = subs_size, y = beta1)) +
     geom_hline(yintercept = ols_res,
                color = "red", 
                size = 1) +
     geom_hline(yintercept = 1.5,
                color = "green",
                size = 1) +
     geom_point(color="darkgreen") + 
     theme_minimal() +
     ylab("Estimated coefficient") +
     xlab("Subsample size")

```




## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>
 