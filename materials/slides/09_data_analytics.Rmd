---
title: "Big Data Analytics"
subtitle: 'Lecture 9: Data Analytics I'
author: "Prof. Dr. Ulrich Matter"
output:
  ioslides_presentation:
    css: ../../style/ioslides.css
    template: ../../style/nologo_template.html
logo: ../img/logo.png
bibliography: ../../references/bigdata.bib
---


```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
library(knitr)
```

# Updates/Announcements

## Project presentations

- *Presentations schedule* is on StudyNet homepage!
- Please note that all teams/students are expected to be present on *BOTH presentation days* and *participate* in the discussions/feedback.
- You will have up to *15 minutes to present your project*. This way, we will have at least 5 minutes for Q&A per project.

## Project presentations

- "Economics-style presentation".
- Focus on (primarily economic) content, not coding (the project reports will provide enough room for the latter).
- Expected knowledge of audience: prerequisites for this course.
- Do not expect the audience to be familiar with every specialized method.


## Project presentations: grading aspects

- Difficulty (Data set size/complexity; methodology)
- Efficiency (Appropriate big data analytics tools used, efficient coding)
- Empirical Strategy (Usefulness of emp. Approach, given the RQ) 
- Accuracy (Explanation of strategy, interpretation of results, etc.)
- Overall Presentation (Convincing, to the point, clear, complete)

## Announcement: RA job opportunity

- Background: [SNF Project on "Personalized Information Provision Online"](https://p3.snf.ch/project-207698).
- Methodology: "Bot-based field experiments".
- Be part of a interdisciplinary team of web developers and economists.
- Flexible hours, 10-20% position, work at SIAW-HSG and from home.
- 6-12 months.


## Announcement: RA job opportunity

- *Your profile*
  - Solid programming skills, preferably in R and Python.
  - Successful coursework in Econometrics/Data Science courses.
  - Experience with web technologies/web development is an asset.
  - Experience with text data/NLP is an asset.
- *Interested?* Contact me during the break/end of class or via ulrich.matter@unisg.ch.


## Goals for today's lecture

- Know basics of visualizing geospatial data in `ggplot2`
- Know how to implement a parallel bootstrapping procedure
- Know how to efficiently estimate large fixed-effects linear model
- Meta: train how to apply knowledge in Big Data Analytics to specific econometric problems.
- Review of econometric concepts: clustered standard errors, fixed effects, within transformation.

# Recap: Aggregation/Visualization

## Data aggregation: The 'split-apply-combine' strategy

- Background: Compute a statistic for specific groups (e.g. women vs men,  etc.)

1. Split the data into subsamples (e.g. one for women, one for men)
2. Compute the statistic for each of the subsamples.
3. Combine all results in one table.


## `ggplot2`

- 'Grammar of Graphics'
- Build plots layer-by-layer
- *Here*: Useful tool for exploratory visualization

## Data import

We use the already familiar `fread()` to import the same first million observations from the January 2009 taxi trips records.

```{r message=FALSE, warning=FALSE}
# load packages
library(data.table)

# import data into RAM (needs around 200MB)
taxi <- fread("../../data/tlc_trips.csv",
              nrows = 50000)

```

## Data preparation


```{r message=FALSE, warning=FALSE}
# first, we remove the empty vars V8 and V9
taxi$V8 <- NULL
taxi$V9 <- NULL


# set covariate names according to the data dictionary
# see https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf
# note instead of taxizonne ids, long/lat are provided

varnames <- c("vendor_id",
              "pickup_time",
              "dropoff_time",
              "passenger_count",
              "trip_distance",
              "start_lat",
              "start_long",
              "dest_lat",
              "dest_long",
              "payment_type",
              "fare_amount",
              "extra",
              "mta_tax",
              "tip_amount",
              "tolls_amount",
              "total_amount")
names(taxi) <- varnames

# clean the factor levels
taxi$payment_type <- tolower(taxi$payment_type)
taxi$payment_type <- factor(taxi$payment_type, levels = unique(taxi$payment_type))     

```



## Final Plot

```{r message=FALSE, warning=FALSE}
library(ggplot2)

# indicate natural numbers
taxi[, dollar_paid := ifelse(tip_amount == round(tip_amount,0), "Full", "Fraction"),]


modelplot <- ggplot(data= taxi[payment_type == "credit" & dollar_paid == "Fraction" & 0 < tip_amount],
                    aes(x = fare_amount, y = tip_amount))
modelplot +
     geom_point(alpha=0.2, colour="darkgreen") +
     geom_smooth(method = "lm", colour = "black") +
     ylab("Amount of tip paid (in USD)") +
     xlab("Amount of fare paid (in USD)") +
     theme_bw(base_size = 18, base_family = "serif")
```

# Hint: `ggplot` and geospatial data

## Download map data

```{r message=FALSE, warning=FALSE}
# download the zipped shapefile to a temporary file, unzip
URL <- "https://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/nycd_19a.zip"
tmp_file <- tempfile()
download.file(URL, tmp_file)
file_path <- unzip(tmp_file, exdir= "../data")
# delete the temporary file
unlink(tmp_file)

```

## Import map data

```{r message=FALSE, warning=FALSE}
# load GIS packages
library(rgdal)
library(rgeos)

# read GIS data
nyc_map <- readOGR(file_path[1], verbose = FALSE)

# have a look at the polygons that constitute the map
summary(nyc_map)
```


## Change map projection

```{r message=FALSE, warning=FALSE}
# transform the projection
nyc_map <- spTransform(nyc_map, 
                       CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
# check result
summary(nyc_map)
```

## Prepare map for plotting with `ggplot2`

```{r warning=FALSE, message=FALSE}
nyc_map <- fortify(nyc_map)
```


## Base plot: Map of NYC


```{r message=FALSE, warning=FALSE}
# set up the canvas
ggplot(data = nyc_map, aes(x=long, y=lat)) +
geom_map(
  map = nyc_map,
  aes(map_id = id))
```



# Case study: Parallel processing



## Case study: Parallel processing

We start with importing the data into R.
```{r message=FALSE, warning=FALSE}
url <- "https://vincentarelbundock.github.io/Rdatasets/csv/carData/MplsStops.csv"
stopdata <- data.table::fread(url) 
```

## Case study: Parallel processing

First, let's remove observations with missing entries (`NA`) and code our main explanatory variable and the dependent variable.

```{r message=FALSE, warning=FALSE}
# remove incomplete obs
stopdata <- na.omit(stopdata)
# code dependent var
stopdata$vsearch <- 0
stopdata$vsearch[stopdata$vehicleSearch=="YES"] <- 1
# code explanatory var
stopdata$white <- 0
stopdata$white[stopdata$race=="White"] <- 1
```


## Case study: Parallel processing

We specify our baseline model as follows. 

```{r message=FALSE, warning=FALSE}
model <- vsearch ~ white + factor(policePrecinct)
```

## Case study: Parallel processing

And estimate the linear probability model via OLS (the `lm` function).

```{r message=FALSE, warning=FALSE}
fit <- lm(model, stopdata)
summary(fit)
```

## Case study: Parallel processing

Compute bootstrap clustered standard errors.

```{r message=FALSE}
# load packages
library(data.table)
# set the 'seed' for random numbers (makes the example reproducible)
set.seed(2)

# set number of bootstrap iterations
B <- 10
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)
# draw bootstrap samples, estimate model for each sample
for (i in 1:B) {
     
     # draw sample of precincts (cluster level)
     precincts_i <- sample(precincts, size = 5, replace = TRUE)
     # get observations
     bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
     bs_i <- rbindlist(bs_i)
     
     # estimate model and record coefficients
     boot_coefs[i,] <- coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
}
```

## Case study: Parallel processing

Finally, let's compute $SE_{boot}$.

```{r message=FALSE, warning=FALSE}
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot
```


## Case study: Parallel processing

Parallel implementation...

```{r message=FALSE, warning=FALSE}
# install.packages("doSNOW", "parallel")
# load packages for parallel processing
library(doSNOW)
# set the 'seed' for random numbers (makes the example reproducible)
set.seed(2)

# get the number of cores available
ncores <- parallel::detectCores()
# set cores for parallel processing
ctemp <- makeCluster(ncores) # 
registerDoSNOW(ctemp)


# set number of bootstrap iterations
B <- 10
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)

# bootstrapping in parallel
boot_coefs <- 
     foreach(i = 1:B, .combine = rbind, .packages="data.table") %dopar% {
          
          # draw sample of precincts (cluster level)
          precincts_i <- sample(precincts, size = 5, replace = TRUE)
          # get observations
          bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
          bs_i <- rbindlist(bs_i)
          
          # estimate model and record coefficients
          coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
      
     }


# be a good citizen and stop the snow clusters
stopCluster(cl = ctemp)


```

## Case study: Parallel processing

As a last step, we compute again $SE_{boot}$.

```{r message=FALSE, warning=FALSE}
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot
```


# Case Study II: Efficient Fixed Effects Estimation

## Case Study II: Efficient Fixed Effects Estimation

- Partial replication of ["Friends in High Places"](https://www.aeaweb.org/articles?id=10.1257/pol.6.3.63) by @cohen_malloy.
- Do US Senators with personal school ties help each other out in critical votes? 
- Data: [http://doi.org/10.3886/E114873V1](http://doi.org/10.3886/E114873V1). 
- Bottleneck: model matrix dimensions in fixed effects estimation.

## Data import and preparation

The data (and code) is provided in STATA format. We can import the main data set with the `foreign` package.

```{r message=FALSE, warning=FALSE}
# SET UP ------------------


# load packages
library(foreign)
library(data.table)
library(lmtest)

# fix vars
DATA_PATH <- "../../data/data_for_tables.dta"

# import data
cm <- as.data.table(read.dta(DATA_PATH))
# keep only clean obs
cm <- cm[!(is.na(yes)|is.na(pctsumyessameparty)|is.na(pctsumyessameschool)|is.na(pctsumyessamestate))] 

```

## Main variables

- Dependent variable: indicator `yes` that is equal to 1 if the corresponding senator voted Yes on the given bill and 0 otherwise.
- Explanatory vars of interest:
  - `pctsumyessameschool` (the percentage of senators from the same school as the corresponding senator who voted Yes on the given bill)
  - `pctsumyessamestate` (the percentage of senators from the same state as the corresponding senator who voted Yes on the given bill)
  - `pctsumyessameparty` (the percentage of senators from the same party as the corresponding senator who voted Yes on the given bill) 

## Where is the bottleneck?

- Consider the two-way fixed effects specifications:
  - Senators (individual FE)
  - Congress (time FE) or Congress-Session-Vote (more granular time FE)
- The fixed effect specification means that we introduce an indicator variable (an intercept) for $N-1$ senators and $M-1$ congresses!

## Model matrix without FEs

The model matrix ($X$) without accounting for fixed effects has dimensions $425653\times4$: 

```{r message=FALSE, warning=FALSE}
# pooled model (no FE)
model0 <-   yes ~ 
  pctsumyessameschool + 
  pctsumyessamestate + 
  pctsumyessameparty 

dim(model.matrix(model0, data=cm))
```



## Model matrix with FE dummies

The model matrix of specification (1) is of dimensions $425653\times221$, and the model matrix of specification (2) even of $425653\times6929$:

```{r message=FALSE, warning=FALSE}
model1 <- 
  yes ~ pctsumyessameschool + pctsumyessamestate + pctsumyessameparty + 
  factor(congress) + factor(id) -1
mm1 <- model.matrix(model1, data=cm)
dim(mm1)

```

## OLS with large model matrices?

- Computation of a very large matrix inversion (because $\hat{\beta}_{OLS} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{y}$)!
- And: the model matrix of specification 2 is about 22GB!


## Standard OLS with FE-dummies

In order to set a point of reference, we first estimate specification (1) with standard OLS.


```{r message=FALSE, warning=FALSE}

# fit specification (1)
runtime <- bench::mark(fit1 <- lm(data = cm, formula = model1))
coeftest(fit1)[2:4,]
# median amount of time needed for estimation
runtime$median
```


## Alternative: within estimator


- "Sweeping out the fixed effects dummies". 
- Preparatory step: "within transformation" or "demeaning" and is quite simple to implement. 
  - For each of the categories in the fixed effect variable the mean of the covariate and subtract the mean from the covariate's value.


## Alternative: within estimator

```{r message=FALSE, warning=FALSE}
# illustration of within transformation for the senator fixed effects
cm_within <- 
  with(cm, data.table(yes = yes - ave(yes, id),
                      pctsumyessameschool = pctsumyessameschool - ave(pctsumyessameschool, id),
                      pctsumyessamestate = pctsumyessamestate - ave(pctsumyessamestate, id),
                      pctsumyessameparty = pctsumyessameparty - ave(pctsumyessameparty, id)
                      ))

# comparison of dummy fixed effects estimator and within estimator
dummy_time <- bench::mark(fit_dummy <- 
              lm(yes ~ pctsumyessameschool + 
                           pctsumyessamestate + pctsumyessameparty + factor(id) -1, data = cm
                         ))
within_time <- bench::mark(fit_within <- 
                             lm(yes ~ pctsumyessameschool + 
                           pctsumyessamestate + pctsumyessameparty -1, data = cm_within))
```

## Comparison of dummies vs within transformation

```{r message=FALSE, warning=FALSE}

# computation time comparison
as.numeric(within_time$median)/as.numeric(dummy_time$median)

# comparison of estimates
coeftest(fit_dummy)[1:3, 1:3]
coeftest(fit_within)[, 1:3]

```

## Two-way FEs and within transformation?

- @GAURE20138 provides a generalization of the linear within-estimator to several fixed effects variables. 
- Implemented in the `lfe` package (@gaure_2013). 

## FE-estimation via `lfe`

```{r warning=FALSE, message=FALSE}
library(lfe)

# model and clustered SE specifications
model1 <- yes ~ pctsumyessameschool + pctsumyessamestate + pctsumyessameparty |congress+id|0|id
model2 <- yes ~ pctsumyessameschool + pctsumyessamestate + pctsumyessameparty |congress_session_votenumber+id|0|id

# estimation
fit1 <- felm(model1, data=cm)
fit2 <- felm(model2, data=cm)
```

## Regression Table

Replication of @cohen_malloy, Table 3, specifications (1) and (2)

```{r warning=FALSE, message=FALSE}
stargazer::stargazer(fit1,fit2,
                     type="text",
                     dep.var.labels = "Vote (yes/no)",
                     covariate.labels = c("School Connected Votes",
                                          "State Votes",
                                          "Party Votes"),
                     keep.stat = c("adj.rsq", "n"))
```




## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


