# Two domains of Big Data Analytics

Data analytics in the context of Big Data can be broadly categorized in two domains: techniques/estimators to address *big P*-problems and techniques/estimators to address *big N* problems. While this book presupposes some knowledge in at least one of these domains and mainly focuses on aspects of Big Data Analytics that precede the estimation of statistical models, it is useful to set the stage for the following chapters with two practical examples concerning *big P* and *big N* methods.

## A practical *big P* problem

Due to the abundance of digital data on all kind of human activities, empirical economists as well as business analysts are increasingly confronted with high-dimensional data (many signals, many variables). While having a lot of variables to work with sounds kind of like a good thing, it introduces new problems for coming up with useful predictive models. In the extreme case of having more variables in the model than observations, traditional methods cannot be used at all. In the less extreme case of just having dozens or hundreds of variables in a model (and plenty of observations) we risk to "falsely" discover seemingly influential variables and consequently come up with a model with potentially very misleading out-of-sample predictions. Thus, how can we find a reasonable model?^[Note that finding a model with good in-sample prediction performance is trivial when you have a lot of variables, simply adding more variables will improve the performance. However, that will inevitably result in a non-sense model as even highly significant variables might not have any actual predictive power when looking at out-of-sample predictions. Hence, in this kind of exercise we should *exclusively focus on out-of-sample predictions* when assessing the performance of candidate models.]

```{r include=FALSE, purl=FALSE, eval=FALSE}
# documents raw data processing

# load packages, credentials
library(bigrquery)
library(data.table)
library(DBI)

# set options
# avoid HTTP2 framing layer error when downloading results from google big query
# https://github.com/cloudyr/googleCloudStorageR/issues/71#issuecomment-332781571
httr::set_config(httr::config(http_version = 0))
# avoid problems with too many pages when downloading results
# https://stackoverflow.com/a/63351689
options(scipen = 20)

# fix vars
BILLING <- readLines("../../../../projects/websearch_polarization/_keys/bq_billing.txt") # the project ID on BigQuery (billing must be enabled)
PROJECT <- readLines("../../../../projects/websearch_polarization/_keys/bq_project.txt") # the project name on BigQuery
DATASET <- "tables"

# connect to DB on BigQuery
con <- dbConnect(
     bigrquery::bigquery(),
     project = PROJECT,
     dataset = DATASET,
     billing = BILLING
)


# run query
#query <- "SELECT * FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`"
query <-
"
SELECT  totals.visits, totals.transactions, trafficSource.source, device.browser, device.isMobile, geoNetwork.city, geoNetwork.country, channelGrouping
  FROM
    `bigquery-public-data.google_analytics_sample.ga_sessions_*`
  WHERE
    _TABLE_SUFFIX BETWEEN '20160101'
    AND '20171231';
"

ga <- as.data.table(dbGetQuery(con, query, page_size=15000))
ga$transactions[is.na(ga$transactions)] <- 0
ga <- ga[ga$city!="not available in demo dataset",]
ga$purchase <- as.integer(0<ga$transactions)
ga$transactions <- NULL
ga_p <- ga[purchase==1]
ga_rest <- ga[purchase==0][sample(1:nrow(ga[purchase==0]), 45000)]
ga <- rbindlist(list(ga_p, ga_rest))
potential_sources <- table(ga$source)
potential_sources <- names(potential_sources[1<potential_sources])
ga <- ga[ga$source %in% potential_sources,]

# store dataset
fwrite(ga, file="data/ga.csv")
```

Let's look at a real-life example. Suppose you work for Google's e-commerce platform [www.googlemerchandiseshop.com](https://shop.googlemerchandisestore.com) and you are in charge of predicting purchases (i.e., a user is actually buying something from your store in a given session) based on user and browser-session characteristics.^[We will in fact be working with a real-life Google Analytics data from www.googlemerchandiseshop.com, see here for details about the data set: https://www.blog.google/products/marketingplatform/analytics/introducing-google-analytics-sample/.] The dependent variable `purchase` is an indicator equal to `1` if the corresponding shop visit lead to a purchase and equal to `0` otherwise. All other variables contain information about the user and the session (where is the user located? which browser is she using? etc.). As the dependent variable is binary, we will use in a first step a simple logit model, in which we use the origins of the store visitors (how did a visitor end up in the shop) as explanatory variables. Note that many of these variables are categorical and the model matrix thus contains a lot of dummies. The plan in this (intentionally naive) first approach is to simply add a lot of explanatory variables to the model, run logit, and then select the variables with statistically significant coefficient estimates as the final predictive model.

```{r}
# import/inspect data
ga <- read.csv("data/ga.csv")
head(ga)

# create model matrix (dummy vars)
mm <- cbind(ga$purchase, model.matrix(purchase~source, data=ga,)[,-1])
mm_df <- as.data.frame(mm) 
# clean variable names
names(mm_df) <- c("purchase", gsub("source", "", names(mm_df)[-1]))

# run logit
model1 <- glm(purchase ~ .,
              data=mm_df, family=binomial)
```

Now we can perform the t-tests and filter out the "relevant" variables.

```{r}
model1_sum <- summary(model1)

# select "significant" variables for final model
pvalues <- model1_sum$coefficients[,"Pr(>|z|)"]
vars <- names(pvalues[which(pvalues<0.05)][-1])
vars

```

Finally, we re-estimate our "final" model

```{r}
# specify and estimate the final model
finalmodel <- glm(purchase ~.,
                  data = mm_df[, c("purchase", vars)], family = binomial)
```


The first problem with this approach is that we should not trust the coefficient t-tests based on which we have selected the covariates too much. The first model model contains 62 explanatory variables (+ the intercept). With that many hypothesis tests, we quite likely reject the NULL of no predictive effect although there is actually no predictive effect. In addition, this approach turns out to be unstable. There might be correlation between some variables in the original set of variables and adding/removing even one variable might substantially affect the predictive power of the model (and the apparent relevance of individual variables). We see this already from the summary of our final model estimate. One of the apparently relevant predictors (`dfa`) is not at all significant anymore in this specification. Thus, we might be tempted to further change the model, which in turn would again change the apparent relevance of other covariates, etc.

```{r}
summary(finalmodel)
```

An alternative approach would be to estimate models based on all possible combinations of covariates and then use that sequence of models to select the final model based on some out-of-sample prediction performance measure. Clearly such an approach would take a long time to compute. 

Instead, the *lasso estimator* provides a convenient and efficient way to get a sequence of candidate model. The key idea behind the lasso is to penalize model complexity (the cause for instability) during the estimation procedure.^[This is done by adding $\lambda\sum_k{|\beta_k|}$ as a 'cost' to the optimization problem.] In a second step, we can then select a final model from the sequence of candidate models based on, for example, "out-of-sample" prediction in a k-fold cross validation.

The `gamlr` package provides both parts of this procedure (lasso for the sequence of candidate models, and selection of "best" model based on k-fold CV).

```{r warning=FALSE, message=FALSE}
# load packages
library(gamlr)

# create model matrix
mm <- model.matrix(purchase~source, data = ga)
```

In cases with both many observations and many candidate explanatory variables, the model matrix might get very large. Even just computing the model matrix might be a computational burden, as we might run out of memory to hold the model matrix object. If this large model matrix is sparse (i.e, has a lot of `0`-entries) there is an alternative way to store it in an R object more memory efficient.

```{r}
# create the sparse model matrix
mm_sparse <- sparse.model.matrix(purchase~source, data = ga)
# the sparse representation needs less than 15% of the memory needed for 
# the standard matrix representation in this case:
as.numeric(object.size(mm_sparse)/object.size(mm))

```
Finally, we run the lasso estimation with K-fold cross-validation.

```{r}
# run K-fold cross-validation lasso
cvpurchase <- cv.gamlr(mm, ga$purchase, family="binomial")
```


We then can illustrate the performance of the selected final model, for example, with a ROC curve.

```{r}
# load packages
library(PRROC)

# use "best" model for prediction 
# (model selection based on average OSS deviance, here CV1se rule
pred <- predict(cvpurchase$gamlr, mm, type="response")

# compute tpr, fpr, plot ROC
comparison <- roc.curve(scores.class0 = pred,
                       weights.class0=ga$purchase,
                       curve=TRUE)
plot(comparison)

```

Hence, econometrics techniques such as the lasso help deal with big P problems by providing reasonable ways to select a good predictive model (in other words, decide which of the many variables should be included).


## A practical *big N* problem

Big N problems are situations in which we know what type of model we want to use but the *number of observations* is too big to run the estimation (computer crashes or slows down a lot). The simplest statistical solution to such a problem is usually to just estimate the model based on a smaller sample. However, we might not want to do that for other reasons (see introduction above). As an illustration of how an alternative statistical procedure can speed up the analysis of big N datasets, we look at a procedure to estimate linear models when the classical OLS estimator is computationally too demanding when analyzing large datasets: The *Uluru* algorithm [@dhillon_2013].

### OLS as a point of reference

Recall the OLS estimator in matrix notation, given the linear model $\mathbf{y}=\mathbf{X}\beta + \epsilon$:

$\hat{\beta}_{OLS} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{y}$.

In order to compute $\hat{\beta}_{OLS}$, we have to compute $(\mathbf{X}^\intercal\mathbf{X})^{-1}$, which implies a computationally expensive matrix inversion.^[The computational complexity of this is larger than $O(n^{2})$. That is, for an input of size $n$, the time needed to compute (or the number of operations needed) is $n^2$.] If our data set is large, $\mathbf{X}$ is large and the inversion can take up a lot of computation time. Moreover, the inversion and matrix multiplication to get $\hat{\beta}_{OLS}$ needs a lot of memory. In practice, it might well be that the estimation of a linear model via OLS with the standard approach in R (`lm()`) brings a computer to its knees, as there is not enough RAM available.

To further illustrate the point, we implement the OLS estimator in R.

```{r}
beta_ols <- 
     function(X, y) {
          
          # compute cross products and inverse
          XXi <- solve(crossprod(X,X))
          Xy <- crossprod(X, y) 
          
          return( XXi  %*% Xy )
     }
```

Now, we will test our OLS estimator function with a few (pseudo) random numbers in a Monte Carlo study. First, we set the sample size parameters `n` (how many observations shall our pseudo sample have?) and `p` (how many variables shall describe these observations?) and initiate the data set `X`.

```{r}
# set parameter values
n <- 10000000
p <- 4 

# Generate sample based on Monte Carlo
# generate a design matrix (~ our 'dataset') with four variables and 10000 observations
X <- matrix(rnorm(n*p, mean = 10), ncol = p)
# add column for intercept
X <- cbind(rep(1, n), X)

```

Now we define how the real linear model looks like that we have in mind and compute the output `y` of this model, given the input `X`.^[In reality we would not know this, of course. Acting as if we knew the real model is exactly the point of Monte Carlo studies. It allows us to analyze the properties of estimators by simulation.]

```{r}
# MC model
y <- 2 + 1.5*X[,2] + 4*X[,3] - 3.5*X[,4] + 0.5*X[,5] + rnorm(n)

```


Finally, we test our `beta_ols` function.

```{r}
# apply the ols estimator
beta_ols(X, y)
```


### The Uluru algorithm as an alternative to OLS

Following @dhillon_2013, we implement a procedure to compute $\hat{\beta}_{Uluru}$:

$$\hat{\beta}_{Uluru}=\hat{\beta}_{FS} + \hat{\beta}_{correct}$$, where
$$\hat{\beta}_{FS} = (\mathbf{X}_{subs}^\intercal\mathbf{X}_{subs})^{-1}\mathbf{X}_{subs}^{\intercal}\mathbf{y}_{subs}$$, and
$$\hat{\beta}_{correct}= \frac{n_{subs}}{n_{rem}} \cdot (\mathbf{X}_{subs}^\intercal\mathbf{X}_{subs})^{-1} \mathbf{X}_{rem}^{\intercal}\mathbf{R}_{rem}$$, and
$$\mathbf{R}_{rem} = \mathbf{Y}_{rem} - \mathbf{X}_{rem}  \cdot \hat{\beta}_{FS}$$.

The key idea behind this is that the computational bottleneck of the OLS estimator, the cross product and matrix inversion,$(\mathbf{X}^\intercal\mathbf{X})^{-1}$, is only computed on a sub-sample ($X_{subs}$, etc.), not the entire data set. However, the remainder of the data set is also taken into consideration (in order to correct a bias arising from the sub-sampling). Again, we implement the estimator in R to further illustrate this point.

```{r}


beta_uluru <-
     function(X_subs, y_subs, X_rem, y_rem) {
          
          # compute beta_fs (this is simply OLS applied to the subsample)
          XXi_subs <- solve(crossprod(X_subs, X_subs))
          Xy_subs <- crossprod(X_subs, y_subs)
          b_fs <- XXi_subs  %*% Xy_subs
          
          # compute \mathbf{R}_{rem}
          R_rem <- y_rem - X_rem %*% b_fs
          
          # compute \hat{\beta}_{correct}
          b_correct <- (nrow(X_subs)/(nrow(X_rem))) * XXi_subs %*% crossprod(X_rem, R_rem)

          # beta uluru       
          return(b_fs + b_correct)
     }

```


Test it with the same input as above:

```{r}
# set size of subsample
n_subs <- 1000
# select subsample and remainder
n_obs <- nrow(X)
X_subs <- X[1L:n_subs,]
y_subs <- y[1L:n_subs]
X_rem <- X[(n_subs+1L):n_obs,]
y_rem <- y[(n_subs+1L):n_obs]

# apply the uluru estimator
beta_uluru(X_subs, y_subs, X_rem, y_rem)
```


This looks quite good already. Let's have a closer look with a little Monte Carlo study. The aim of the simulation study is to visualize the difference between the classical OLS approach and the *Uluru* algorithm with regard to bias and time complexity if we increase the sub-sample size in *Uluru*. For simplicity, we only look at the first estimated coefficient $\beta_{1}$.

```{r}
# define subsamples
n_subs_sizes <- seq(from = 1000, to = 500000, by=10000)
n_runs <- length(n_subs_sizes)
# compute uluru result, stop time
mc_results <- rep(NA, n_runs)
mc_times <- rep(NA, n_runs)
for (i in 1:n_runs) {
     # set size of subsample
     n_subs <- n_subs_sizes[i]
     # select subsample and remainder
     n_obs <- nrow(X)
     X_subs <- X[1L:n_subs,]
     y_subs <- y[1L:n_subs]
     X_rem <- X[(n_subs+1L):n_obs,]
     y_rem <- y[(n_subs+1L):n_obs]
     
     mc_results[i] <- beta_uluru(X_subs, y_subs, X_rem, y_rem)[2] # the first element is the intercept
     mc_times[i] <- system.time(beta_uluru(X_subs, y_subs, X_rem, y_rem))[3]
     
}

# compute ols results and ols time
ols_time <- system.time(beta_ols(X, y))
ols_res <- beta_ols(X, y)[2]

```

Let's visualize the comparison with OLS.

```{r message=FALSE, warning=FALSE}
# load packages
library(ggplot2)

# prepare data to plot
plotdata <- data.frame(beta1 = mc_results,
                       time_elapsed = mc_times,
                       subs_size = n_subs_sizes)
```

First, let's look at the time used estimate the linear model.

```{r}
ggplot(plotdata, aes(x = subs_size, y = time_elapsed)) +
     geom_point(color="darkgreen") + 
     geom_hline(yintercept = ols_time[3],
                color = "red", 
                size = 1) +
     theme_minimal() +
     ylab("Time elapsed") +
     xlab("Subsample size")
```

The horizontal red line indicates the computation time for estimation via OLS, the green points indicate the computation time for the estimation via the Ulruru algorithm. Note that even for large sub-samples, the computation time is substantially lower than for OLS.

Finally, let's have a look at how close the results are to OLS.
```{r}
ggplot(plotdata, aes(x = subs_size, y = beta1)) +
     geom_hline(yintercept = ols_res,
                color = "red", 
                size = 1) +
       geom_hline(yintercept = 1.5,
                color = "green",
                size = 1) +
     geom_point(color="darkgreen") + 

     theme_minimal() +
     ylab("Estimated coefficient") +
     xlab("Subsample size")
```

The horizontal red line indicates the size of the estimated coefficient, when using OLS. The horizontal green line indicates the size of the actual coefficient. The green points indicate the size of the same coefficient estimated by the Uluru algorithm for different sub-sample sizes. Note that even relatively small sub-samples already deliver estimates very close to the OLS estimates.




