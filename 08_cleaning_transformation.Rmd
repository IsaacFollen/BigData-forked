

# Big Data Cleaning and Transformation 

Preceding the filtering/selection/aggregation of raw data,  data cleaning and transformation typically have to be run on large parts of the overall data set. In practice, the bottleneck is often a lack of RAM. In the following, we explore two strategies that broadly build on the idea of *virtual memory* (using parts of the hard disk as RAM).


## 'Out-of-memory' strategies

Virtual memory is in simple words an approach to combining the RAM and mass storage components in order to cope with a lack of RAM. Modern operating systems come with a virtual memory manager that would automatically handle the swapping between RAM and the hard-disk, when running processes that use up too much RAM. However, a virtual memory manager is not specifically developed to perform this task in the context of data analysis. Several strategies have thus been developed to build on the basic idea of virtual memory in the context of data analysis tasks.

- *Chunked data files on disk*: The data analytics software 'partitions' the large data set, maps, and stores the chunks of raw data on disk. What is actually 'read' into RAM when importing the data file with this approach is the mapping to the partitions of the actual data set (the data structure) and some metadata describing the data set. In R, this approach is implemented in the `ff` package and several packages building on `ff`. In this approach, the usage of disk space and the linking between RAM and files on disk is very explicit (and well visible to the user).

- *Memory mapped files and shared memory*: The data analytics software uses segments of virtual memory for the data set and allows different programs/processes to access it in the same memory segment. Thus, virtual memory is explicitly allocated for one or several specific data analytics tasks. In R, this approach is prominently implemented in the `bigmemory` package and several packages building on `bigmemory`.


### Chunking data with the `ff`-package

Before looking at the more detailed code examples in @walkowiak_2016, we investigate how the `ff` package (and the concept of chunked files) basically works. In order to do so, we first install and load the `ff` and `ffbase` packages, as well as the `pryr` package. We use the already known `flights.csv`-data set as an example. When importing data via the `ff` package, we first have to set up a directory where `ff` can store the partitioned data set (recall that this is explicitly/visibly done on disk). As in the code examples of the book, we call this new directory `ffdf` (after `ff`-data.frame).

```{r message=FALSE}

# SET UP --------------

# install.packages(c("ff", "ffbase"))
# load packages
library(ff)
library(ffbase)
library(pryr)

# create directory for ff chunks, and assign directory to ff 
system("mkdir ffdf")
options(fftempdir = "ffdf")

```

Now we can read in the data with `read.table.ffdf`. In order to better understand the underlying concept, we record the change in memory in the R environment with `mem_change()`.

```{r}
mem_change(
flights <- 
     read.table.ffdf(file="data/flights.csv",
                     sep=",",
                     VERBOSE=TRUE,
                     header=TRUE,
                     next.rows=100000,
                     colClasses=NA)
)
```

Note that there are two substantial differences to what we have previously seen when using `fread()`. It takes much longer to import a csv into the ffdf structure. However, the RAM allocated to it is much smaller. This is exactly what we would expect, keeping in mind what `read.table.ffdf()` does in comparison to what `fread()` does. Now we can actually have a look at the data chunks created by `ff`.

```{r}
# show the files in the directory keeping the chunks
head(list.files("ffdf"))

```



### Memory mapping with `bigmemory`

The `bigmemory`-package handles data in matrices, and therefore only accepts variables in the same data type. Before importing data via the `bigmemory`-package, we thus have to ensure that all variables in the raw data can be imported in a common type. This example follows the example of the package authors given [here](https://cran.r-project.org/web/packages/bigmemory/vignettes/Overview.pdf).^[We only use a fraction of the data used in the package vignette example, the full raw data used there can be downloaded [here](http://stat-computing.org/dataexpo/2009/the-data.html).]

```{r message=FALSE, warning=FALSE}

# SET UP ----------------

# load packages
library(bigmemory)
library(biganalytics)

# import the data
flights <- read.big.matrix("data/flights.csv",
                     type="integer",
                     header=TRUE,
                     backingfile="flights.bin",
                     descriptorfile="flights.desc")
```

Note that, similar to the `ff`-example, `read.big.matrix()` initiates a local file-backing `flights.bin` on disk which is linked to the `flights`-object in RAM. From looking at the imported file, we see that various variable values have been discarded. This is due to the fact that we have forced all variables to be of type `"integer"` when importing the data set. 

```{r}
summary(flights)
```







## Typical cleaning tasks

- Normalize/standardize.
- Code additional variables (indicators, strings to categorical, etc.).
- Remove, add covariates.
- Merge data sets.
- Set data types.


1. Import raw data.
2. Clean/transform.
3. Store for analysis.
     - Write to file.
     - Write to database.
     

- RAM:
     - Raw data does not fit into memory.
     - Transformations enlarge RAM allocation (copying).
- Mass Storage: Reading/Writing
- CPU: Parsing (data types)

### Data Preparation with `ff`

#### Set up

The following examples are based on @walkowiak_2016, Chapter 3. You can download the original data sets used in these examples from [the book's GitHub repository](https://github.com/PacktPublishing/Big-Data-Analytics-with-R/tree/master/Chapter%203). The set up for our analysis script involves the loading of the `ff` and `ffbase` packages, the intitiation of fix variables to hold the paths to the data sets, as well as the creation and assignment of a new local directory `ffdf` in which the binary flat files-partitioned chunks of the original data sets will be stored.

```{r warning=FALSE, message=FALSE}

## SET UP ------------------------

# create and set directory for ff files
system("mkdir ffdf")
options(fftempdir = "ffdf")

# load packages
library(ff)
library(ffbase)
library(pryr)

# fix vars
FLIGHTS_DATA <- "data/flights_sep_oct15.txt"
AIRLINES_DATA <- "data/airline_id.csv"

```

#### Data import

In a first step we read (or 'upload') the data into R. This step involves the creation of the binary chunked files as well as the mapping of these files and the metadata.  In comparison to the traditional `read.csv` approach, you will notice two things. On the one hand the data import takes longer, on the other hand it uses up much less RAM than than with `read.csv`. 

```{r}

# DATA IMPORT ------------------

# check memory used
mem_used()

# 1. Upload flights_sep_oct15.txt and airline_id.csv files from flat files. 

system.time(flights.ff <- read.table.ffdf(file=FLIGHTS_DATA,
                                          sep=",",
                                          VERBOSE=TRUE,
                                          header=TRUE,
                                          next.rows=100000,
                                          colClasses=NA))

system.time(airlines.ff <- read.csv.ffdf(file= AIRLINES_DATA,
                             VERBOSE=TRUE,
                             header=TRUE,
                             next.rows=100000,
                             colClasses=NA))

# check memory used
mem_used()
```


Comparison with `read.table`

```{r}

##Using read.table()
system.time(flights.table <- read.table(FLIGHTS_DATA, 
                                        sep=",",
                                        header=TRUE))

gc()

system.time(airlines.table <- read.csv(AIRLINES_DATA,
                                       header = TRUE))


# check memory used
mem_used()

```


#### Inspect imported files

A particularly useful aspect of working with the ff-package and the packages building on them is that many of the simple R functions that work on usual data.frames in RAM also work on ffdfs. Hence, without actually having loaded the entire raw data of a large data set into RAM, we can quickly get an overview of the key characteristics such as the number of observations and the number of variables.

```{r}
# 2. Inspect the ffdf objects.
## For flights.ff object:
class(flights.ff)
dim(flights.ff)
## For airlines.ff object:
class(airlines.ff)
dim(airlines.ff)

```

#### Data cleaning and transformation

After inspecting the data, we go through several steps of cleaning and transformation, with the goal of then merging the two data sets. That is, we want to create a new data set that contains detailed flights information but with additional information on the carriers/airlines. First, we want to rename some of the variables.


```{r}
# step 1: 
## Rename "Code" variable from airlines.ff to "AIRLINE_ID" and "Description" into "AIRLINE_NM".
names(airlines.ff) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.ff)
str(airlines.ff[1:20,])
```

Now we can join the two data sets via the unique airline identifier `"AIRLINE_ID"`. Note that these kind of operations would usually take up substantially more RAM on the spot, if both original data sets would also be fully loaded into RAM. As illustrated by the `mem_change()`-function, this is not the case here. All that is needed is a small chunk of RAM to keep the metadata and mapping-information of the new `ffdf` object, all the actual data is cached on the hard disk.

```{r}
# merge of ffdf objects
mem_change(flights.data.ff <- merge.ffdf(flights.ff, airlines.ff, by="AIRLINE_ID"))
#The new object is only 551.2 Kb in size
class(flights.data.ff)
dim(flights.data.ff)
names(flights.data.ff)
```

Inspect difference to in-memory operation

```{r}
##For flights.table:
names(airlines.table) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.table)
str(airlines.table[1:20,])

# check memory usage of merge in RAM 
mem_change(flights.data.table <- merge(flights.table,
                                       airlines.table,
                                       by="AIRLINE_ID"))
#The new object is already 105.7 Mb in size
#A rapid spike in RAM use when processing
```




#### Subsetting

Now, we want to filter out some observations as well as select only specific variables for a subset of the overall data set.

```{r}
mem_used()

# Subset the ffdf object flights.data.ff:
subs1.ff <- subset.ffdf(flights.data.ff, CANCELLED == 1, 
                        select = c(FL_DATE, AIRLINE_ID, 
                                   ORIGIN_CITY_NAME,
                                   ORIGIN_STATE_NM,
                                   DEST_CITY_NAME,
                                   DEST_STATE_NM,
                                   CANCELLATION_CODE))

dim(subs1.ff)
mem_used()

```


#### Save/load/export ffdf-files

In order to better organize and easily reload newly created `ffdf`s, we can explicitly save them to disk.

```{r}
# Save a newly created ffdf object to a data file:

save.ffdf(subs1.ff, overwrite = TRUE) #7 files (one for each column) created in the ffdb directory

```



If we want to reload a previously saved `ffdf`, we do not have to go through the chunking of a raw data file again, but can very quickly load the data mapping and metadata into RAM in order to further work with the data (stored on disk).

```{r}
# Loading previously saved ffdf files:
rm(subs1.ff)
gc()
load.ffdf("ffdb")
# check the class and structure of the loaded data
class(subs1.ff) 
str(subs1.ff)
dim(subs1.ff)
dimnames(subs1.ff)
```


In case we want to store an `ffdf` data set in a format more accessible for other users (such as csv), we can do so as follows. This last step is also quite common in practice. The initial raw data set is very large, thus we perform all the theoretically very memory-intense tasks of preparing the analytic data set via `ff` and then store the (often much smaller) analytic data set in a more accessible csv file in order to later read it into RAM and run more computationally intense analyses directly in RAM.

```{r message=FALSE}
#  Export subs1.ff into CSV and TXT files:
write.csv.ffdf(subs1.ff, "subset1.csv")

```


