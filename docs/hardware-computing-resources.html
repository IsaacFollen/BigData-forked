<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Hardware: Computing Resources | Big Data Analytics</title>
  <meta name="description" content="A guide to data science practitioners making the transition to Big Data." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Hardware: Computing Resources | Big Data Analytics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://umatter.github.io/BigData/img/cover.png" />
  <meta property="og:description" content="A guide to data science practitioners making the transition to Big Data." />
  <meta name="github-repo" content="umatter/BigData" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Hardware: Computing Resources | Big Data Analytics" />
  
  <meta name="twitter:description" content="A guide to data science practitioners making the transition to Big Data." />
  <meta name="twitter:image" content="https://umatter.github.io/BigData/img/cover.png" />

<meta name="author" content="Ulrich Matter" />


<meta name="date" content="2022-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="software-programming-with-big-data.html"/>
<link rel="next" href="distributed-systems.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
</ul></li>
<li class="part"><span><b>I Setting the Scene: Analyzing Big Data</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-big-in-big-data"><i class="fa fa-check"></i><b>1.1</b> What is <em>big</em> in “Big Data”?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#approaches-to-analyzing-big-data"><i class="fa fa-check"></i><b>1.2</b> Approaches to analyzing Big Data</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#content-overview"><i class="fa fa-check"></i><b>1.3</b> Content overview</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="two-domains-of-big-data-analytics.html"><a href="two-domains-of-big-data-analytics.html"><i class="fa fa-check"></i><b>2</b> Two domains of Big Data Analytics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="two-domains-of-big-data-analytics.html"><a href="two-domains-of-big-data-analytics.html#a-practical-big-p-problem"><i class="fa fa-check"></i><b>2.1</b> A practical <em>big P</em> problem</a></li>
<li class="chapter" data-level="2.2" data-path="two-domains-of-big-data-analytics.html"><a href="two-domains-of-big-data-analytics.html#a-practical-big-n-problem"><i class="fa fa-check"></i><b>2.2</b> A practical <em>big N</em> problem</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="two-domains-of-big-data-analytics.html"><a href="two-domains-of-big-data-analytics.html#ols-as-a-point-of-reference"><i class="fa fa-check"></i><b>2.2.1</b> OLS as a point of reference</a></li>
<li class="chapter" data-level="2.2.2" data-path="two-domains-of-big-data-analytics.html"><a href="two-domains-of-big-data-analytics.html#the-uluru-algorithm-as-an-alternative-to-ols"><i class="fa fa-check"></i><b>2.2.2</b> The <em>Uluru</em> algorithm as an alternative to OLS</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="two-domains-of-big-data-analytics.html"><a href="two-domains-of-big-data-analytics.html#conclusion"><i class="fa fa-check"></i><b>2.3</b> Conclusion</a></li>
</ul></li>
<li class="part"><span><b>II Platform: Software and Computing Resources</b></span></li>
<li class="chapter" data-level="3" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html"><i class="fa fa-check"></i><b>3</b> Software: Programming with (Big) Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#domains-of-programming-with-big-data"><i class="fa fa-check"></i><b>3.1</b> Domains of programming with (big) data</a></li>
<li class="chapter" data-level="3.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#measuring-r-performance"><i class="fa fa-check"></i><b>3.2</b> Measuring R performance</a></li>
<li class="chapter" data-level="3.3" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#writing-efficient-r-code"><i class="fa fa-check"></i><b>3.3</b> Writing efficient R code</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#memory-allocation-and-growing-objects"><i class="fa fa-check"></i><b>3.3.1</b> Memory allocation and growing objects</a></li>
<li class="chapter" data-level="3.3.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#vectorization-in-basic-r-functions"><i class="fa fa-check"></i><b>3.3.2</b> Vectorization in basic R functions</a></li>
<li class="chapter" data-level="3.3.3" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#apply-type-functions-and-vectorization"><i class="fa fa-check"></i><b>3.3.3</b> <code>apply</code>-type functions and vectorization</a></li>
<li class="chapter" data-level="3.3.4" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#avoid-unnecessary-copying"><i class="fa fa-check"></i><b>3.3.4</b> Avoid unnecessary copying</a></li>
<li class="chapter" data-level="3.3.5" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#releasing-memory"><i class="fa fa-check"></i><b>3.3.5</b> Releasing memory</a></li>
<li class="chapter" data-level="3.3.6" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#beyond-r"><i class="fa fa-check"></i><b>3.3.6</b> Beyond R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#sql-basics"><i class="fa fa-check"></i><b>3.4</b> SQL basics</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#first-steps-in-sqlite"><i class="fa fa-check"></i><b>3.4.1</b> First steps in SQL(ite)</a></li>
<li class="chapter" data-level="3.4.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#joins"><i class="fa fa-check"></i><b>3.4.2</b> Joins</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html"><i class="fa fa-check"></i><b>4</b> Hardware: Computing Resources</a>
<ul>
<li class="chapter" data-level="4.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#components-of-a-standard-computing-environment"><i class="fa fa-check"></i><b>4.1</b> Components of a standard computing environment</a></li>
<li class="chapter" data-level="4.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#mass-storage"><i class="fa fa-check"></i><b>4.2</b> Mass storage</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#avoid-redundancies"><i class="fa fa-check"></i><b>4.2.1</b> Avoid redundancies</a></li>
<li class="chapter" data-level="4.2.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#data-compression"><i class="fa fa-check"></i><b>4.2.2</b> Data compression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#random-access-memory-ram"><i class="fa fa-check"></i><b>4.3</b> Random access memory (RAM)</a></li>
<li class="chapter" data-level="4.4" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#combining-ram-and-hard-disk-virtual-memory"><i class="fa fa-check"></i><b>4.4</b> Combining RAM and hard disk: virtual memory</a></li>
<li class="chapter" data-level="4.5" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#cpu-and-parallelization"><i class="fa fa-check"></i><b>4.5</b> CPU and parallelization</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#naive-multi-session-approach"><i class="fa fa-check"></i><b>4.5.1</b> Naive multi-session approach</a></li>
<li class="chapter" data-level="4.5.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#multi-core-and-multi-node-approach"><i class="fa fa-check"></i><b>4.5.2</b> Multi-core and multi-node approach</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#gpus-for-scientific-computing"><i class="fa fa-check"></i><b>4.6</b> GPUs for scientific computing</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#gpus-in-r"><i class="fa fa-check"></i><b>4.6.1</b> GPUs in R</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#the-road-ahead-hardware-made-for-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The road ahead: Hardware made for machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#insufficient-computing-resources"><i class="fa fa-check"></i><b>4.8</b> Insufficient computing resources?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="distributed-systems.html"><a href="distributed-systems.html"><i class="fa fa-check"></i><b>5</b> Distributed Systems</a>
<ul>
<li class="chapter" data-level="5.1" data-path="distributed-systems.html"><a href="distributed-systems.html#mapreduce"><i class="fa fa-check"></i><b>5.1</b> MapReduce</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="distributed-systems.html"><a href="distributed-systems.html#mapreduce-concept-illustrated-in-r"><i class="fa fa-check"></i><b>5.1.1</b> Map/Reduce Concept Illustrated in R</a></li>
<li class="chapter" data-level="5.1.2" data-path="distributed-systems.html"><a href="distributed-systems.html#mapper"><i class="fa fa-check"></i><b>5.1.2</b> Mapper</a></li>
<li class="chapter" data-level="5.1.3" data-path="distributed-systems.html"><a href="distributed-systems.html#reducer"><i class="fa fa-check"></i><b>5.1.3</b> Reducer</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="distributed-systems.html"><a href="distributed-systems.html#hadoop"><i class="fa fa-check"></i><b>5.2</b> Hadoop</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="distributed-systems.html"><a href="distributed-systems.html#hadoop-word-count-example"><i class="fa fa-check"></i><b>5.2.1</b> Hadoop word count example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="distributed-systems.html"><a href="distributed-systems.html#spark"><i class="fa fa-check"></i><b>5.3</b> Spark</a></li>
<li class="chapter" data-level="5.4" data-path="distributed-systems.html"><a href="distributed-systems.html#spark-with-r"><i class="fa fa-check"></i><b>5.4</b> Spark with R</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="distributed-systems.html"><a href="distributed-systems.html#data-import-and-summary-statistics"><i class="fa fa-check"></i><b>5.4.1</b> Data import and summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="distributed-systems.html"><a href="distributed-systems.html#spark-with-sql"><i class="fa fa-check"></i><b>5.5</b> Spark with SQL</a></li>
<li class="chapter" data-level="5.6" data-path="distributed-systems.html"><a href="distributed-systems.html#spark-with-r-sql"><i class="fa fa-check"></i><b>5.6</b> Spark with R + SQL</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cloud-computing.html"><a href="cloud-computing.html"><i class="fa fa-check"></i><b>6</b> Cloud Computing</a>
<ul>
<li class="chapter" data-level="6.1" data-path="cloud-computing.html"><a href="cloud-computing.html#cloud-computing-basics-and-platforms"><i class="fa fa-check"></i><b>6.1</b> Cloud computing basics and platforms</a></li>
<li class="chapter" data-level="6.2" data-path="cloud-computing.html"><a href="cloud-computing.html#scaling-up-in-the-cloud"><i class="fa fa-check"></i><b>6.2</b> Scaling up in the cloud</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="cloud-computing.html"><a href="cloud-computing.html#scaling-up-with-aws-ec2-and-rrstudio"><i class="fa fa-check"></i><b>6.2.1</b> Scaling up with AWS EC2 and R/RStudio</a></li>
<li class="chapter" data-level="6.2.2" data-path="cloud-computing.html"><a href="cloud-computing.html#scaling-up-with-gpus"><i class="fa fa-check"></i><b>6.2.2</b> Scaling up with GPUs</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="cloud-computing.html"><a href="cloud-computing.html#gpus-on-google-colab"><i class="fa fa-check"></i><b>6.3</b> GPUs on Google Colab</a></li>
<li class="chapter" data-level="6.4" data-path="cloud-computing.html"><a href="cloud-computing.html#aws-emr-mapreduce-in-the-cloud"><i class="fa fa-check"></i><b>6.4</b> AWS EMR: MapReduce in the cloud</a></li>
</ul></li>
<li class="part"><span><b>III Applied Big Data Analytics</b></span></li>
<li class="chapter" data-level="7" data-path="forms-of-big-data-and-the-data-pipeline.html"><a href="forms-of-big-data-and-the-data-pipeline.html"><i class="fa fa-check"></i><b>7</b> Forms of Big Data and the Data Pipeline</a>
<ul>
<li class="chapter" data-level="7.1" data-path="forms-of-big-data-and-the-data-pipeline.html"><a href="forms-of-big-data-and-the-data-pipeline.html#unstructured-semi-structured-structured-data"><i class="fa fa-check"></i><b>7.1</b> Unstructured, semi-structured, structured data</a></li>
<li class="chapter" data-level="7.2" data-path="forms-of-big-data-and-the-data-pipeline.html"><a href="forms-of-big-data-and-the-data-pipeline.html#data-pipelines-a-systematic-approach-to-processing-big-data"><i class="fa fa-check"></i><b>7.2</b> Data pipelines: a systematic approach to processing big data</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html"><i class="fa fa-check"></i><b>8</b> Data Collection and Data Storage</a>
<ul>
<li class="chapter" data-level="8.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#gathering-and-compilation-of-raw-data"><i class="fa fa-check"></i><b>8.1</b> Gathering and compilation of raw data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#nyc-taxi-data"><i class="fa fa-check"></i><b>8.1.1</b> NYC taxi data</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#data-import-and-memory-allocation"><i class="fa fa-check"></i><b>8.2</b> Data import and memory allocation</a></li>
<li class="chapter" data-level="8.3" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#efficient-local-data-storage"><i class="fa fa-check"></i><b>8.3</b> Efficient local data storage</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#rdbms-basics"><i class="fa fa-check"></i><b>8.3.1</b> RDBMS basics</a></li>
<li class="chapter" data-level="8.3.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#efficient-data-access-indices-and-joins-in-sqlite"><i class="fa fa-check"></i><b>8.3.2</b> Efficient data access: indices and joins in SQLite</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#connecting-r-to-rdbms"><i class="fa fa-check"></i><b>8.4</b> Connecting R to RDBMS</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#creating-a-new-database-with-rsqlite"><i class="fa fa-check"></i><b>8.4.1</b> Creating a new database with <code>RSQLite</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#importing-data"><i class="fa fa-check"></i><b>8.4.2</b> Importing data</a></li>
<li class="chapter" data-level="8.4.3" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#issue-queries"><i class="fa fa-check"></i><b>8.4.3</b> Issue queries</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#cloud-solutions-for-big-data-storage"><i class="fa fa-check"></i><b>8.5</b> Cloud solutions for (big) data storage</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#easy-to-use-rdbms-in-the-cloud-aws-rds"><i class="fa fa-check"></i><b>8.5.1</b> Easy-to-use RDBMS in the cloud: AWS RDS</a></li>
<li class="chapter" data-level="8.5.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#database-server-in-the-cloud-mariadb-on-an-ec2-instance"><i class="fa fa-check"></i><b>8.5.2</b> Database server in the cloud: MariaDB on an EC2 instance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html"><i class="fa fa-check"></i><b>9</b> Big Data Cleaning and Transformation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#out-of-memory-strategies"><i class="fa fa-check"></i><b>9.1</b> ‘Out-of-memory’ strategies</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#chunking-data-with-the-ff-package"><i class="fa fa-check"></i><b>9.1.1</b> Chunking data with the <code>ff</code>-package</a></li>
<li class="chapter" data-level="9.1.2" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#memory-mapping-with-bigmemory"><i class="fa fa-check"></i><b>9.1.2</b> Memory mapping with <code>bigmemory</code></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#typical-cleaning-tasks"><i class="fa fa-check"></i><b>9.2</b> Typical cleaning tasks</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#data-preparation-with-ff"><i class="fa fa-check"></i><b>9.2.1</b> Data Preparation with <code>ff</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html"><i class="fa fa-check"></i><b>10</b> Descriptive Statistics and Aggregation</a>
<ul>
<li class="chapter" data-level="10.1" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#data-aggregation-the-split-apply-combine-strategy"><i class="fa fa-check"></i><b>10.1</b> Data aggregation: The ‘split-apply-combine’ strategy</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#data-aggregation-with-chunked-data-files"><i class="fa fa-check"></i><b>10.1.1</b> Data aggregation with chunked data files</a></li>
<li class="chapter" data-level="10.1.2" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#cross-tabulation-of-ff-vectors"><i class="fa fa-check"></i><b>10.1.2</b> Cross-tabulation of <code>ff</code> vectors</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#high-speed-in-memory-data-aggregation-with-data.table"><i class="fa fa-check"></i><b>10.2</b> High-speed in-memory data aggregation with <code>data.table</code></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="big-data-visualization.html"><a href="big-data-visualization.html"><i class="fa fa-check"></i><b>11</b> (Big) Data Visualization</a>
<ul>
<li class="chapter" data-level="11.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#data-exploration-with-ggplot2"><i class="fa fa-check"></i><b>11.1</b> Data exploration with <code>ggplot2</code></a></li>
<li class="chapter" data-level="11.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#excursus-modify-and-create-themes"><i class="fa fa-check"></i><b>11.2</b> Excursus: modify and create themes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#create-your-own-theme-simple-approach"><i class="fa fa-check"></i><b>11.2.1</b> Create your own theme: simple approach</a></li>
<li class="chapter" data-level="11.2.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#implementing-actual-themes-as-functions."><i class="fa fa-check"></i><b>11.2.2</b> Implementing actual themes as functions.</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="big-data-visualization.html"><a href="big-data-visualization.html#visualize-time-and-space"><i class="fa fa-check"></i><b>11.3</b> Visualize Time and Space</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#preparations"><i class="fa fa-check"></i><b>11.3.1</b> Preparations</a></li>
<li class="chapter" data-level="11.3.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#pick-up-and-drop-off-locations"><i class="fa fa-check"></i><b>11.3.2</b> Pick-up and drop-off locations</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="big-data-visualization.html"><a href="big-data-visualization.html#excursus-change-color-schemes"><i class="fa fa-check"></i><b>11.4</b> Excursus: change color schemes</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html"><i class="fa fa-check"></i><b>12</b> Bottle Necks in Local Big Data Analytics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#case-study-data-import-and-memory-allocation"><i class="fa fa-check"></i><b>12.1</b> Case study: Data Import and Memory Allocation</a></li>
<li class="chapter" data-level="12.2" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#case-study-loops-memory-and-vectorization"><i class="fa fa-check"></i><b>12.2</b> Case Study: Loops, Memory, and Vectorization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#preparation"><i class="fa fa-check"></i><b>12.2.1</b> Preparation</a></li>
<li class="chapter" data-level="12.2.2" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#naïve-approach-ignorant-of-r"><i class="fa fa-check"></i><b>12.2.2</b> Naïve Approach (ignorant of R)</a></li>
<li class="chapter" data-level="12.2.3" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#improvement-1-pre-allocation-of-memory"><i class="fa fa-check"></i><b>12.2.3</b> Improvement 1: Pre-allocation of memory</a></li>
<li class="chapter" data-level="12.2.4" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#improvement-2-exploit-vectorization"><i class="fa fa-check"></i><b>12.2.4</b> Improvement 2: Exploit vectorization</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#case-study-bootstrapping-and-parallel-processing"><i class="fa fa-check"></i><b>12.3</b> Case study: Bootstrapping and Parallel Processing</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#parallelization-with-an-ec2-instance-1"><i class="fa fa-check"></i><b>12.3.1</b> Parallelization with an EC2 instance</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#case-study-efficient-fixed-effects-estimation"><i class="fa fa-check"></i><b>12.4</b> Case Study: Efficient Fixed Effects Estimation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="gpus-and-machine-learning.html"><a href="gpus-and-machine-learning.html"><i class="fa fa-check"></i><b>13</b> GPUs and Machine Learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="gpus-and-machine-learning.html"><a href="gpus-and-machine-learning.html#tensorflowkeras-example-predict-housing-prices"><i class="fa fa-check"></i><b>13.1</b> Tensorflow/Keras example: predict housing prices</a></li>
<li class="chapter" data-level="13.2" data-path="gpus-and-machine-learning.html"><a href="gpus-and-machine-learning.html#data-preparation-1"><i class="fa fa-check"></i><b>13.2</b> Data preparation</a></li>
<li class="chapter" data-level="13.3" data-path="gpus-and-machine-learning.html"><a href="gpus-and-machine-learning.html#model-specification"><i class="fa fa-check"></i><b>13.3</b> Model specification</a></li>
<li class="chapter" data-level="13.4" data-path="gpus-and-machine-learning.html"><a href="gpus-and-machine-learning.html#training-and-prediction"><i class="fa fa-check"></i><b>13.4</b> Training and prediction</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="gpus-and-machine-learning.html"><a href="gpus-and-machine-learning.html#a-word-of-caution"><i class="fa fa-check"></i><b>13.4.1</b> A word of caution</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Application: Topics in Big Data Econometrics</b></span></li>
<li class="chapter" data-level="14" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html"><i class="fa fa-check"></i><b>14</b> Regression Analysis and Categorization with Spark and R</a>
<ul>
<li class="chapter" data-level="14.1" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#simple-regression-analysis"><i class="fa fa-check"></i><b>14.1</b> Simple regression analysis</a></li>
<li class="chapter" data-level="14.2" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#machine-learning-for-classification"><i class="fa fa-check"></i><b>14.2</b> Machine learning for classification</a></li>
<li class="chapter" data-level="14.3" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#building-machine-learning-pipelines-with-r-and-spark"><i class="fa fa-check"></i><b>14.3</b> Building machine learning pipelines with R and Spark</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#set-up-and-data-import"><i class="fa fa-check"></i><b>14.3.1</b> Set up and data import</a></li>
<li class="chapter" data-level="14.3.2" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#building-the-pipeline"><i class="fa fa-check"></i><b>14.3.2</b> Building the pipeline</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html"><i class="fa fa-check"></i><b>15</b> Large-scale Text Analysis with sparklyr</a>
<ul>
<li class="chapter" data-level="15.1" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#getting-started-import-preparation-and-word-frequencies"><i class="fa fa-check"></i><b>15.1</b> Getting started: import, preparation, and word frequencies</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i><b>A</b> Appendix A</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appendix-a.html"><a href="appendix-a.html#github"><i class="fa fa-check"></i><b>A.1</b> GitHub</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appendix-a.html"><a href="appendix-a.html#initiate-a-new-repository"><i class="fa fa-check"></i><b>A.1.1</b> Initiate a new repository</a></li>
<li class="chapter" data-level="A.1.2" data-path="appendix-a.html"><a href="appendix-a.html#clone-this-courses-repository"><i class="fa fa-check"></i><b>A.1.2</b> Clone this course’s repository</a></li>
<li class="chapter" data-level="A.1.3" data-path="appendix-a.html"><a href="appendix-a.html#fork-this-courses-repository"><i class="fa fa-check"></i><b>A.1.3</b> Fork this course’s repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i><b>B</b> Appendix B</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendix-b.html"><a href="appendix-b.html#data-types-and-memorystorage"><i class="fa fa-check"></i><b>B.1</b> Data types and memory/storage</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="appendix-b.html"><a href="appendix-b.html#example-in-r-data-types-and-information-storage"><i class="fa fa-check"></i><b>B.1.1</b> Example in R: Data types and information storage</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="appendix-b.html"><a href="appendix-b.html#data-structures"><i class="fa fa-check"></i><b>B.2</b> Data structures</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="appendix-b.html"><a href="appendix-b.html#vectors-vs-factors-in-r"><i class="fa fa-check"></i><b>B.2.1</b> Vectors vs Factors in R</a></li>
<li class="chapter" data-level="B.2.2" data-path="appendix-b.html"><a href="appendix-b.html#matricesarrays"><i class="fa fa-check"></i><b>B.2.2</b> Matrices/Arrays</a></li>
<li class="chapter" data-level="B.2.3" data-path="appendix-b.html"><a href="appendix-b.html#data-frames-tibbles-and-data-tables"><i class="fa fa-check"></i><b>B.2.3</b> Data frames, tibbles, and data tables</a></li>
<li class="chapter" data-level="B.2.4" data-path="appendix-b.html"><a href="appendix-b.html#lists"><i class="fa fa-check"></i><b>B.2.4</b> Lists</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="appendix-b.html"><a href="appendix-b.html#r-tools-to-investigate-structures-and-types"><i class="fa fa-check"></i><b>B.3</b> R-tools to investigate structures and types</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i><b>C</b> Appendix C</a>
<ul>
<li class="chapter" data-level="C.1" data-path="appendix-c.html"><a href="appendix-c.html#install-hadoop-on-ubuntu-linux"><i class="fa fa-check"></i><b>C.1</b> Install Hadoop (on Ubuntu Linux)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://umatter.github.io" target="blank">umatter.github.io</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hardware-computing-resources" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Hardware: Computing Resources<a href="hardware-computing-resources.html#hardware-computing-resources" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In order to better understand how we can use the available computing resources most efficiently in an analytics task, this chapter first briefly reviews the most basic hardware components and how they matter for computation. We then look at each of these components (and additional specialized components) through the lens of Big Data. That is, for each component, we look at how this component becomes a crucial bottleneck when processing large amounts of data and what we can do about it in R.</p>
<div id="components-of-a-standard-computing-environment" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Components of a standard computing environment<a href="hardware-computing-resources.html#components-of-a-standard-computing-environment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Figure <a href="#components"><strong>??</strong></a> illustrates the key components of a standard computing environment to process digital data. In our case, these components serve the purpose of computing a statistic, given a large dataset as input.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:components"></span>
<img src="img/computing_environment.png" alt="Basic components of a standard computing environment." width="99%" />
<p class="caption">
Figure 4.1: Basic components of a standard computing environment.
</p>
</div>

<ul>
<li><p><em>Mass storage</em> refers to the type of computer memory we use to store data in the long run. This is what we call the <em>hard drive</em> or <em>hard disk</em>.</p></li>
<li><p>In order to work with data (e.g., in R), it first has to be loaded into the <em>memory</em> of our computer. More specifically, into the random access memory (<em>RAM</em>). Typically, data is only loaded in the RAM for as long as we are working with it.</p></li>
<li><p>The component actually <em>processing</em> data is the central processing unit (CPU). When using R to process data, R commands are translated into complex combinations of a small sets of basic operations, which the <em>CPU</em> then executes.</p></li>
</ul>
<p>For this and the later chapters, consider the main difference between common ‘data analytics’ and ‘Big Data analytics’ to be the following: in a Big Data analytics context, the standard usage of one or several of the standard hardware components in your local computer fails to work or works very inefficiently because the amount of data overwhelms its normal capacity.</p>
<p>From the hardware perspective, there are two basic strategies to cope with the situation that one of these components is overwhelmed by the amount of data:</p>
<ul>
<li><em>Scale up (‘horizontal scaling’)</em>: Extend the physical capacity of the affected component by building a system with a large amount of RAM shared between applications. This sounds like a trivial solution (‘if RAM is too small, buy more RAM…’), but in practice it can be very expensive.</li>
<li><em>Scale out (‘vertical scaling’)</em>: Distribute the workload over several computers (or separate components of a system).</li>
</ul>
<p>From a software perspective, there are many (context-specific) strategies that can help us to use the resources available more efficiently in order to process large amounts of data. In the following sub-sections, we first get an idea of what we mean by capacity and <em>big</em> regarding the most important hardware components. First we focus on mass storage and memory, then on the CPU, and finally on new alternatives to the CPU.</p>
</div>
<div id="mass-storage" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Mass storage<a href="hardware-computing-resources.html#mass-storage" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a simple computing environment, the mass storage device (hard disk) is where the data is stored to be analyzed. So, in what units do we measure the size of datasets and consequently the mass storage capacity of a computer? The smallest unit of information in computing/digital data is called a <em>bit</em> (from <em>bi</em>nary dig<em>it</em>; abbrev. ‘b’) and can take one of two (symbolic) values, either a <code>0</code> or a <code>1</code> (“off” or “on”). Consider, for example, the decimal number <code>139</code>. Written in the binary system, <code>139</code> corresponds to the binary number <code>10001011</code>. In order to store this number on a hard disk, we require a capacity of 8 bits, or one <em>byte</em> (1 byte = 8 bits; abbrev. ‘B’). Historically, one byte encoded a single character of text (i.e., in the ASCII character encoding system). When thinking of a given dataset in its raw/binary representation, we can simply think of it as a row of <code>0</code>s and <code>1</code>s.</p>
<p>Bigger units for storage capacity usually build on bytes, for example:</p>
<ul>
<li><span class="math inline">\(1 \text{ kilobyte (KB)} = 1000^{1} \approx 2^{10} \text{ bytes}\)</span></li>
<li><span class="math inline">\(1 \text{ megabyte (MB)} = 1000^{2} \approx 2^{20} \text{ bytes}\)</span></li>
<li><span class="math inline">\(1 \text{ gigabyte (GB)} = 1000^{3} \approx 2^{30} \text{ bytes}\)</span></li>
</ul>
<p>Currently, a common laptop or desktop computer has several hundred GBs of mass storage capacity. The problems related to a lack of mass storage capacity in Big Data analytics are likely the easiest to understand. Suppose you collect large amounts of data from an online source such as the Twitter API (application programming interface). At some point, R will throw an error and stop the data collection procedure as the operating system will not allow R to use up more disk space. The simplest solution to this problem is to clean up your hard disk: empty the trash, archive files in the cloud or on an external drive and delete them on the main disk, etc. In addition, there are some easy-to-learn tricks to use from within R to save some disk space.</p>
<div id="avoid-redundancies" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Avoid redundancies<a href="hardware-computing-resources.html#avoid-redundancies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Different formats for structuring data stored on disk use up more or less space. A simple example is the comparison of JSON (JavaScript Object Notation) and CSV (Comma Separated Values) – both data structures that are widely used to store data for analytics purposes. JSON is much more flexible in that it allows the definition of arbitrarily complex hierarchical data structures (and even allows for hints at data types). However, this flexibility comes with some overhead in the usage of special characters to define the structure. Consider the following JSON excerpt of an economic time series fetched from the Federal Reserve’s <a href="https://fred.stlouisfed.org/docs/api/fred/series_observations.html#example_json">FRED API</a>.</p>
<pre><code>{
    &quot;realtime_start&quot;: &quot;2013-08-14&quot;,
    &quot;realtime_end&quot;: &quot;2013-08-14&quot;,
    &quot;observation_start&quot;: &quot;1776-07-04&quot;,
    &quot;observation_end&quot;: &quot;9999-12-31&quot;,
    &quot;units&quot;: &quot;lin&quot;,
    &quot;output_type&quot;: 1,
    &quot;file_type&quot;: &quot;json&quot;,
    &quot;order_by&quot;: &quot;observation_date&quot;,
    &quot;sort_order&quot;: &quot;asc&quot;,
    &quot;count&quot;: 84,
    &quot;offset&quot;: 0,
    &quot;limit&quot;: 100000,
    &quot;observations&quot;: [
        {
            &quot;realtime_start&quot;: &quot;2013-08-14&quot;,
            &quot;realtime_end&quot;: &quot;2013-08-14&quot;,
            &quot;date&quot;: &quot;1929-01-01&quot;,
            &quot;value&quot;: &quot;1065.9&quot;
        },
        {
            &quot;realtime_start&quot;: &quot;2013-08-14&quot;,
            &quot;realtime_end&quot;: &quot;2013-08-14&quot;,
            &quot;date&quot;: &quot;1930-01-01&quot;,
            &quot;value&quot;: &quot;975.5&quot;
        },
        ...,
        {
            &quot;realtime_start&quot;: &quot;2013-08-14&quot;,
            &quot;realtime_end&quot;: &quot;2013-08-14&quot;,
            &quot;date&quot;: &quot;2012-01-01&quot;,
            &quot;value&quot;: &quot;15693.1&quot;
        }
    ]
}</code></pre>
<p>The JSON format is very practical here in separating metadata (such as what time frame is covered by this dataset, etc.) in the first few lines on top from the actual data in <code>"observations"</code> further down. However, note that due to this structure, the key names like <code>"date"</code>, and <code>"value"</code> occur for each observation in that time series. In addition, <code>"realtime_start"</code> and <code>"realtime_end"</code> occur both in the metadata section and again in each observation. Each of those occurrences costs some bytes of storage space on your hard disk but does not add any information once you have parsed and imported the time series into R. The same information could also be stored in a more efficient way on your hard disk by simply storing the metadata in a separate text file and the actual observations in a CSV file (in a table-like structure):</p>
<pre><code>&quot;date&quot;,&quot;value&quot;
&quot;1929-01-01&quot;, &quot;1065.9&quot;
&quot;1930-01-01&quot;, &quot;975.5&quot;

...,

&quot;2012-01-01&quot;, 15693.1&quot;</code></pre>
<p>In fact, in that particular example, storing the data in JSON format would take up more than double the hard-disk space as CSV. Of course, this is not to say that one should generally store data in CSV files. In many situations, you might really have to rely on JSON’s flexibility to represent more complex structures. However, in practice it is very much worthwhile to think about whether you can improve storage efficiency by simply storing raw data in a different format.</p>
<p>Another related point to storing data in CSV files is to remove redundancies by splitting the data into several tables/CSV files, whereby each table contains the variables exclusively describing the type of observation in it. For example, when analyzing customer data for marketing purposes, the dataset stored in one CSV file might be at the level of individual purchases. That is, each row contains both information on what has been purchased on which day by which customer as well as additional variables describing the customer (such as customer id, name, address, etc.). Instead of keeping all of this data in one file, we could split it into two files, where one only contains the order ids and corresponding customer ids as well as attributes of individual orders (but not additional attributes of the customers themselves) and the other contains the customer ids and all customer attributes. Thereby, we avoid redundancies in the form of repeatedly storing the same values of customer attributes (like name and address) for each order.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
</div>
<div id="data-compression" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Data compression<a href="hardware-computing-resources.html#data-compression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Data compression essentially follows from the same basic idea of avoiding redundancies in data storage as the simple approaches discussed above. However, it happens on a much more fundamental level. Data compression algorithms encode the information contained in the original representation of the data with fewer bits. In the case of lossless compression, this results in a new data file containing the exact same information but taking up less space on disk. In simple terms, compression replaces repeatedly occurring sequences with shorter expressions and keeps track of replacements in a table. Based on the table, the file can then be de-compressed to recreate the original representation of the data. For example, consider the following character string.</p>
<pre><code>&quot;xxxxxyyyyyzzzz&quot;</code></pre>
<p>The same data could be represented with fewer bits as:</p>
<pre><code>&quot;5x6y4z&quot;</code></pre>
<p>which needs fewer than half the number of bits to be stored (but contains the same information).</p>
<p>There are several easy ways to use your mass storage capacity more efficiently with data compression in R. Most conveniently, some functions to import/export data in R directly allow for reading and writing of compressed formats. For example, the <code>fread()</code>/<code>fwrite()</code> functions provided in the <code>data.table</code> package will automatically use the GZIP (de-)compression utility when writing to (reading from) a CSV file with a <code>.gz</code> file extension in the file name.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="hardware-computing-resources.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb120-2"><a href="hardware-computing-resources.html#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb120-3"><a href="hardware-computing-resources.html#cb120-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-4"><a href="hardware-computing-resources.html#cb120-4" aria-hidden="true" tabindex="-1"></a><span class="co"># load example data from basic R installation</span></span>
<span id="cb120-5"><a href="hardware-computing-resources.html#cb120-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;LifeCycleSavings&quot;</span>)</span>
<span id="cb120-6"><a href="hardware-computing-resources.html#cb120-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-7"><a href="hardware-computing-resources.html#cb120-7" aria-hidden="true" tabindex="-1"></a><span class="co"># write data to normal csv file and check size</span></span>
<span id="cb120-8"><a href="hardware-computing-resources.html#cb120-8" aria-hidden="true" tabindex="-1"></a><span class="fu">fwrite</span>(LifeCycleSavings, <span class="at">file=</span><span class="st">&quot;lcs.csv&quot;</span>)</span>
<span id="cb120-9"><a href="hardware-computing-resources.html#cb120-9" aria-hidden="true" tabindex="-1"></a><span class="fu">file.size</span>(<span class="st">&quot;lcs.csv&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 1441</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="hardware-computing-resources.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="co"># write data to a GZIPped (compressed) csv file and check size</span></span>
<span id="cb122-2"><a href="hardware-computing-resources.html#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fwrite</span>(LifeCycleSavings, <span class="at">file=</span><span class="st">&quot;lcs.csv.gz&quot;</span>)</span>
<span id="cb122-3"><a href="hardware-computing-resources.html#cb122-3" aria-hidden="true" tabindex="-1"></a><span class="fu">file.size</span>(<span class="st">&quot;lcs.csv.gz&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 744</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="hardware-computing-resources.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read/import the compressed data</span></span>
<span id="cb124-2"><a href="hardware-computing-resources.html#cb124-2" aria-hidden="true" tabindex="-1"></a>lcs <span class="ot">&lt;-</span> data.table<span class="sc">::</span><span class="fu">fread</span>(<span class="st">&quot;lcs.csv.gz&quot;</span>)</span></code></pre></div>
<p>Alternatively, you can also use other types of data compression as follows.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="hardware-computing-resources.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="co"># common ZIP compression (independent of data.table package)</span></span>
<span id="cb125-2"><a href="hardware-computing-resources.html#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(LifeCycleSavings, <span class="at">file=</span><span class="st">&quot;lcs.csv&quot;</span>)</span>
<span id="cb125-3"><a href="hardware-computing-resources.html#cb125-3" aria-hidden="true" tabindex="-1"></a><span class="fu">file.size</span>(<span class="st">&quot;lcs.csv&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 1984</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="hardware-computing-resources.html#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">zip</span>(<span class="at">zipfile =</span> <span class="st">&quot;lcs.csv.zip&quot;</span>, <span class="at">files =</span>  <span class="st">&quot;lcs.csv&quot;</span>)</span>
<span id="cb127-2"><a href="hardware-computing-resources.html#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="fu">file.size</span>(<span class="st">&quot;lcs.csv.zip&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 1205</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="hardware-computing-resources.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># unzip/decompress and read/import data</span></span>
<span id="cb129-2"><a href="hardware-computing-resources.html#cb129-2" aria-hidden="true" tabindex="-1"></a>lcs_path <span class="ot">&lt;-</span> <span class="fu">unzip</span>(<span class="st">&quot;lcs.csv.zip&quot;</span>)</span>
<span id="cb129-3"><a href="hardware-computing-resources.html#cb129-3" aria-hidden="true" tabindex="-1"></a>lcs <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(lcs_path)</span></code></pre></div>
<p>Note that data compression is subject to a time–memory trade-off. Compression and de-compression is computationally intense and needs time. When using compression in order to make more efficient use of the available mass storage capacity, think about how frequently you expect the data to be loaded into R as part of the data analysis tasks ahead and for how long your will need to keep the data stored on your hard disk. Importing GBs of compressed data can be uncomfortably slower than importing from a decompressed file.</p>
<p>So far, we have only focused on data size in the context of mass storage capacity. But what happens once you load a large dataset into R (e.g., by means of <code>read.csv()</code>)? A program called a “parser” is executed that reads the raw data from the hard disk and creates a representation of that data in the R environment, that is, in random access memory (RAM). All common computers have more GBs of mass storage available than GBs of RAM. Hence, new issues of hardware capacity loom at the stage of data import, which brings us to the next subsection.</p>
</div>
</div>
<div id="random-access-memory-ram" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Random access memory (RAM)<a href="hardware-computing-resources.html#random-access-memory-ram" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Currently, a common laptop or desktop computer has 8–32 GB of RAM capacity. These are more-or-less the numbers you should keep in the back of your mind for the examples/discussions that follow. That is, we will consider a dataset as “big” if it takes up several GBs in RAM (and therefore might overwhelm a machine with 8GB RAM capacity).</p>
<p>There are several types of problems that you might run into in practice when attempting to import and analyze a dataset of the size close to or larger than your computer’s RAM capacity. First, importing the data might take much longer than expected, your computer might freeze during import (or later during the analysis), R/Rstudio might crash, or you might get an error message hinting at a lack of RAM. How can you anticipate such problems, and what can you do about them?</p>
<p>Many of the techniques and packages discussed in the following chapters are in one way or another solutions to these kinds of problems. However, there are a few relatively simple things to keep in mind before we go into the details.</p>
<ol style="list-style-type: decimal">
<li><p>The same data stored on the mass storage device (e.g., in a CSV file) might take up more or less space in RAM. This is due to the fact that the data is (technically speaking) structured differently in a CSV or JSON file than in, for example, a data table or a matrix in R. For example, it is reasonable to anticipate that the example JSON file with the economic time series data will take up less space as a time series object in R (in RAM) than it does on the hard disk (for one thing just simply due to the fact that we will not keep the redundancies mentioned before).</p></li>
<li><p>The import might work well, but some parts of the data analysis script might require much more memory to run through even without loading additional data from disk. A classic example of this is regression analysis performed with, for example, <code>lm()</code> in R. As part of the OLS estimation procedure, <code>lm</code> will need to create the model matrix (usually denoted <span class="math inline">\(X\)</span>). Depending on the model you want to estimate, the model matrix might actually be larger than the data frame containing the dataset. In fact, this can happen quite easily if you specify a fixed effects model in which you want to account for the fixed effects via dummy variables (for example, for each country except for one).<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> Again, the result can be one of several: an error message hinting at a lack of memory, a crash, or the computer slowing down significantly. Anticipating these types of problems is very tricky since memory problems are often caused at a lower level of a function from the package that provides you with the data analytics routine you intend to use. Accordingly, error messages can be rather cryptic.</p></li>
<li><p>Keep in mind that you have some leeway to guide how much space imported data takes up in R by considering data structures and data types. For example, you can use factors instead of character vectors when importing categorical variables into R (the default in <code>read.csv</code>), and for some operations it makes sense to work with matrices instead of data frames.</p></li>
</ol>
<p>Finally, recall the lessons regarding memory usage from the section “Writing efficient R code” in Chapter 1.</p>
</div>
<div id="combining-ram-and-hard-disk-virtual-memory" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Combining RAM and hard disk: virtual memory<a href="hardware-computing-resources.html#combining-ram-and-hard-disk-virtual-memory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What if all the RAM in our computer is not enough to store all the data we want to analyze?</p>
<p>Modern operating systems (OSs) have a way of dealing with such a situation. Once all RAM is used up by the currently running programs, the OS allocates parts of the memory back to the hard disk, which then works as <em>virtual memory</em>. Figure 4.2 illustrates this point.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:vm"></span>
<img src="img/virtual_memory.png" alt="Virtual memory. Overall memory is mapped to RAM and parts of the hard disk." width="75%" />
<p class="caption">
Figure 4.2: Virtual memory. Overall memory is mapped to RAM and parts of the hard disk.
</p>
</div>

<p>For example, when we implement an R-script that imports one file after the other into the R environment, ignoring the RAM capacity of our computer, the OS will start <em>paging</em> data to the virtual memory. This happens ‘under the hood’ without explicit instructions by the user. We will quite likely notice that the computer slows down a lot when this happens.</p>
<p>While this default usage of virtual memory by the OS is helpful for running several applications at the same time, each taking up a moderate amount of memory, it is not a really useful tool for processing large amounts of data in one application (R). However, the underlying idea of using both RAM and mass storage simultaneously in order to cope with a lack of memory is very useful in the context of Big Data analytics.</p>
<p>Several R packages have been developed that exploit the idea behind virtual memory explicitly for analyzing large amounts of data. The basic idea behind these packages is to map a dataset to the hard disk when loading it into R. The actual data values are stored in chunks on the hard disk, while the structure/metadata of the dataset is loaded into R.</p>
</div>
<div id="cpu-and-parallelization" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> CPU and parallelization<a href="hardware-computing-resources.html#cpu-and-parallelization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The actual processing of the data is done in the computer’s central processing unit (CPU). Consequently, the performance of the CPU has a substantial effect on how fast a data analytics task runs. A CPU’s performance is usually denoted by its <em>clock rate</em> measured in gigaherz (GHz). In simple terms, a CPU with a clock rate of 4.8 GHz can execute 4.8 billion basic operations per second. Holding all other aspects constant, you can thus expect an analytics task to run faster if it runs on a computer with higher CPU clock rate. Alternatively to scaling up the CPU, we can exploit the fact that modern CPUs have several <em>cores</em>. In the normal usage of a PC, the operating system makes use of these cores to run several applications smoothly <em>in parallel</em> (e.g., you listen to music on Spotify while browsing the web and running some analytics script in RStudio in the background).</p>
<p>Modern computing environments such as R allow us to explicitly run parts of the same analytics task in parallel, that is, on several CPU cores at the same time. Following the same logic, we can also connect several computers (each with several CPU cores) in a cluster computer and run the program in parallel on all of these computing nodes. Both of these approaches are generally referred to as <em>parallelization</em> and both are supported in several R packages.</p>
<p>An R program run in parallel typically involves the following steps</p>
<ul>
<li>First, several instances of R are running at the same time (across one machine with multiple CPU cores or across a cluster computer). One of the instances (i.e., the <em>master</em> instance) breaks the computation into batches and sends those to the other instances.</li>
<li>Second, each of the instances processes its batch and sends the results back to the master instance.</li>
<li>Finally, the master instance combines the partial results into the final result and returns it to the user.</li>
</ul>
<p>To illustrate this point, consider the following econometric problem: you have a customer <a href="https://www.kaggle.com/jackdaoud/marketing-data?select=marketing_data.csv">dataset</a> with detailed data on customer characteristics, past customer behavior, and information on online marketing campaigns. Your task is to figure out which customers are more likely to react positively to the most recent online marketing campaign. The aim is to optimize personalized marketing campaigns in the future based on insights gained from this exercise. In a first step you take a computationally intense “brute force” approach: you run all possible regressions with the dependent variable <code>Response</code> (equal to 1 if the customer took the offer in the campaign and 0 otherwise). In total you have 21 independent variables, thus you need to run <span class="math inline">\(2^20=1,048,576\)</span> logit regressions (this is without considering linear combinations of covariates etc.). Finally, you want to select the model with the best fit according to deviance.</p>
<p>A simple sequential implementation to solve this problem could look like this (for the sake of time, we cap the number of regression models to N=10).</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="hardware-computing-resources.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># you can download the dataset from </span></span>
<span id="cb130-2"><a href="hardware-computing-resources.html#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="co"># https://www.kaggle.com/jackdaoud/marketing-data?</span></span>
<span id="cb130-3"><a href="hardware-computing-resources.html#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="co"># select=marketing_data.csv</span></span>
<span id="cb130-4"><a href="hardware-computing-resources.html#cb130-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-5"><a href="hardware-computing-resources.html#cb130-5" aria-hidden="true" tabindex="-1"></a><span class="co"># PREPARATION -----------------------------</span></span>
<span id="cb130-6"><a href="hardware-computing-resources.html#cb130-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-7"><a href="hardware-computing-resources.html#cb130-7" aria-hidden="true" tabindex="-1"></a><span class="co"># packages</span></span>
<span id="cb130-8"><a href="hardware-computing-resources.html#cb130-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb130-9"><a href="hardware-computing-resources.html#cb130-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-10"><a href="hardware-computing-resources.html#cb130-10" aria-hidden="true" tabindex="-1"></a><span class="co"># import data</span></span>
<span id="cb130-11"><a href="hardware-computing-resources.html#cb130-11" aria-hidden="true" tabindex="-1"></a>marketing <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/marketing_data.csv&quot;</span>)</span>
<span id="cb130-12"><a href="hardware-computing-resources.html#cb130-12" aria-hidden="true" tabindex="-1"></a><span class="co"># clean/prepare data</span></span>
<span id="cb130-13"><a href="hardware-computing-resources.html#cb130-13" aria-hidden="true" tabindex="-1"></a>marketing<span class="sc">$</span>Income <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">gsub</span>(<span class="st">&quot;[[:punct:]]&quot;</span>,</span>
<span id="cb130-14"><a href="hardware-computing-resources.html#cb130-14" aria-hidden="true" tabindex="-1"></a>                                    <span class="st">&quot;&quot;</span>,</span>
<span id="cb130-15"><a href="hardware-computing-resources.html#cb130-15" aria-hidden="true" tabindex="-1"></a>                                    marketing<span class="sc">$</span>Income)) </span>
<span id="cb130-16"><a href="hardware-computing-resources.html#cb130-16" aria-hidden="true" tabindex="-1"></a>marketing<span class="sc">$</span>days_customer <span class="ot">&lt;-</span> </span>
<span id="cb130-17"><a href="hardware-computing-resources.html#cb130-17" aria-hidden="true" tabindex="-1"></a>     <span class="fu">as.Date</span>(<span class="fu">Sys.Date</span>())<span class="sc">-</span> </span>
<span id="cb130-18"><a href="hardware-computing-resources.html#cb130-18" aria-hidden="true" tabindex="-1"></a>     <span class="fu">as.Date</span>(marketing<span class="sc">$</span>Dt_Customer, <span class="st">&quot;%m/%d/%y&quot;</span>)</span>
<span id="cb130-19"><a href="hardware-computing-resources.html#cb130-19" aria-hidden="true" tabindex="-1"></a>marketing<span class="sc">$</span>Dt_Customer <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb130-20"><a href="hardware-computing-resources.html#cb130-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-21"><a href="hardware-computing-resources.html#cb130-21" aria-hidden="true" tabindex="-1"></a><span class="co"># all sets of independent vars</span></span>
<span id="cb130-22"><a href="hardware-computing-resources.html#cb130-22" aria-hidden="true" tabindex="-1"></a>indep <span class="ot">&lt;-</span> <span class="fu">names</span>(marketing)[ <span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">19</span>, <span class="dv">27</span>,<span class="dv">28</span>)]</span>
<span id="cb130-23"><a href="hardware-computing-resources.html#cb130-23" aria-hidden="true" tabindex="-1"></a>combinations_list <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(indep),</span>
<span id="cb130-24"><a href="hardware-computing-resources.html#cb130-24" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">function</span>(x) <span class="fu">combn</span>(indep, x,</span>
<span id="cb130-25"><a href="hardware-computing-resources.html#cb130-25" aria-hidden="true" tabindex="-1"></a>                                              <span class="at">simplify =</span> <span class="cn">FALSE</span>))</span>
<span id="cb130-26"><a href="hardware-computing-resources.html#cb130-26" aria-hidden="true" tabindex="-1"></a>combinations_list <span class="ot">&lt;-</span> <span class="fu">unlist</span>(combinations_list, </span>
<span id="cb130-27"><a href="hardware-computing-resources.html#cb130-27" aria-hidden="true" tabindex="-1"></a>                            <span class="at">recursive =</span> <span class="cn">FALSE</span>)</span>
<span id="cb130-28"><a href="hardware-computing-resources.html#cb130-28" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">lapply</span>(combinations_list,</span>
<span id="cb130-29"><a href="hardware-computing-resources.html#cb130-29" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">function</span>(x) <span class="fu">paste</span>(<span class="st">&quot;Response ~&quot;</span>, </span>
<span id="cb130-30"><a href="hardware-computing-resources.html#cb130-30" aria-hidden="true" tabindex="-1"></a>                                   <span class="fu">paste</span>(x, <span class="at">collapse=</span><span class="st">&quot;+&quot;</span>)))</span>
<span id="cb130-31"><a href="hardware-computing-resources.html#cb130-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-32"><a href="hardware-computing-resources.html#cb130-32" aria-hidden="true" tabindex="-1"></a><span class="co"># COMPUTE REGRESSIONS --------------------------</span></span>
<span id="cb130-33"><a href="hardware-computing-resources.html#cb130-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-34"><a href="hardware-computing-resources.html#cb130-34" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co">#  N &lt;- length(models) for all</span></span>
<span id="cb130-35"><a href="hardware-computing-resources.html#cb130-35" aria-hidden="true" tabindex="-1"></a>pseudo_Rsq <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb130-36"><a href="hardware-computing-resources.html#cb130-36" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(pseudo_Rsq) <span class="ot">&lt;-</span> N</span>
<span id="cb130-37"><a href="hardware-computing-resources.html#cb130-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb130-38"><a href="hardware-computing-resources.html#cb130-38" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit the logit model via maximum likelihood</span></span>
<span id="cb130-39"><a href="hardware-computing-resources.html#cb130-39" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(models[[i]], <span class="at">data=</span>marketing, <span class="at">family =</span> <span class="fu">binomial</span>())</span>
<span id="cb130-40"><a href="hardware-computing-resources.html#cb130-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the proportion of deviance explained by </span></span>
<span id="cb130-41"><a href="hardware-computing-resources.html#cb130-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the independent vars (~R^2)</span></span>
<span id="cb130-42"><a href="hardware-computing-resources.html#cb130-42" aria-hidden="true" tabindex="-1"></a>  pseudo_Rsq[[i]] <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(fit<span class="sc">$</span>deviance<span class="sc">/</span>fit<span class="sc">$</span>null.deviance)</span>
<span id="cb130-43"><a href="hardware-computing-resources.html#cb130-43" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb130-44"><a href="hardware-computing-resources.html#cb130-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-45"><a href="hardware-computing-resources.html#cb130-45" aria-hidden="true" tabindex="-1"></a><span class="co"># SELECT THE WINNER ---------------</span></span>
<span id="cb130-46"><a href="hardware-computing-resources.html#cb130-46" aria-hidden="true" tabindex="-1"></a>models[[<span class="fu">which.max</span>(pseudo_Rsq)]]</span></code></pre></div>
<pre><code>## [1] &quot;Response ~ MntWines&quot;</code></pre>
<div id="naive-multi-session-approach" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Naive multi-session approach<a href="hardware-computing-resources.html#naive-multi-session-approach" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is actually a simple way of doing this “manually” on a multi-core PC, which intuitively illustrates the point of parallelization (although it would not be a very practical approach): you write an R script that loads the dataset, runs the fist <span class="math inline">\(n\)</span> of the total of <span class="math inline">\(N\)</span> regressions and stores the result in a local text file. Next, you run the script in your current RStudio session, open an additional RStudio session and run the script with the next <span class="math inline">\(n\)</span> regressions, and so on until all cores are occupied with one RStudio session. At the end you collect all of the results from the separate text files and combine them to get the final result. Depending on the problem at hand, this could indeed speed up the overall task, and it is technically speaking a form of “multi-session” approach. However, as you have surely noticed, this is unlikely to be a very practical approach.</p>
</div>
<div id="multi-core-and-multi-node-approach" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Multi-core and multi-node approach<a href="hardware-computing-resources.html#multi-core-and-multi-node-approach" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A more practical approach is to write one R script (with the help of some specialized packages) that instructs R to automatically distribute the batches to different cores (or different computing nodes in a cluster computer), control and monitor the progress in all cores, and then automatically collect and combine the results from all cores. There are several approaches to achieve this in R.</p>
<div id="parallel-for-loops-using-socket" class="section level4 hasAnchor" number="4.5.2.1">
<h4><span class="header-section-number">4.5.2.1</span> Parallel for-loops using socket<a href="hardware-computing-resources.html#parallel-for-loops-using-socket" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Likely the most intuitive approach to parallelizing a task in R is the <code>foreach</code> package. It allows you to write a <code>foreach</code> statement that is very similar to the for-loop syntax in R. Hence, you can straightforwardly “translate” an already implemented sequential approach with a common for-loop to a parallel implementation.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="hardware-computing-resources.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># COMPUTE REGRESSIONS IN PARALLEL (MULTI-CORE) --------------------------</span></span>
<span id="cb132-2"><a href="hardware-computing-resources.html#cb132-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-3"><a href="hardware-computing-resources.html#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="co"># packages for parallel processing</span></span>
<span id="cb132-4"><a href="hardware-computing-resources.html#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(parallel)</span>
<span id="cb132-5"><a href="hardware-computing-resources.html#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doSNOW)</span>
<span id="cb132-6"><a href="hardware-computing-resources.html#cb132-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-7"><a href="hardware-computing-resources.html#cb132-7" aria-hidden="true" tabindex="-1"></a><span class="co"># get the number of cores available</span></span>
<span id="cb132-8"><a href="hardware-computing-resources.html#cb132-8" aria-hidden="true" tabindex="-1"></a>ncores <span class="ot">&lt;-</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>()</span>
<span id="cb132-9"><a href="hardware-computing-resources.html#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="co"># set cores for parallel processing</span></span>
<span id="cb132-10"><a href="hardware-computing-resources.html#cb132-10" aria-hidden="true" tabindex="-1"></a>ctemp <span class="ot">&lt;-</span> <span class="fu">makeCluster</span>(ncores)</span>
<span id="cb132-11"><a href="hardware-computing-resources.html#cb132-11" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoSNOW</span>(ctemp)</span>
<span id="cb132-12"><a href="hardware-computing-resources.html#cb132-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-13"><a href="hardware-computing-resources.html#cb132-13" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare loop</span></span>
<span id="cb132-14"><a href="hardware-computing-resources.html#cb132-14" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#  N &lt;- length(models) for all</span></span>
<span id="cb132-15"><a href="hardware-computing-resources.html#cb132-15" aria-hidden="true" tabindex="-1"></a><span class="co"># run loop in parallel</span></span>
<span id="cb132-16"><a href="hardware-computing-resources.html#cb132-16" aria-hidden="true" tabindex="-1"></a>pseudo_Rsq <span class="ot">&lt;-</span></span>
<span id="cb132-17"><a href="hardware-computing-resources.html#cb132-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">foreach</span> ( <span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span>N, <span class="at">.combine =</span> c) <span class="sc">%dopar%</span> {</span>
<span id="cb132-18"><a href="hardware-computing-resources.html#cb132-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit the logit model via maximum likelihood</span></span>
<span id="cb132-19"><a href="hardware-computing-resources.html#cb132-19" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(models[[i]], </span>
<span id="cb132-20"><a href="hardware-computing-resources.html#cb132-20" aria-hidden="true" tabindex="-1"></a>               <span class="at">data=</span>marketing,</span>
<span id="cb132-21"><a href="hardware-computing-resources.html#cb132-21" aria-hidden="true" tabindex="-1"></a>               <span class="at">family =</span> <span class="fu">binomial</span>())</span>
<span id="cb132-22"><a href="hardware-computing-resources.html#cb132-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the proportion of deviance explained by </span></span>
<span id="cb132-23"><a href="hardware-computing-resources.html#cb132-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the independent vars (~R^2)</span></span>
<span id="cb132-24"><a href="hardware-computing-resources.html#cb132-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="dv">1</span><span class="sc">-</span>(fit<span class="sc">$</span>deviance<span class="sc">/</span>fit<span class="sc">$</span>null.deviance))</span>
<span id="cb132-25"><a href="hardware-computing-resources.html#cb132-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb132-26"><a href="hardware-computing-resources.html#cb132-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-27"><a href="hardware-computing-resources.html#cb132-27" aria-hidden="true" tabindex="-1"></a><span class="co"># SELECT THE WINNER ---------------</span></span>
<span id="cb132-28"><a href="hardware-computing-resources.html#cb132-28" aria-hidden="true" tabindex="-1"></a>models[[<span class="fu">which.max</span>(pseudo_Rsq)]]</span></code></pre></div>
<pre><code>## [1] &quot;Response ~ Year_Birth+Teenhome+Recency+MntWines+days_customer&quot;</code></pre>
<p>With relatively few cases, this approach is not very fast due to the overhead of “distributing” variables/objects from the master process to all cores/workers. In simple terms, the socket approach means that the cores do not share the same variables/the same environment, which creates overhead. However, this approach is usually very stable and runs on all platforms.</p>
</div>
<div id="parallel-lapply-using-forking" class="section level4 hasAnchor" number="4.5.2.2">
<h4><span class="header-section-number">4.5.2.2</span> Parallel lapply using forking<a href="hardware-computing-resources.html#parallel-lapply-using-forking" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Finally, let us look at the implementation based on forking. In the fork approach, each core works with the same objects/variables in a shared environment, which makes this approach very fast. However, depending on what exactly is being computed, sharing an environment can cause problems.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> If you are not sure whether your set up might run into issues with forking, it would be better to rely on a non-fork approach.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="hardware-computing-resources.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="co"># COMPUTE REGRESSIONS IN PARALLEL (MULTI-CORE) ---------------</span></span>
<span id="cb134-2"><a href="hardware-computing-resources.html#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="hardware-computing-resources.html#cb134-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-4"><a href="hardware-computing-resources.html#cb134-4" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare parallel lapply (based on forking, </span></span>
<span id="cb134-5"><a href="hardware-computing-resources.html#cb134-5" aria-hidden="true" tabindex="-1"></a><span class="co"># here clearly faster than foreach)</span></span>
<span id="cb134-6"><a href="hardware-computing-resources.html#cb134-6" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#  N &lt;- length(models) for all</span></span>
<span id="cb134-7"><a href="hardware-computing-resources.html#cb134-7" aria-hidden="true" tabindex="-1"></a><span class="co"># run parallel lapply</span></span>
<span id="cb134-8"><a href="hardware-computing-resources.html#cb134-8" aria-hidden="true" tabindex="-1"></a>pseudo_Rsq <span class="ot">&lt;-</span> <span class="fu">mclapply</span>(<span class="dv">1</span><span class="sc">:</span>N,</span>
<span id="cb134-9"><a href="hardware-computing-resources.html#cb134-9" aria-hidden="true" tabindex="-1"></a>                       <span class="at">mc.cores =</span> ncores,</span>
<span id="cb134-10"><a href="hardware-computing-resources.html#cb134-10" aria-hidden="true" tabindex="-1"></a>                       <span class="at">FUN =</span> <span class="cf">function</span>(i){</span>
<span id="cb134-11"><a href="hardware-computing-resources.html#cb134-11" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># fit the logit model </span></span>
<span id="cb134-12"><a href="hardware-computing-resources.html#cb134-12" aria-hidden="true" tabindex="-1"></a>                         fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(models[[i]],</span>
<span id="cb134-13"><a href="hardware-computing-resources.html#cb134-13" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">data=</span>marketing,</span>
<span id="cb134-14"><a href="hardware-computing-resources.html#cb134-14" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">family =</span> <span class="fu">binomial</span>())</span>
<span id="cb134-15"><a href="hardware-computing-resources.html#cb134-15" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># compute the proportion of deviance </span></span>
<span id="cb134-16"><a href="hardware-computing-resources.html#cb134-16" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># explained  by the independent vars (~R^2)</span></span>
<span id="cb134-17"><a href="hardware-computing-resources.html#cb134-17" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">return</span>(<span class="dv">1</span><span class="sc">-</span>(fit<span class="sc">$</span>deviance<span class="sc">/</span>fit<span class="sc">$</span>null.deviance))</span>
<span id="cb134-18"><a href="hardware-computing-resources.html#cb134-18" aria-hidden="true" tabindex="-1"></a>                         })</span>
<span id="cb134-19"><a href="hardware-computing-resources.html#cb134-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-20"><a href="hardware-computing-resources.html#cb134-20" aria-hidden="true" tabindex="-1"></a><span class="co"># SELECT THE WINNER, SHOW FINAL OUTPUT ---------------</span></span>
<span id="cb134-21"><a href="hardware-computing-resources.html#cb134-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-22"><a href="hardware-computing-resources.html#cb134-22" aria-hidden="true" tabindex="-1"></a>best_model <span class="ot">&lt;-</span> models[[<span class="fu">which.max</span>(pseudo_Rsq)]]</span>
<span id="cb134-23"><a href="hardware-computing-resources.html#cb134-23" aria-hidden="true" tabindex="-1"></a>best_model</span></code></pre></div>
<pre><code>## [1] &quot;Response ~ Year_Birth+Teenhome+Recency+MntWines+days_customer&quot;</code></pre>
<!-- #### Parallelization based on futures -->
<!-- https://github.com/HenrikBengtsson/future -->
<!-- https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html -->
</div>
</div>
</div>
<div id="gpus-for-scientific-computing" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> GPUs for scientific computing<a href="hardware-computing-resources.html#gpus-for-scientific-computing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The success of the computer games industry in the late 1990s/early 2000s led to an interesting positive externality for scientific computing. The ever more demanding graphics of modern computer games and the huge economic success of the computer games industry set incentives for hardware producers to invest in research and development of more powerful ‘graphic cards’, extending a normal PC/computing environment with additional computing power solely dedicated to graphics. At the heart of these graphic cards are so-called GPUs (graphic processing units), microprocessors specifically optimized for graphics processing. Figure 4.3 depicts a modern graphics card similar to those commonly built into today’s ‘gaming’ PCs.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gpu"></span>
<img src="img/GPU.png" alt="Illustration of a graphic processing unit (GPU)." width="40%" />
<p class="caption">
Figure 4.3: Illustration of a graphic processing unit (GPU).
</p>
</div>

<p>Why did the hardware industry not simply invest in the development of more powerful CPUs to deal with the more demanding PC games? The main reason is that the architecture of CPUs is designed not only for efficiency but also flexibility. That is, a CPU needs to perform well in all kinds of computations, some parallel, some sequential, etc. Computing graphics is a comparatively narrow domain of computation, and designing a processing unit architecture that is custom-made to excel just at this one task is thus much more cost efficient. Interestingly, this graphics-specific architecture (specialized in highly parallel numerical [floating point] workloads) turns out to also be very useful in some core scientific computing tasks – in particular, matrix multiplications (see <span class="citation">Fatahalian, Sugerman, and Hanrahan (<a href="#ref-fatahalian_etal2004" role="doc-biblioref">2004</a>)</span> for a detailed discussion of why that is the case). A key aspect of GPUs is that they are composed of several multiprocessor units, of which each in turn has several cores. GPUs can thus perform computations with hundreds or even thousands of threads in parallel. The figure below illustrates this point by showing the typical architecture of a NVIDIA GPU.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gpuarchitecture"></span>
<img src="img/gpu_details.png" alt="Illustration of a graphic processing unit (GPU)." width="95%" />
<p class="caption">
Figure 4.4: Illustration of a graphic processing unit (GPU).
</p>
</div>

<p>While, initially, programming GPUs for scientific computing required a very good understanding of the hardware, graphics card producers have realized that there is an additional market for their products (in particular with the recent rise of deep learning) and now provide several high-level APIs to use GPUs for tasks other than graphics processing. Over the last few years, more high-level software has been developed that makes it much easier to use GPUs in parallel computing tasks. The following subsections show some examples of such software in the R environment.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></p>
<div id="gpus-in-r" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> GPUs in R<a href="hardware-computing-resources.html#gpus-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- ## Installation -->
<!-- This is for pop OS machines. Install drivers etc. for NVIDIA card -->
<!-- ```{bash eval=FALSE} -->
<!-- # sudo apt install tensorflow-cuda-latest -->
<!-- ``` -->
<!-- Install OpenCL -->
<!-- ```{bash eval=FALSE} -->
<!-- # sudo apt install tensorflow-cuda-latest -->
<!-- ``` -->
<!-- Install `gpuR` in R (`install.packages("gpuR")`). -->
<div id="gpu-computing-example-matrix-multiplication-comparison-gpur" class="section level4 hasAnchor" number="4.6.1.1">
<h4><span class="header-section-number">4.6.1.1</span> GPU computing example: Matrix multiplication comparison (<code>gpuR</code>)<a href="hardware-computing-resources.html#gpu-computing-example-matrix-multiplication-comparison-gpur" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <code>gpuR</code> package provides basic R functions to compute with GPUs from within the R environment. In the following example we compare the performance of a CPU with a GPU based on a matrix multiplication exercise. For a large <span class="math inline">\(N\times P\)</span> matrix <span class="math inline">\(X\)</span>, we want to compute <span class="math inline">\(X^tX\)</span>.</p>
<p>In a first step, we load the <code>gpuR</code>-package.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a> Note the output to the console. It shows the type of GPU identified by <code>gpuR</code>. This is the platform on which <code>gpuR</code> will compute the GPU examples. In order to compare the performances, we also load the <code>bench</code> package.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="hardware-computing-resources.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load package</span></span>
<span id="cb136-2"><a href="hardware-computing-resources.html#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bench)</span>
<span id="cb136-3"><a href="hardware-computing-resources.html#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gpuR)</span></code></pre></div>
<pre><code>## Number of platforms: 1
## - platform: NVIDIA Corporation: OpenCL 3.0 CUDA 11.6.134
##   - context device index: 0
##     - NVIDIA GeForce GTX 1650
## checked all devices
## completed initialization</code></pre>
<p>Next, we initialize a large matrix filled with pseudo-random numbers, representing a dataset with <span class="math inline">\(N\)</span> observations and <span class="math inline">\(P\)</span> variables.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="hardware-computing-resources.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize dataset with pseudo-random numbers</span></span>
<span id="cb138-2"><a href="hardware-computing-resources.html#cb138-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10000</span>  <span class="co"># number of observations</span></span>
<span id="cb138-3"><a href="hardware-computing-resources.html#cb138-3" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># number of variables</span></span>
<span id="cb138-4"><a href="hardware-computing-resources.html#cb138-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> P, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">nrow =</span> N, <span class="at">ncol =</span>P)</span></code></pre></div>
<p>For the GPU examples to work, we need one more preparatory step. GPUs have their own memory, which they can access faster than they can access RAM. However, this GPU memory is typically not very large compared to the memory CPUs have access to. Hence, there is a potential trade-off between losing some efficiency but working with more data or vice versa.<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a> Here, we show both variants. With <code>gpuMatrix()</code> we create an object representing matrix <code>X</code> for computation on the GPU. However, this only points the GPU to the matrix and does not actually transfer data to the GPU’s memory. The latter is done in the other variant, with <code>vclMatrix()</code>.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="hardware-computing-resources.html#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare GPU-specific objects/settings</span></span>
<span id="cb139-2"><a href="hardware-computing-resources.html#cb139-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-3"><a href="hardware-computing-resources.html#cb139-3" aria-hidden="true" tabindex="-1"></a><span class="co"># point GPU to matrix (matrix stored in non-GPU memory)</span></span>
<span id="cb139-4"><a href="hardware-computing-resources.html#cb139-4" aria-hidden="true" tabindex="-1"></a>gpuX <span class="ot">&lt;-</span> <span class="fu">gpuMatrix</span>(X, <span class="at">type =</span> <span class="st">&quot;float&quot;</span>)</span>
<span id="cb139-5"><a href="hardware-computing-resources.html#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="co"># transfer matrix to GPU (matrix stored in GPU memory)</span></span>
<span id="cb139-6"><a href="hardware-computing-resources.html#cb139-6" aria-hidden="true" tabindex="-1"></a>vclX <span class="ot">&lt;-</span> <span class="fu">vclMatrix</span>(X, <span class="at">type =</span> <span class="st">&quot;float&quot;</span>)  </span></code></pre></div>
<p>Now we run the three examples: first, based on standard R, using the CPU. Then, computing on the GPU but using CPU memory. And finally, computing on the GPU and using GPU memory. In order to make the comparison fair, we force <code>bench::mark()</code> to run at least 20 iterations per benchmarked variant.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="hardware-computing-resources.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare three approaches</span></span>
<span id="cb140-2"><a href="hardware-computing-resources.html#cb140-2" aria-hidden="true" tabindex="-1"></a>(gpu_cpu <span class="ot">&lt;-</span> bench<span class="sc">::</span><span class="fu">mark</span>(</span>
<span id="cb140-3"><a href="hardware-computing-resources.html#cb140-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb140-4"><a href="hardware-computing-resources.html#cb140-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute with CPU </span></span>
<span id="cb140-5"><a href="hardware-computing-resources.html#cb140-5" aria-hidden="true" tabindex="-1"></a>  cpu <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X,</span>
<span id="cb140-6"><a href="hardware-computing-resources.html#cb140-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb140-7"><a href="hardware-computing-resources.html#cb140-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># GPU version, GPU pointer to CPU memory </span></span>
<span id="cb140-8"><a href="hardware-computing-resources.html#cb140-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (gpuMatrix is simply a pointer)</span></span>
<span id="cb140-9"><a href="hardware-computing-resources.html#cb140-9" aria-hidden="true" tabindex="-1"></a>  gpu1_pointer <span class="ot">&lt;-</span> <span class="fu">t</span>(gpuX) <span class="sc">%*%</span> gpuX,</span>
<span id="cb140-10"><a href="hardware-computing-resources.html#cb140-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb140-11"><a href="hardware-computing-resources.html#cb140-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># GPU version, in GPU memory </span></span>
<span id="cb140-12"><a href="hardware-computing-resources.html#cb140-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (vclMatrix formation is a memory transfer)</span></span>
<span id="cb140-13"><a href="hardware-computing-resources.html#cb140-13" aria-hidden="true" tabindex="-1"></a>  gpu2_memory <span class="ot">&lt;-</span> <span class="fu">t</span>(vclX) <span class="sc">%*%</span> vclX,</span>
<span id="cb140-14"><a href="hardware-computing-resources.html#cb140-14" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb140-15"><a href="hardware-computing-resources.html#cb140-15" aria-hidden="true" tabindex="-1"></a><span class="at">check =</span> <span class="cn">FALSE</span>, <span class="at">memory =</span> <span class="cn">FALSE</span>, <span class="at">min_iterations =</span> <span class="dv">20</span>))</span></code></pre></div>
<pre><code>## # A tibble: 3 × 6
##   expression                            min   median
##   &lt;bch:expr&gt;                       &lt;bch:tm&gt; &lt;bch:tm&gt;
## 1 cpu &lt;- t(X) %*% X                    69ms   76.2ms
## 2 gpu1_pointer &lt;- t(gpuX) %*% gpuX   32.8ms   33.4ms
## 3 gpu2_memory &lt;- t(vclX) %*% vclX      10ms   14.9ms
## # … with 3 more variables: `itr/sec` &lt;dbl&gt;,
## #   mem_alloc &lt;bch:byt&gt;, `gc/sec` &lt;dbl&gt;
## # ℹ Use `colnames()` to see all variable names</code></pre>
<p>The performance comparison is visualized with boxplots.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="hardware-computing-resources.html#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gpu_cpu, <span class="at">type =</span> <span class="st">&quot;boxplot&quot;</span>)</span></code></pre></div>
<p><img src="bigdata_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<p>The theoretically expected pattern becomes clearly visible. When using the GPU + GPU memory, the matrix operation takes on average less than 20 ms, with GPU + RAM over 30 ms, and with the common CPU operation close to 100 ms to finish. In the chapters on applied data analysis, we will look at some applications of GPUs in the domain of deep learning (which relies heavily on matrix multiplications).</p>
</div>
</div>
</div>
<div id="the-road-ahead-hardware-made-for-machine-learning" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> The road ahead: Hardware made for machine learning<a href="hardware-computing-resources.html#the-road-ahead-hardware-made-for-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Due to the high demand for more computational power in the domain of training complex neural network models (for example, in computer vision), Google has recently developed a new hardware platform specifically designed to work with complex neural networks using TensorFlow: Tensor Processing Units (TPUs). TPUs were designed from the ground up to improve performance in dense vector and matrix computations with the aim of substantially increasing the speed of training deep learning models implemented with TensorFlow.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tpu"></span>
<img src="img/TPU.png" alt="Illustration of a tensor processing unit (TPU)." width="40%" />
<p class="caption">
Figure 4.5: Illustration of a tensor processing unit (TPU).
</p>
</div>

<p>While initially only used internally by Google, the Google Cloud platform now offers cloud TPUs to the general public.</p>
</div>
<div id="insufficient-computing-resources" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Insufficient computing resources?<a href="hardware-computing-resources.html#insufficient-computing-resources" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When working with very large datasets (i.e., terabytes of data), processing the data on one common computer might not work due to a lack of memory or would be way too slow due to a lack of computing power (CPU cores). The architecture or basic hardware set up of a common computer is subject to a limited amount of RAM and a limited number of CPUs/CPU cores. Hence simply scaling up might not be sufficient. Instead we need to scale out. In simple terms, this means connecting several computers (each with their own RAM, CPU, and mass storage) in a network, distributing the dataset across all computers (“nodes”) in this network, and working on the data simultaneously across all nodes. In the next chapter, we look into how such ``distributed systems’’ basically work, what software frameworks are commonly used to work on distributed systems, and how we can interact with this software (and the distributed system) via R and SQL.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-fatahalian_etal2004" class="csl-entry">
Fatahalian, K., J. Sugerman, and P. Hanrahan. 2004. <span>“Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication.”</span> In <em>Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware</em>, 133–37. HWWS ’04. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/1058129.1058148">https://doi.org/10.1145/1058129.1058148</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p>This concept of organizing data into several tables is the basis of relational database management systems, which we will look at in more detail in Chapter 5. However, the basic idea is also very useful for storing raw data efficiently even if there is no intention to later build a database and run SQL queries on it.<a href="hardware-computing-resources.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>For example, if you specify something like <code>lm(y~x1 + x2 + country, data=mydata)</code> and <code>country</code> is a categorical variable (factor).<a href="hardware-computing-resources.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>Also, this approach does not work on Windows machines (see <code>?mclapply</code> for details).<a href="hardware-computing-resources.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>Note that while these examples are easy to implement and run, setting up a GPU for scientific computing still can involve many steps and some knowledge of your computer’s system. The examples presuppose that all installation and configuration steps (GPU drivers, CUDA, etc.) have already been completed successfully.<a href="hardware-computing-resources.html#fnref22" class="footnote-back">↩︎</a></p></li>
<li id="fn23"><p>As with setting up GPUs on your machine in general, installing all prerequisites to make <code>gpuR</code> work on your local machine can be a bit of work and can depend a lot on your system.<a href="hardware-computing-resources.html#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p>If we instruct the GPU to use its own memory but the data does not fit in it, the program will result in an error.<a href="hardware-computing-resources.html#fnref24" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="software-programming-with-big-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distributed-systems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
