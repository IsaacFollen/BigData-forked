<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Distributed Systems | Big Data Analytics</title>
  <meta name="description" content="A guide to data science practitioners making the transition to Big Data." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Distributed Systems | Big Data Analytics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://umatter.github.io/BigData/img/cover.jpg" />
  <meta property="og:description" content="A guide to data science practitioners making the transition to Big Data." />
  <meta name="github-repo" content="umatter/BigData" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Distributed Systems | Big Data Analytics" />
  
  <meta name="twitter:description" content="A guide to data science practitioners making the transition to Big Data." />
  <meta name="twitter:image" content="https://umatter.github.io/BigData/img/cover.jpg" />

<meta name="author" content="Ulrich Matter" />


<meta name="date" content="2023-02-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hardware-computing-resources.html"/>
<link rel="next" href="cloud-computing.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="c.html"><a href="c.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="c.html"><a href="c.html#background-and-goals-of-this-book"><i class="fa fa-check"></i>Background and goals of this book</a></li>
<li class="chapter" data-level="" data-path="c.html"><a href="c.html#a-moving-target"><i class="fa fa-check"></i>A moving target</a></li>
<li class="chapter" data-level="" data-path="c.html"><a href="c.html#content-and-organization-of-the-book"><i class="fa fa-check"></i>Content and organization of the book</a></li>
<li class="chapter" data-level="" data-path="c.html"><a href="c.html#prerequisits-and-requirements"><i class="fa fa-check"></i>Prerequisits and requirements</a></li>
<li class="chapter" data-level="" data-path="c.html"><a href="c.html#thanks"><i class="fa fa-check"></i>Thanks</a></li>
</ul></li>
<li class="part"><span><b>I Setting the Scene: Analyzing Big Data</b></span></li>
<li class="chapter" data-level="" data-path="s.html"><a href="s.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="what-is-big-in-big-data.html"><a href="what-is-big-in-big-data.html"><i class="fa fa-check"></i><b>1</b> What is <em>Big</em> in “Big Data”?</a></li>
<li class="chapter" data-level="2" data-path="approaches-to-analyzing-big-data.html"><a href="approaches-to-analyzing-big-data.html"><i class="fa fa-check"></i><b>2</b> Approaches to Analyzing Big Data</a></li>
<li class="chapter" data-level="3" data-path="the-two-domains-of-big-data-analytics.html"><a href="the-two-domains-of-big-data-analytics.html"><i class="fa fa-check"></i><b>3</b> The Two Domains of Big Data Analytics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-two-domains-of-big-data-analytics.html"><a href="the-two-domains-of-big-data-analytics.html#a-practical-big-p-problem"><i class="fa fa-check"></i><b>3.1</b> A practical <em>big P</em> problem </a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="the-two-domains-of-big-data-analytics.html"><a href="the-two-domains-of-big-data-analytics.html#simple-logistig-regression-naive-approach"><i class="fa fa-check"></i><b>3.1.1</b> Simple logistig regression (naive approach)</a></li>
<li class="chapter" data-level="3.1.2" data-path="the-two-domains-of-big-data-analytics.html"><a href="the-two-domains-of-big-data-analytics.html#regularization-the-lasso-estimator"><i class="fa fa-check"></i><b>3.1.2</b> Regularization: the lasso estimator</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="the-two-domains-of-big-data-analytics.html"><a href="the-two-domains-of-big-data-analytics.html#a-practical-big-n-problem"><i class="fa fa-check"></i><b>3.2</b> A practical <em>big N</em> problem </a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-two-domains-of-big-data-analytics.html"><a href="the-two-domains-of-big-data-analytics.html#ols-as-a-point-of-reference"><i class="fa fa-check"></i><b>3.2.1</b> OLS as a point of reference </a></li>
<li class="chapter" data-level="3.2.2" data-path="the-two-domains-of-big-data-analytics.html"><a href="the-two-domains-of-big-data-analytics.html#the-uluru-algorithm-as-an-alternative-to-ols"><i class="fa fa-check"></i><b>3.2.2</b> The <em>Uluru</em> algorithm as an alternative to OLS </a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Platform: Software and Computing Resources</b></span></li>
<li class="chapter" data-level="" data-path="p.html"><a href="p.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="4" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html"><i class="fa fa-check"></i><b>4</b> Software: Programming with (Big) Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#domains-of-programming-with-big-data"><i class="fa fa-check"></i><b>4.1</b> Domains of programming with (big) data</a></li>
<li class="chapter" data-level="4.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#measuring-r-performance"><i class="fa fa-check"></i><b>4.2</b> Measuring R performance</a></li>
<li class="chapter" data-level="4.3" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#writing-efficient-r-code"><i class="fa fa-check"></i><b>4.3</b> Writing efficient R code</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#memory-allocation-and-growing-objects"><i class="fa fa-check"></i><b>4.3.1</b> Memory allocation and growing objects</a></li>
<li class="chapter" data-level="4.3.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#vectorization-in-basic-r-functions"><i class="fa fa-check"></i><b>4.3.2</b> Vectorization in basic R functions</a></li>
<li class="chapter" data-level="4.3.3" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#apply-type-functions-and-vectorization"><i class="fa fa-check"></i><b>4.3.3</b> <code>apply</code>-type functions and vectorization</a></li>
<li class="chapter" data-level="4.3.4" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#avoid-unnecessary-copying"><i class="fa fa-check"></i><b>4.3.4</b> Avoid unnecessary copying</a></li>
<li class="chapter" data-level="4.3.5" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#releasing-memory"><i class="fa fa-check"></i><b>4.3.5</b> Releasing memory</a></li>
<li class="chapter" data-level="4.3.6" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#beyond-r"><i class="fa fa-check"></i><b>4.3.6</b> Beyond R</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#sql-basics"><i class="fa fa-check"></i><b>4.4</b> SQL basics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#first-steps-in-sqlite"><i class="fa fa-check"></i><b>4.4.1</b> First steps in SQL(ite)</a></li>
<li class="chapter" data-level="4.4.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#joins"><i class="fa fa-check"></i><b>4.4.2</b> Joins</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#with-a-little-help-from-my-friends-gpt-and-rsql-coding"><i class="fa fa-check"></i><b>4.5</b> With a little help from my friends: GPT and R/SQL coding</a></li>
<li class="chapter" data-level="4.6" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#wrapping-up"><i class="fa fa-check"></i><b>4.6</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html"><i class="fa fa-check"></i><b>5</b> Hardware: Computing Resources</a>
<ul>
<li class="chapter" data-level="5.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#mass-storage"><i class="fa fa-check"></i><b>5.1</b> Mass storage</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#avoid-redundancies"><i class="fa fa-check"></i><b>5.1.1</b> Avoid redundancies</a></li>
<li class="chapter" data-level="5.1.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#data-compression"><i class="fa fa-check"></i><b>5.1.2</b> Data compression</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#random-access-memory-ram"><i class="fa fa-check"></i><b>5.2</b> Random access memory (RAM)</a></li>
<li class="chapter" data-level="5.3" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#combining-ram-and-hard-disk-virtual-memory"><i class="fa fa-check"></i><b>5.3</b> Combining RAM and hard disk: Virtual memory</a></li>
<li class="chapter" data-level="5.4" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#cpu-and-parallelization"><i class="fa fa-check"></i><b>5.4</b> CPU and parallelization</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#naive-multi-session-approach"><i class="fa fa-check"></i><b>5.4.1</b> Naive multi-session approach</a></li>
<li class="chapter" data-level="5.4.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#multi-session-approach-with-futures"><i class="fa fa-check"></i><b>5.4.2</b> Multi-session approach with futures</a></li>
<li class="chapter" data-level="5.4.3" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#multi-core-and-multi-node-approach"><i class="fa fa-check"></i><b>5.4.3</b> Multi-core and multi-node approach</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#gpus-for-scientific-computing"><i class="fa fa-check"></i><b>5.5</b> GPUs for scientific computing</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#gpus-in-r"><i class="fa fa-check"></i><b>5.5.1</b> GPUs in R</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#the-road-ahead-hardware-made-for-machine-learning"><i class="fa fa-check"></i><b>5.6</b> The road ahead: Hardware made for machine learning</a></li>
<li class="chapter" data-level="5.7" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#wrapping-up-1"><i class="fa fa-check"></i><b>5.7</b> Wrapping up</a></li>
<li class="chapter" data-level="5.8" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#still-have-insufficient-computing-resources"><i class="fa fa-check"></i><b>5.8</b> Still have insufficient computing resources?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="distributed-systems.html"><a href="distributed-systems.html"><i class="fa fa-check"></i><b>6</b> Distributed Systems</a>
<ul>
<li class="chapter" data-level="6.1" data-path="distributed-systems.html"><a href="distributed-systems.html#mapreduce"><i class="fa fa-check"></i><b>6.1</b> MapReduce</a></li>
<li class="chapter" data-level="6.2" data-path="distributed-systems.html"><a href="distributed-systems.html#apache-hadoop"><i class="fa fa-check"></i><b>6.2</b> Apache Hadoop</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="distributed-systems.html"><a href="distributed-systems.html#hadoop-word-count-example"><i class="fa fa-check"></i><b>6.2.1</b> Hadoop word count example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="distributed-systems.html"><a href="distributed-systems.html#apache-spark"><i class="fa fa-check"></i><b>6.3</b> Apache Spark</a></li>
<li class="chapter" data-level="6.4" data-path="distributed-systems.html"><a href="distributed-systems.html#spark-with-r"><i class="fa fa-check"></i><b>6.4</b> Spark with R</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="distributed-systems.html"><a href="distributed-systems.html#data-import-and-summary-statistics"><i class="fa fa-check"></i><b>6.4.1</b> Data import and summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="distributed-systems.html"><a href="distributed-systems.html#spark-with-sql"><i class="fa fa-check"></i><b>6.5</b> Spark with SQL</a></li>
<li class="chapter" data-level="6.6" data-path="distributed-systems.html"><a href="distributed-systems.html#spark-with-r-sql"><i class="fa fa-check"></i><b>6.6</b> Spark with R + SQL</a></li>
<li class="chapter" data-level="6.7" data-path="distributed-systems.html"><a href="distributed-systems.html#wrapping-up-2"><i class="fa fa-check"></i><b>6.7</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="cloud-computing.html"><a href="cloud-computing.html"><i class="fa fa-check"></i><b>7</b> Cloud Computing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="cloud-computing.html"><a href="cloud-computing.html#cloud-computing-basics-and-platforms"><i class="fa fa-check"></i><b>7.1</b> Cloud computing basics and platforms</a></li>
<li class="chapter" data-level="7.2" data-path="cloud-computing.html"><a href="cloud-computing.html#transitioning-to-the-cloud"><i class="fa fa-check"></i><b>7.2</b> Transitioning to the cloud</a></li>
<li class="chapter" data-level="7.3" data-path="cloud-computing.html"><a href="cloud-computing.html#scaling-up-in-the-cloud-virtual-servers"><i class="fa fa-check"></i><b>7.3</b> Scaling up in the cloud: Virtual servers</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="cloud-computing.html"><a href="cloud-computing.html#parallelization-with-an-ec2-instance"><i class="fa fa-check"></i><b>7.3.1</b> Parallelization with an EC2 instance</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cloud-computing.html"><a href="cloud-computing.html#scaling-up-with-gpus"><i class="fa fa-check"></i><b>7.4</b> Scaling up with GPUs</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="cloud-computing.html"><a href="cloud-computing.html#gpus-on-google-colab"><i class="fa fa-check"></i><b>7.4.1</b> GPUs on Google Colab</a></li>
<li class="chapter" data-level="7.4.2" data-path="cloud-computing.html"><a href="cloud-computing.html#rstudio-and-ec2-with-gpus-on-aws"><i class="fa fa-check"></i><b>7.4.2</b> RStudio and EC2 with GPUs on AWS</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="cloud-computing.html"><a href="cloud-computing.html#scaling-out-mapreduce-in-the-cloud"><i class="fa fa-check"></i><b>7.5</b> Scaling out: MapReduce in the cloud</a></li>
<li class="chapter" data-level="7.6" data-path="cloud-computing.html"><a href="cloud-computing.html#wrapping-up-3"><i class="fa fa-check"></i><b>7.6</b> Wrapping up</a></li>
</ul></li>
<li class="part"><span><b>III Components of Big Data Analytics</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="8" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html"><i class="fa fa-check"></i><b>8</b> Data Collection and Data Storage</a>
<ul>
<li class="chapter" data-level="8.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#gathering-and-compilation-of-raw-data"><i class="fa fa-check"></i><b>8.1</b> Gathering and compilation of raw data</a></li>
<li class="chapter" data-level="8.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#stackcombine-raw-source-files"><i class="fa fa-check"></i><b>8.2</b> Stack/combine raw source files</a></li>
<li class="chapter" data-level="8.3" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#efficient-local-data-storage"><i class="fa fa-check"></i><b>8.3</b> Efficient local data storage</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#rdbms-basics"><i class="fa fa-check"></i><b>8.3.1</b> RDBMS basics</a></li>
<li class="chapter" data-level="8.3.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#efficient-data-access-indices-and-joins-in-sqlite"><i class="fa fa-check"></i><b>8.3.2</b> Efficient data access: Indices and joins in SQLite</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#connecting-r-to-an-rdbms"><i class="fa fa-check"></i><b>8.4</b> Connecting R to an RDBMS</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#creating-a-new-database-with-rsqlite"><i class="fa fa-check"></i><b>8.4.1</b> Creating a new database with <code>RSQLite</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#importing-data"><i class="fa fa-check"></i><b>8.4.2</b> Importing data</a></li>
<li class="chapter" data-level="8.4.3" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#issue-queries"><i class="fa fa-check"></i><b>8.4.3</b> Issue queries</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#cloud-solutions-for-big-data-storage"><i class="fa fa-check"></i><b>8.5</b> Cloud solutions for (big) data storage</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#easy-to-use-rdbms-in-the-cloud-aws-rds"><i class="fa fa-check"></i><b>8.5.1</b> Easy-to-use RDBMS in the cloud: AWS RDS</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#column-based-analytics-databases"><i class="fa fa-check"></i><b>8.6</b> Column-based analytics databases</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#installation-and-start-up"><i class="fa fa-check"></i><b>8.6.1</b> Installation and start up</a></li>
<li class="chapter" data-level="8.6.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#first-steps-via-druids-gui"><i class="fa fa-check"></i><b>8.6.2</b> First steps via Druid’s GUI</a></li>
<li class="chapter" data-level="8.6.3" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#query-druid-from-r"><i class="fa fa-check"></i><b>8.6.3</b> Query Druid from R</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#data-warehouses"><i class="fa fa-check"></i><b>8.7</b> Data warehouses</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#data-warehouse-for-analytics-google-bigquery-example"><i class="fa fa-check"></i><b>8.7.1</b> Data warehouse for analytics: Google BigQuery example</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#data-lakes-and-simple-storage-service"><i class="fa fa-check"></i><b>8.8</b> Data lakes and simple storage service</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#aws-s3-with-r-first-steps"><i class="fa fa-check"></i><b>8.8.1</b> AWS S3 with R: First steps</a></li>
<li class="chapter" data-level="8.8.2" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#uploading-data-to-s3"><i class="fa fa-check"></i><b>8.8.2</b> Uploading data to S3</a></li>
<li class="chapter" data-level="8.8.3" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#more-than-just-simple-storage-s3-amazon-athena"><i class="fa fa-check"></i><b>8.8.3</b> More than just simple storage: S3 + Amazon Athena</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="data-collection-and-data-storage.html"><a href="data-collection-and-data-storage.html#wrapping-up-4"><i class="fa fa-check"></i><b>8.9</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html"><i class="fa fa-check"></i><b>9</b> Big Data Cleaning and Transformation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#out-of-memory-strategies-and-lazy-evaluation-practical-basics"><i class="fa fa-check"></i><b>9.1</b> Out-of-memory strategies and lazy evaluation: Practical basics</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#chunking-data-with-the-ff-package"><i class="fa fa-check"></i><b>9.1.1</b> Chunking data with the <code>ff</code> package</a></li>
<li class="chapter" data-level="9.1.2" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#memory-mapping-with-bigmemory"><i class="fa fa-check"></i><b>9.1.2</b> Memory mapping with <code>bigmemory</code></a></li>
<li class="chapter" data-level="9.1.3" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#connecting-to-apache-arrow"><i class="fa fa-check"></i><b>9.1.3</b> Connecting to Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#big-data-preparation-tutorial-with-ff"><i class="fa fa-check"></i><b>9.2</b> Big Data preparation tutorial with <code>ff</code></a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#set-up"><i class="fa fa-check"></i><b>9.2.1</b> Set up</a></li>
<li class="chapter" data-level="9.2.2" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#data-import"><i class="fa fa-check"></i><b>9.2.2</b> Data import</a></li>
<li class="chapter" data-level="9.2.3" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#inspect-imported-files"><i class="fa fa-check"></i><b>9.2.3</b> Inspect imported files</a></li>
<li class="chapter" data-level="9.2.4" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#data-cleaning-and-transformation"><i class="fa fa-check"></i><b>9.2.4</b> Data cleaning and transformation</a></li>
<li class="chapter" data-level="9.2.5" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#inspect-difference-to-in-memory-operation"><i class="fa fa-check"></i><b>9.2.5</b> Inspect difference to in-memory operation</a></li>
<li class="chapter" data-level="9.2.6" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#subsetting"><i class="fa fa-check"></i><b>9.2.6</b> Subsetting</a></li>
<li class="chapter" data-level="9.2.7" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#saveloadexport-ff-files"><i class="fa fa-check"></i><b>9.2.7</b> Save/load/export <code>ff</code> files</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#big-data-preparation-tutorial-with-arrow"><i class="fa fa-check"></i><b>9.3</b> Big Data preparation tutorial with <code>arrow</code></a></li>
<li class="chapter" data-level="9.4" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#wrapping-up-5"><i class="fa fa-check"></i><b>9.4</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html"><i class="fa fa-check"></i><b>10</b> Descriptive Statistics and Aggregation</a>
<ul>
<li class="chapter" data-level="10.1" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#data-aggregation-the-split-apply-combine-strategy"><i class="fa fa-check"></i><b>10.1</b> Data aggregation: The ‘split-apply-combine’ strategy</a></li>
<li class="chapter" data-level="10.2" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#data-aggregation-with-chunked-data-files"><i class="fa fa-check"></i><b>10.2</b> Data aggregation with chunked data files</a></li>
<li class="chapter" data-level="10.3" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#high-speed-in-memory-data-aggregation-with-arrow"><i class="fa fa-check"></i><b>10.3</b> High-speed in-memory data aggregation with <code>arrow</code></a></li>
<li class="chapter" data-level="10.4" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#high-speed-in-memory-data-aggregation-with-data.table"><i class="fa fa-check"></i><b>10.4</b> High-speed in-memory data aggregation with <code>data.table</code></a></li>
<li class="chapter" data-level="10.5" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#wrapping-up-6"><i class="fa fa-check"></i><b>10.5</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="big-data-visualization.html"><a href="big-data-visualization.html"><i class="fa fa-check"></i><b>11</b> (Big) Data Visualization</a>
<ul>
<li class="chapter" data-level="11.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#challenges-of-big-data-visualization"><i class="fa fa-check"></i><b>11.1</b> Challenges of Big Data visualization</a></li>
<li class="chapter" data-level="11.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#data-exploration-with-ggplot2"><i class="fa fa-check"></i><b>11.2</b> Data exploration with <code>ggplot2</code></a></li>
<li class="chapter" data-level="11.3" data-path="big-data-visualization.html"><a href="big-data-visualization.html#visualizing-time-and-space"><i class="fa fa-check"></i><b>11.3</b> Visualizing time and space</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#preparations"><i class="fa fa-check"></i><b>11.3.1</b> Preparations</a></li>
<li class="chapter" data-level="11.3.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#pick-up-and-drop-off-locations"><i class="fa fa-check"></i><b>11.3.2</b> Pick-up and drop-off locations</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="big-data-visualization.html"><a href="big-data-visualization.html#wrapping-up-7"><i class="fa fa-check"></i><b>11.4</b> Wrapping up</a></li>
</ul></li>
<li class="part"><span><b>IV Application: Topics in Big Data Econometrics</b></span></li>
<li class="chapter" data-level="" data-path="a.html"><a href="a.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="12" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html"><i class="fa fa-check"></i><b>12</b> Bottlenecks in Everyday Data Analytics Tasks</a>
<ul>
<li class="chapter" data-level="12.1" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html#case-study-efficient-fixed-effects-estimation"><i class="fa fa-check"></i><b>12.1</b> Case study: Efficient fixed effects estimation</a></li>
<li class="chapter" data-level="12.2" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html#case-study-loops-memory-and-vectorization"><i class="fa fa-check"></i><b>12.2</b> Case study: Loops, memory, and vectorization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html#naïve-approach-ignorant-of-r"><i class="fa fa-check"></i><b>12.2.1</b> Naïve approach (ignorant of R)</a></li>
<li class="chapter" data-level="12.2.2" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html#improvement-1-pre-allocation-of-memory"><i class="fa fa-check"></i><b>12.2.2</b> Improvement 1: Pre-allocation of memory</a></li>
<li class="chapter" data-level="12.2.3" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html#improvement-2-exploit-vectorization"><i class="fa fa-check"></i><b>12.2.3</b> Improvement 2: Exploit vectorization</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html#case-study-bootstrapping-and-parallel-processing"><i class="fa fa-check"></i><b>12.3</b> Case study: Bootstrapping and parallel processing</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="bottlenecks-in-everyday-data-analytics-tasks.html"><a href="bottlenecks-in-everyday-data-analytics-tasks.html#parallelization-with-an-ec2-instance-1"><i class="fa fa-check"></i><b>12.3.1</b> Parallelization with an EC2 instance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html"><i class="fa fa-check"></i><b>13</b> Econometrics with GPUs</a>
<ul>
<li class="chapter" data-level="13.1" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#ols-on-gpus"><i class="fa fa-check"></i><b>13.1</b> OLS on GPUs</a></li>
<li class="chapter" data-level="13.2" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#a-word-of-caution"><i class="fa fa-check"></i><b>13.2</b> A word of caution</a></li>
<li class="chapter" data-level="13.3" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#higher-level-interfaces-for-basic-econometrics-with-gpus"><i class="fa fa-check"></i><b>13.3</b> Higher-level interfaces for basic econometrics with GPUs</a></li>
<li class="chapter" data-level="13.4" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#tensorflowkeras-example-predict-housing-prices"><i class="fa fa-check"></i><b>13.4</b> TensorFlow/Keras example: predict housing prices</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#data-preparation"><i class="fa fa-check"></i><b>13.4.1</b> Data preparation</a></li>
<li class="chapter" data-level="13.4.2" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#model-specification"><i class="fa fa-check"></i><b>13.4.2</b> Model specification</a></li>
<li class="chapter" data-level="13.4.3" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#training-and-prediction"><i class="fa fa-check"></i><b>13.4.3</b> Training and prediction</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="econometrics-with-gpus.html"><a href="econometrics-with-gpus.html#wrapping-up-8"><i class="fa fa-check"></i><b>13.5</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html"><i class="fa fa-check"></i><b>14</b> Regression Analysis and Categorization with Spark and R</a>
<ul>
<li class="chapter" data-level="14.1" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#simple-linear-regression-analysis"><i class="fa fa-check"></i><b>14.1</b> Simple linear regression analysis</a></li>
<li class="chapter" data-level="14.2" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#machine-learning-for-classification"><i class="fa fa-check"></i><b>14.2</b> Machine learning for classification</a></li>
<li class="chapter" data-level="14.3" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#building-machine-learning-pipelines-with-r-and-spark"><i class="fa fa-check"></i><b>14.3</b> Building machine learning pipelines with R and Spark</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#set-up-and-data-import"><i class="fa fa-check"></i><b>14.3.1</b> Set up and data import</a></li>
<li class="chapter" data-level="14.3.2" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#building-the-pipeline"><i class="fa fa-check"></i><b>14.3.2</b> Building the pipeline</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="regression-analysis-and-categorization-with-spark-and-r.html"><a href="regression-analysis-and-categorization-with-spark-and-r.html#wrapping-up-9"><i class="fa fa-check"></i><b>14.4</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html"><i class="fa fa-check"></i><b>15</b> Large-scale Text Analysis with sparklyr</a>
<ul>
<li class="chapter" data-level="15.1" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#getting-started-import-pre-processing-and-word-count"><i class="fa fa-check"></i><b>15.1</b> Getting started: Import, pre-processing, and word count</a></li>
<li class="chapter" data-level="15.2" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#tutorial-political-slant"><i class="fa fa-check"></i><b>15.2</b> Tutorial: political slant</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#data-download-and-import"><i class="fa fa-check"></i><b>15.2.1</b> Data download and import</a></li>
<li class="chapter" data-level="15.2.2" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#cleaning-speeches-data"><i class="fa fa-check"></i><b>15.2.2</b> Cleaning speeches data</a></li>
<li class="chapter" data-level="15.2.3" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#create-a-bigrams-count-per-party"><i class="fa fa-check"></i><b>15.2.3</b> Create a bigrams count per party</a></li>
<li class="chapter" data-level="15.2.4" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#find-partisan-phrases"><i class="fa fa-check"></i><b>15.2.4</b> Find “partisan” phrases</a></li>
<li class="chapter" data-level="15.2.5" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#results-most-partisan-phrases-by-congress"><i class="fa fa-check"></i><b>15.2.5</b> Results: most partisan phrases by congress</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#natural-language-processing-at-scale"><i class="fa fa-check"></i><b>15.3</b> Natural Language Processing at Scale</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#preparatory-steps"><i class="fa fa-check"></i><b>15.3.1</b> Preparatory steps</a></li>
<li class="chapter" data-level="15.3.2" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#sentiment-annotation"><i class="fa fa-check"></i><b>15.3.2</b> Sentiment annotation</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#aggregation-and-visualization"><i class="fa fa-check"></i><b>15.4</b> Aggregation and visualization</a></li>
<li class="chapter" data-level="15.5" data-path="large-scale-text-analysis-with-sparklyr.html"><a href="large-scale-text-analysis-with-sparklyr.html#sparklyr-and-lazy-evaluation"><i class="fa fa-check"></i><b>15.5</b> <code>sparklyr</code> and lazy evaluation</a></li>
</ul></li>
<li class="part"><span><b>V Appendices</b></span></li>
<li class="chapter" data-level="" data-path="appendix-a-github.html"><a href="appendix-a-github.html"><i class="fa fa-check"></i>Appendix A: GitHub</a>
<ul>
<li class="chapter" data-level="" data-path="appendix-a-github.html"><a href="appendix-a-github.html#initiate-a-new-repository"><i class="fa fa-check"></i>Initiate a new repository</a></li>
<li class="chapter" data-level="" data-path="appendix-a-github.html"><a href="appendix-a-github.html#clone-this-books-repository"><i class="fa fa-check"></i>Clone this book’s repository</a></li>
<li class="chapter" data-level="" data-path="appendix-a-github.html"><a href="appendix-a-github.html#fork-this-books-repository"><i class="fa fa-check"></i>Fork this book’s repository</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html"><i class="fa fa-check"></i>Appendix B: R Basics</a>
<ul>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#data-types-and-memorystorage"><i class="fa fa-check"></i>Data types and memory/storage</a>
<ul>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#example-data-types-and-information-storage"><i class="fa fa-check"></i>Example: Data types and information storage</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#data-structures"><i class="fa fa-check"></i>Data structures</a>
<ul>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#vectors-vs-factors-in-r"><i class="fa fa-check"></i>Vectors vs Factors in R</a></li>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#matricesarrays"><i class="fa fa-check"></i>Matrices/Arrays</a></li>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#data-frames-tibbles-and-data-tables"><i class="fa fa-check"></i>Data frames, tibbles, and data tables</a></li>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#lists"><i class="fa fa-check"></i>Lists</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b-r-basics.html"><a href="appendix-b-r-basics.html#r-tools-to-investigate-structures-and-types"><i class="fa fa-check"></i>R-tools to investigate structures and types</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c-install-hadoop.html"><a href="appendix-c-install-hadoop.html"><i class="fa fa-check"></i>Appendix C: Install Hadoop</a></li>
<li class="part"><span><b>VI References and Index</b></span></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://umatter.github.io" target="blank">umatter.github.io</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distributed-systems" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Distributed Systems<a href="distributed-systems.html#distributed-systems" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>When we connect several computers in a network to jointly process large amounts of data, such a computing system is commonly referred to as a “distributed system”. From a technical standpoint the key difference between a distributed system and the more familiar parallel system (e.g., our desktop computer with its multicore CPU) is that in distributed systems the different components do not share the same memory (and storage). Figure <a href="distributed-systems.html#fig:distributedsystems">6.1</a> illustrates this point.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:distributedsystems"></span>
<img src="img/distributed_system.jpg" alt="Panel A illustrates a distributed system, in contrast to the illustration of a parallel system in Panel B." width="99%" />
<p class="caption">
Figure 6.1: Panel A illustrates a distributed system, in contrast to the illustration of a parallel system in Panel B.
</p>
</div>

<p>In a distributed system, the dataset is literally split up into pieces that then reside separately on different nodes. This requires an additional layer of software (that coordinates the distribution/loading of data as well as the simultaneous processing) and different approaches (different programming models) to defining computing/data analytics tasks. Below, we will look at each of these aspects in turn.</p>
<div id="mapreduce" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> MapReduce<a href="distributed-systems.html#mapreduce" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p></p>
<p>A broadly used programming model for processing big data on distributed systems is called MapReduce. It essentially consists of two procedures and is conceptually very close to the “split-apply-combine” strategy in data analysis. First, the Map function sorts/filters the data (on each node/computer). Then, a Reduce function aggregates the sorted/filtered data. Thereby, all of these processes are orchestrated to run across many nodes of a cluster computer. Finally, the master node collects the results and returns them to the user.</p>
<p>Let us illustrate the basic idea behind MapReduce with a simple example. Suppose you are working on a text mining task in which all the raw text in thousands of digitized books (stored as text files) need to be processed. In a first step, you want to compute word frequencies (count the number of occurrences of specific words in all books combined).</p>
<p>For simplicity, let us only focus on the following very simple and often referred to MapReduce word count example<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a>:</p>
<center>
<p>Text in book 1:</p>
<p><em>Apple Orange Mango</em>
 </p>
<p><em>Orange Grapes Plum</em>
 </p>
</center>
<center>
<p>Text in book 2:</p>
<p><em>Apple Plum Mango</em>
 </p>
<p><em>Apple Apple Plum</em>
 </p>
</center>
<p>The MapReduce procedure is then as follows:</p>
<ul>
<li>First, the data is loaded from the original text files.</li>
<li>Each line of text is then passed to individual mapper instances, which separately split the lines of text into key–value pairs. In the example above, the first key-value pair of the first document/line would then be <em>Apple,1</em>.</li>
<li>Then the system sorts and shuffles all key–value pairs across all instances; next, the reducer aggregates the sorted/shuffled key–value pairs (here: counts the number of word occurrences). In the example above, this means all values with key <em>Apple</em> are summed up, resulting in <em>Apple,4</em>.</li>
<li>Finally, the master instance collects all the results and returns the final output.</li>
</ul>
<p>The result would be as follows:</p>
<center>
<p>Text in book 2:</p>
<p><em>Apple,4</em>
 </p>
<p><em>Grapes,1</em>
 </p>
<p><em>Mango,2</em>
 </p>
<p><em>Orange,2</em>
 </p>
<p><em>Plum,3</em>
 </p>
</center>
<p>From this simple example, a key aspect of MapReduce should become clear: for the key tasks of mapping and reducing, the data processing on one node/instance can happen completely independently of the processing on the other instances. Note that this is not as easily achievable for every data analytics task as it is for computing word frequencies.</p>
<div class="infobox">
<div class="center">
<p><strong>Aside: MapReduce concept illustrated in R</strong></p>
</div>
<p>In order to better understand the basic concept behind the MapReduce framework on a distributed system, let’s look at how we can combine the functions <code>map()</code> and <code>reduce()</code> in R to implement the basic MapReduce example shown above (this is just to illustrate the underlying idea, <em>not</em> to suggest that MapReduce actually is simply an application of the classical <code>map</code> and <code>reduce (fold)</code> functions in functional programming).<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a> The overall aim of the program is to count the number of times each word is repeated in a given text. The input to the program is thus a text, and the output is a list of key–value pairs with the unique words occurring in the text as keys and their respective number of occurrences as values.</p>
<p>In the code example, we will use the following text as input.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="distributed-systems.html#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the input text (for simplicity as one text string)</span></span>
<span id="cb149-2"><a href="distributed-systems.html#cb149-2" aria-hidden="true" tabindex="-1"></a>input_text <span class="ot">&lt;-</span></span>
<span id="cb149-3"><a href="distributed-systems.html#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;Apple Orange Mango</span></span>
<span id="cb149-4"><a href="distributed-systems.html#cb149-4" aria-hidden="true" tabindex="-1"></a><span class="st">Orange Grapes Plum</span></span>
<span id="cb149-5"><a href="distributed-systems.html#cb149-5" aria-hidden="true" tabindex="-1"></a><span class="st">Apple Plum Mango</span></span>
<span id="cb149-6"><a href="distributed-systems.html#cb149-6" aria-hidden="true" tabindex="-1"></a><span class="st">Apple Apple Plum&quot;</span></span></code></pre></div>
<p><em>Mapper</em></p>
<p>The Mapper first splits the text into lines, and then splits the lines into key–value pairs, assigning to each key the value <code>1</code>. For the first step we use <code>strsplit()</code>, which takes a character string as input and splits it into a list of sub-strings according to the matches of a sub-string (here <code>"\n"</code>, indicating the end of a line).</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="distributed-systems.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mapper splits input into lines</span></span>
<span id="cb150-2"><a href="distributed-systems.html#cb150-2" aria-hidden="true" tabindex="-1"></a>lines <span class="ot">&lt;-</span> <span class="fu">as.list</span>(<span class="fu">strsplit</span>(input_text, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)[[<span class="dv">1</span>]])</span>
<span id="cb150-3"><a href="distributed-systems.html#cb150-3" aria-hidden="true" tabindex="-1"></a>lines[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [[1]]
## [1] &quot;Apple Orange Mango&quot;
## 
## [[2]]
## [1] &quot;Orange Grapes Plum&quot;</code></pre>
<p>In a second step, we apply our own function (<code>map_fun()</code>) to each line of text via <code>Map()</code>. <code>map_fun()</code> splits each line into words (keys) and assigns a value of <code>1</code> to each key.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="distributed-systems.html#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mapper splits lines into key–value pairs</span></span>
<span id="cb152-2"><a href="distributed-systems.html#cb152-2" aria-hidden="true" tabindex="-1"></a>map_fun <span class="ot">&lt;-</span></span>
<span id="cb152-3"><a href="distributed-systems.html#cb152-3" aria-hidden="true" tabindex="-1"></a>     <span class="cf">function</span>(x){</span>
<span id="cb152-4"><a href="distributed-systems.html#cb152-4" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb152-5"><a href="distributed-systems.html#cb152-5" aria-hidden="true" tabindex="-1"></a>          <span class="co"># remove special characters</span></span>
<span id="cb152-6"><a href="distributed-systems.html#cb152-6" aria-hidden="true" tabindex="-1"></a>          x_clean <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;[[:punct:]]&quot;</span>, <span class="st">&quot;&quot;</span>, x)</span>
<span id="cb152-7"><a href="distributed-systems.html#cb152-7" aria-hidden="true" tabindex="-1"></a>          <span class="co"># split line into words</span></span>
<span id="cb152-8"><a href="distributed-systems.html#cb152-8" aria-hidden="true" tabindex="-1"></a>          keys <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">strsplit</span>(x_clean, <span class="st">&quot; &quot;</span>))</span>
<span id="cb152-9"><a href="distributed-systems.html#cb152-9" aria-hidden="true" tabindex="-1"></a>          <span class="co"># initialize key–value pairs</span></span>
<span id="cb152-10"><a href="distributed-systems.html#cb152-10" aria-hidden="true" tabindex="-1"></a>          key_values <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">length</span>(keys))</span>
<span id="cb152-11"><a href="distributed-systems.html#cb152-11" aria-hidden="true" tabindex="-1"></a>          <span class="fu">names</span>(key_values) <span class="ot">&lt;-</span> keys</span>
<span id="cb152-12"><a href="distributed-systems.html#cb152-12" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb152-13"><a href="distributed-systems.html#cb152-13" aria-hidden="true" tabindex="-1"></a>          <span class="fu">return</span>(key_values)</span>
<span id="cb152-14"><a href="distributed-systems.html#cb152-14" aria-hidden="true" tabindex="-1"></a>     }</span>
<span id="cb152-15"><a href="distributed-systems.html#cb152-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-16"><a href="distributed-systems.html#cb152-16" aria-hidden="true" tabindex="-1"></a>kv_pairs <span class="ot">&lt;-</span> <span class="fu">Map</span>(map_fun, lines)</span>
<span id="cb152-17"><a href="distributed-systems.html#cb152-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-18"><a href="distributed-systems.html#cb152-18" aria-hidden="true" tabindex="-1"></a><span class="co"># look at the result</span></span>
<span id="cb152-19"><a href="distributed-systems.html#cb152-19" aria-hidden="true" tabindex="-1"></a>kv_pairs[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [[1]]
##  Apple Orange  Mango 
##      1      1      1 
## 
## [[2]]
## Orange Grapes   Plum 
##      1      1      1</code></pre>
<p><em>Reducer</em></p>
<p>The Reducer first sorts and shuffles the input from the Mapper and then reduces the key–value pairs by summing up the values for each key.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="distributed-systems.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="co"># order and shuffle</span></span>
<span id="cb154-2"><a href="distributed-systems.html#cb154-2" aria-hidden="true" tabindex="-1"></a>kv_pairs <span class="ot">&lt;-</span> <span class="fu">unlist</span>(kv_pairs)</span>
<span id="cb154-3"><a href="distributed-systems.html#cb154-3" aria-hidden="true" tabindex="-1"></a>keys <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">names</span>(kv_pairs))</span>
<span id="cb154-4"><a href="distributed-systems.html#cb154-4" aria-hidden="true" tabindex="-1"></a>keys <span class="ot">&lt;-</span> keys[<span class="fu">order</span>(keys)]</span>
<span id="cb154-5"><a href="distributed-systems.html#cb154-5" aria-hidden="true" tabindex="-1"></a>shuffled <span class="ot">&lt;-</span> <span class="fu">lapply</span>(keys,</span>
<span id="cb154-6"><a href="distributed-systems.html#cb154-6" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">function</span>(x) kv_pairs[x <span class="sc">==</span> <span class="fu">names</span>(kv_pairs)])</span>
<span id="cb154-7"><a href="distributed-systems.html#cb154-7" aria-hidden="true" tabindex="-1"></a>shuffled[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [[1]]
## Apple Apple Apple Apple 
##     1     1     1     1 
## 
## [[2]]
## Grapes 
##      1</code></pre>
<p>Now we can sum up the keys to get the word count for the entire input.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="distributed-systems.html#cb156-1" aria-hidden="true" tabindex="-1"></a>sums <span class="ot">&lt;-</span> <span class="fu">lapply</span>(shuffled, Reduce, <span class="at">f=</span>sum)</span>
<span id="cb156-2"><a href="distributed-systems.html#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(sums) <span class="ot">&lt;-</span> keys</span>
<span id="cb156-3"><a href="distributed-systems.html#cb156-3" aria-hidden="true" tabindex="-1"></a>sums[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span></code></pre></div>
<pre><code>## $Apple
## [1] 4
## 
## $Grapes
## [1] 1</code></pre>
<!-- ### Simpler example: Compute the total number of words -->
<!-- ```{r} -->
<!-- # assigns the number of words per line as value -->
<!-- map_fun2 <-  -->
<!--      function(x){ -->
<!--           # remove special characters -->
<!--           x_clean <- gsub("[[:punct:]]", "", x) -->
<!--           # split line into words, count no. of words per line -->
<!--           values <- length(unlist(strsplit(x_clean, " "))) -->
<!--           return(values) -->
<!--      } -->
<!-- # Mapper -->
<!-- mapped <- Map(map_fun2, lines) -->
<!-- mapped -->
<!-- # Reducer -->
<!-- reduced <- Reduce(sum, mapped) -->
<!-- reduced -->
<!-- ``` -->
</div>
</div>
<div id="apache-hadoop" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Apache Hadoop<a href="distributed-systems.html#apache-hadoop" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p></p>
<p>Hadoop MapReduce is the most widely known and used implementation of the MapReduce framework. A decade ago, Big Data Analytics with really large datasets often involved directly interacting with/ working in Hadoop to run MapReduce jobs. However, over the last few years various higher-level interfaces have been developed that make the usage of MapReduce/Hadoop for data analysts much more easily accessible. The purpose of this section is thus to give a lightweight introduction to the underlying basics that power some of the code examples and tutorials discussed in the data analytics chapters toward the end of this book.</p>
<!-- ```{r hadoop, echo=FALSE, out.width = "90%", fig.align='center', purl=FALSE} -->
<!-- include_graphics("img/hadoop.png") -->
<!-- ``` -->
<div id="hadoop-word-count-example" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Hadoop word count example<a href="distributed-systems.html#hadoop-word-count-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To get an idea of what running a Hadoop job looks like, we run the same simple word count example introduced above on a local Hadoop installation. The example presupposes a local installation of Hadoop version 2.10.1 (see Appendix C for details) and can easily be run on a completely normal desktop/laptop computer running Ubuntu Linux. As a side remark, this actually illustrates an important aspect of developing MapReducescripts in Hadoop (and many of the software packages building on it): the code can easily be developed and tested locally on a small machine and only later transferred to the actual Hadoop cluster to be run on the full dataset.</p>
<p>The basic Hadoopinstallation comes with a few templates for very typical map/reduce programs.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a> Below we replicate the same word-count example as shown in simple R code above.</p>
<p>In a first step, we create an input directory where we store the input file(s) to feed to Hadoop.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb158-1"><a href="distributed-systems.html#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create directory for input files (typically text files)</span></span>
<span id="cb158-2"><a href="distributed-systems.html#cb158-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> ~/input</span></code></pre></div>
<p>Then we add a text file containing the same text as in the example above.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb159-1"><a href="distributed-systems.html#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Apple Orange Mango</span></span>
<span id="cb159-2"><a href="distributed-systems.html#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="st">Orange Grapes Plum</span></span>
<span id="cb159-3"><a href="distributed-systems.html#cb159-3" aria-hidden="true" tabindex="-1"></a><span class="st">Apple Plum Mango</span></span>
<span id="cb159-4"><a href="distributed-systems.html#cb159-4" aria-hidden="true" tabindex="-1"></a><span class="st">Apple Apple Plum&quot;</span> <span class="op">&gt;&gt;</span>  ~/input/text.txt</span></code></pre></div>
<p>Now we can run the MapReduce/Hadoop word count as follows, storing the results in a new directory called <code>wordcount_example</code>. We use the already-implemented Hadoop script to run a word count job, MapReduce style. This is where we rely on the already implemented word-count example provided with the Hadoop installation (located in <code>/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar</code>).</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb160-1"><a href="distributed-systems.html#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run mapreduce word count</span></span>
<span id="cb160-2"><a href="distributed-systems.html#cb160-2" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/local/hadoop/bin/hadoop</span> jar <span class="dt">\</span></span>
<span id="cb160-3"><a href="distributed-systems.html#cb160-3" aria-hidden="true" tabindex="-1"></a>/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar <span class="dt">\</span></span>
<span id="cb160-4"><a href="distributed-systems.html#cb160-4" aria-hidden="true" tabindex="-1"></a>wordcount </span>
<span id="cb160-5"><a href="distributed-systems.html#cb160-5" aria-hidden="true" tabindex="-1"></a><span class="ex">~/input</span> ~/wc_example</span></code></pre></div>
<p>What this line says is: Run the Hadoop program called <code>wordcount</code> implemented in the jar-file <code>hadoop-mapreduce-examples-2.10.1.jar</code>; use the files in directory <code>~/input</code> containing the raw text as input, and store the final output in directory <code>~/wc_example</code>.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb161-1"><a href="distributed-systems.html#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> ~/wc_example/<span class="pp">*</span></span></code></pre></div>
<pre><code>## Apple    4
## Grapes   1
## Mango    2
## Orange   2
## Plum 3</code></pre>
<p>What looks rather simple in this example can get very complex once you want to write an entire data analysis script with all kinds of analysis for Hadoop. Also, Hadoop was designed for batch processing and does not offer a simple interface for interactive sessions. All of this makes it rather impractical for a typical analytics workflow as we know it from working with R. This is where <a href="https://spark.apache.org/">Apache Spark</a> <span class="citation">(<a href="#ref-Spark" role="doc-biblioref">Zaharia et al. 2016</a>)</span> comes to the rescue.</p>
</div>
</div>
<div id="apache-spark" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Apache Spark<a href="distributed-systems.html#apache-spark" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p></p>
<p>Spark <span class="citation">(<a href="#ref-Spark" role="doc-biblioref">Zaharia et al. 2016</a>)</span> is a data analytics engine specifically designed for processing large amounts of data on cluster computers. It partially builds on the broader Apache Hadoop framework for handling storage and resource management, but it is often faster than Hadoop MapReduce by an order of magnitude. In addition, it offers way more easy-to-use high-level interfaces for typical analytics tasks than Hadoop. In contrast to Hadoop, Spark
is specifically made for interactively developing and running data analytics scripts and is therefore more easily accessible to people with an applied econometrics background but no substantial knowledge in MapReduce and/or cluster computing. In particular, it comes with several high-level operators that make it rather easy to implement analytics tasks. As we will see in later chapters, it is very easy to use interactively from within R (and other languages like Python, SQL, and Scala). This makes the platform much more accessible and worthwhile for empirical economic research, even for relatively simple econometric analyses.</p>
<p>The following figure illustrates the basic components of Spark. The main functionality includes memory management, task scheduling, and the implementation of Spark’s capabilities to handle and manipulate data distributed across many nodes in parallel. Several built-in libraries extend the core implementation, covering specific domains of practical data analytics tasks (querying structured data via SQL, processing streams of data, machine learning, and network/graph analysis). The last two provide various common functions/algorithms frequently used in data analytics/applied econometrics, such as generalized linear regression, summary statistics, and principal component analysis.</p>
<!-- ```{r sparkstack, echo=FALSE, out.width = "60%", fig.align='center', fig.cap= "(ref:sparkstack)", purl=FALSE} -->
<!-- include_graphics("img/spark_components.jpg") -->
<!-- ``` -->
<!-- (ref:sparkstack) Basic Spark stack (based on https://spark.apache.org/images/spark-stack.png). -->
<p>At the heart of Big Data Analytics with Spark
is the fundamental data structure called ‘resilient distributed dataset’ (RDD). When loading/importing data into Spark, the data is automatically distributed across the cluster in RDDs (~ as distributed collections of elements), and manipulations are then executed in parallel on these RDDs. However, the entire Spark framework also works locally on a simple laptop or desktop computer. This is a great advantage when learning Spark and when testing/debugging an analytics script on a small sample of the real dataset.</p>
</div>
<div id="spark-with-r" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Spark with R<a href="distributed-systems.html#spark-with-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are two prominent packages for using Spark in connection with R: <code>SparkR</code> <span class="citation">(<a href="#ref-SparkR" role="doc-biblioref">Venkataraman et al. 2021</a>)</span> and RStudio’s <code>sparklyr</code> <span class="citation">(<a href="#ref-sparklyr" role="doc-biblioref">Luraschi et al. 2022</a>)</span>. The former is in some ways closer to Spark’s Python API; the latter is closer to the <code>dplyr</code>-type of data handling (and is compatible with the <code>tidyverse</code> <span class="citation">(<a href="#ref-tidyverse" role="doc-biblioref">Wickham et al. 2019</a>)</span>).<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> For the very simple introductory examples below, either package could have been used equally well. For the general introduction we focus on <code>SparkR</code> and later have a look at a simple regression example based on <code>sparklyr</code>.</p>
<p>To install and use Spark from the R shell, only a few preparatory steps are needed. The following examples are based on installing/running Spark on a Linux machine with the <code>SparkR</code> package. <code>SparkR</code> depends on Java (version 8). Thus, we first should make sure the right Java version is installed. If several Java versions are installed, we might have to select version 8 manually via the following terminal command (Linux):</p>
<!-- <!-- in Mac OS (after doing this: https://stackoverflow.com/questions/21964709/how-to-set-or-change-the-default-java-jdk-version-on-os-x) -->
<!-- ```{bash eval = FALSE} -->
<!-- cd  -->
<!-- source .bash_profile -->
<!-- j8 -->
<!-- ``` -->
<!-- ```{r} -->
<!-- system("cd -->
<!--        source .bash_profile -->
<!--        j8") -->
<!-- ``` -->
<p>With the right version of Java running, we can install <code>SparkR</code> from GitHub (needs the <code>devtools</code> package <span class="citation">(<a href="#ref-devtools" role="doc-biblioref">Wickham et al. 2022</a>)</span>) <code>devtools::install_github("cran/SparkR")</code>. After installing <code>SparkR</code>, the call <code>SparkR::install.spark()</code> will download and install Apache Spark to a local directory.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> Now we can start an interactive SparkR session from the terminal with</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb163-1"><a href="distributed-systems.html#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> SPARK-HOME/bin/sparkR</span></code></pre></div>
<p>where <code>SPARK-HOME</code> is a placeholder for the path to your local Spark installation (printed to the console after running <code>SparkR::install.spark()</code>). Or simply run SparkR from within RStudio by loading <code>SparkR</code> and initiating Spark with <code>sparkR.session()</code>.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="distributed-systems.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to install use</span></span>
<span id="cb164-2"><a href="distributed-systems.html#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;cran/SparkR&quot;)</span></span>
<span id="cb164-3"><a href="distributed-systems.html#cb164-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb164-4"><a href="distributed-systems.html#cb164-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SparkR)</span>
<span id="cb164-5"><a href="distributed-systems.html#cb164-5" aria-hidden="true" tabindex="-1"></a><span class="co"># start session</span></span>
<span id="cb164-6"><a href="distributed-systems.html#cb164-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sparkR.session</span>()</span></code></pre></div>
<p>By default this starts a local standalone session (no connection to a cluster computer needed). While the examples below are all intended to run on a local machine, it is straightforward to connect to a remote Spark cluster and run the same examples there.<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a></p>
<div id="data-import-and-summary-statistics" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Data import and summary statistics<a href="distributed-systems.html#data-import-and-summary-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, we want to have a brief look at how to perform the first few steps of a typical econometric analysis: import data and compute summary statistics. We will analyze the already familiar <code>flights.csv</code> dataset. The basic Spark installation provides direct support to import common data formats such as CSV and JSON via the <code>read.df()</code> function (for many additional formats, specific Spark libraries are available). To import<code>flights.csv</code>, we set the <code>source</code> argument to <code>"csv"</code>.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="distributed-systems.html#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import data and create a SparkDataFrame </span></span>
<span id="cb165-2"><a href="distributed-systems.html#cb165-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (a distributed collection of data, RDD)</span></span>
<span id="cb165-3"><a href="distributed-systems.html#cb165-3" aria-hidden="true" tabindex="-1"></a>flights <span class="ot">&lt;-</span> <span class="fu">read.df</span>(<span class="st">&quot;data/flights.csv&quot;</span>, <span class="at">source =</span> <span class="st">&quot;csv&quot;</span>, <span class="at">header=</span><span class="st">&quot;true&quot;</span>)</span>
<span id="cb165-4"><a href="distributed-systems.html#cb165-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-5"><a href="distributed-systems.html#cb165-5" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect the object</span></span>
<span id="cb165-6"><a href="distributed-systems.html#cb165-6" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(flights)</span></code></pre></div>
<pre><code>## [1] &quot;SparkDataFrame&quot;
## attr(,&quot;package&quot;)
## [1] &quot;SparkR&quot;</code></pre>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="distributed-systems.html#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(flights)</span></code></pre></div>
<pre><code>## [1] 336776     19</code></pre>
<p>By default, all variables have been imported as type <code>character</code>. For several variables this is, of course, not the optimal data type to compute summary statistics. We thus first have to convert some columns to other data types with the <code>cast</code> function.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="distributed-systems.html#cb169-1" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>dep_delay <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>dep_delay, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb169-2"><a href="distributed-systems.html#cb169-2" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>dep_time <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>dep_time, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb169-3"><a href="distributed-systems.html#cb169-3" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>arr_time <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>arr_time, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb169-4"><a href="distributed-systems.html#cb169-4" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>arr_delay <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>arr_delay, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb169-5"><a href="distributed-systems.html#cb169-5" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>air_time <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>air_time, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb169-6"><a href="distributed-systems.html#cb169-6" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>distance <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>distance, <span class="st">&quot;double&quot;</span>)</span></code></pre></div>
<p>Suppose we only want to compute average arrival delays per carrier for flights with a distance over 1000 miles. Variable selection and filtering of observations is implemented in <code>select()</code> and <code>filter()</code> (as in the <code>dplyr</code> package).</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="distributed-systems.html#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="co"># filter</span></span>
<span id="cb170-2"><a href="distributed-systems.html#cb170-2" aria-hidden="true" tabindex="-1"></a>long_flights <span class="ot">&lt;-</span> <span class="fu">select</span>(flights, <span class="st">&quot;carrier&quot;</span>, <span class="st">&quot;year&quot;</span>, <span class="st">&quot;arr_delay&quot;</span>, <span class="st">&quot;distance&quot;</span>)</span>
<span id="cb170-3"><a href="distributed-systems.html#cb170-3" aria-hidden="true" tabindex="-1"></a>long_flights <span class="ot">&lt;-</span> <span class="fu">filter</span>(long_flights, long_flights<span class="sc">$</span>distance <span class="sc">&gt;=</span> <span class="dv">1000</span>)</span>
<span id="cb170-4"><a href="distributed-systems.html#cb170-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(long_flights)</span></code></pre></div>
<pre><code>##   carrier year arr_delay distance
## 1      UA 2013        11     1400
## 2      UA 2013        20     1416
## 3      AA 2013        33     1089
## 4      B6 2013       -18     1576
## 5      B6 2013        19     1065
## 6      B6 2013        -2     1028</code></pre>
<p>Now we summarize the arrival delays for the subset of long flights by carrier. This is the ‘split-apply-combine’ approach applied in <code>SparkR</code>.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="distributed-systems.html#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co"># aggregation: mean delay per carrier</span></span>
<span id="cb172-2"><a href="distributed-systems.html#cb172-2" aria-hidden="true" tabindex="-1"></a>long_flights_delays<span class="ot">&lt;-</span> <span class="fu">summarize</span>(<span class="fu">groupBy</span>(long_flights, long_flights<span class="sc">$</span>carrier),</span>
<span id="cb172-3"><a href="distributed-systems.html#cb172-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">avg_delay =</span> <span class="fu">mean</span>(long_flights<span class="sc">$</span>arr_delay))</span>
<span id="cb172-4"><a href="distributed-systems.html#cb172-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(long_flights_delays)</span></code></pre></div>
<pre><code>##   carrier avg_delay
## 1      UA    3.2622
## 2      AA    0.4958
## 3      EV   15.6876
## 4      B6    9.0364
## 5      DL   -0.2394
## 6      OO   -2.0000</code></pre>
<p>Finally, we want to convert the result back into a usual <code>data.frame</code> (loaded in our current R session) in order to further process the summary statistics (output to LaTeX table, plot, etc.). Note that as in the previous aggregation exercises with the <code>ff</code> package, the computed summary statistics (in the form of a table/df) are obviously much smaller than the raw data. However, note that converting a <code>SparkDataFrame</code> back into a native R object generally means all the data stored in the RDDs constituting the <code>SparkDataFrame</code> object is loaded into local RAM. Hence, when working with actual Big Data on a Spark cluster, this type of operation can quickly overflow local RAM.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="distributed-systems.html#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert result back into native R object</span></span>
<span id="cb174-2"><a href="distributed-systems.html#cb174-2" aria-hidden="true" tabindex="-1"></a>delays <span class="ot">&lt;-</span> <span class="fu">collect</span>(long_flights_delays)</span>
<span id="cb174-3"><a href="distributed-systems.html#cb174-3" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(delays)</span></code></pre></div>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="distributed-systems.html#cb176-1" aria-hidden="true" tabindex="-1"></a>delays</span></code></pre></div>
<pre><code>##    carrier avg_delay
## 1       UA    3.2622
## 2       AA    0.4958
## 3       EV   15.6876
## 4       B6    9.0364
## 5       DL   -0.2394
## 6       OO   -2.0000
## 7       F9   21.9207
## 8       US    0.5567
## 9       MQ    8.2331
## 10      HA   -6.9152
## 11      AS   -9.9309
## 12      VX    1.7645
## 13      WN    9.0842
## 14      9E    6.6730</code></pre>
</div>
</div>
<div id="spark-with-sql" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Spark with SQL<a href="distributed-systems.html#spark-with-sql" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>
Instead of interacting with Spark via R, you can do the same via SQL. This can be very convenient at the stage of data exploration and data preparation. Also note that this is a very good example of how knowing some SQL can be very useful when working with Big Data even if you are not interacting with an actual relational database.<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a></p>
<p>To directly interact with Spark via SQL, open a terminal window, switch to the <code>SPARK-HOME</code> directory,</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb178-1"><a href="distributed-systems.html#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> SPARK-HOME</span></code></pre></div>
<p>and enter the following command:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb179-1"><a href="distributed-systems.html#cb179-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb179-2"><a href="distributed-systems.html#cb179-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> bin/spark-sql</span></code></pre></div>
<p>where <code>SPARK-HOME</code> is again the placeholder for the path to your local Spark installation (printed to the console after running <code>SparkR::install.spark()</code>). This will start up Spark and connect to it via Spark’s SQL interface. You will notice that the prompt in the terminal changes (similar to when you start <code>sqlite</code>).</p>
<p>Let’s run some example queries. The Spark installation comes with several data and script examples. The example datasets are located at <code>SPARK-HOME/examples/src/main/resources</code>. For example, the file <code>employees.json</code> contains the following records in JSON format:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb180-1"><a href="distributed-systems.html#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;name&quot;</span><span class="fu">:</span><span class="st">&quot;Michael&quot;</span><span class="fu">,</span> <span class="dt">&quot;salary&quot;</span><span class="fu">:</span><span class="dv">3000</span><span class="fu">}</span></span>
<span id="cb180-2"><a href="distributed-systems.html#cb180-2" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;name&quot;</span><span class="fu">:</span><span class="st">&quot;Andy&quot;</span><span class="fu">,</span> <span class="dt">&quot;salary&quot;</span><span class="fu">:</span><span class="dv">4500</span><span class="fu">}</span></span>
<span id="cb180-3"><a href="distributed-systems.html#cb180-3" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;name&quot;</span><span class="fu">:</span><span class="st">&quot;Justin&quot;</span><span class="fu">,</span> <span class="dt">&quot;salary&quot;</span><span class="fu">:</span><span class="dv">3500</span><span class="fu">}</span></span>
<span id="cb180-4"><a href="distributed-systems.html#cb180-4" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;name&quot;</span><span class="fu">:</span><span class="st">&quot;Berta&quot;</span><span class="fu">,</span> <span class="dt">&quot;salary&quot;</span><span class="fu">:</span><span class="dv">4000</span><span class="fu">}</span></span></code></pre></div>
<p>We can query this data directly via SQL commands by referring to the location of the original JSON file.</p>
<p><strong>Select all observations</strong></p>
<div class="sourceCode" id="cb181"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb181-1"><a href="distributed-systems.html#cb181-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb181-2"><a href="distributed-systems.html#cb181-2" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span> </span>
<span id="cb181-3"><a href="distributed-systems.html#cb181-3" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> json.`examples<span class="op">/</span>src<span class="op">/</span>main<span class="op">/</span>resources<span class="op">/</span>employees.json`</span>
<span id="cb181-4"><a href="distributed-systems.html#cb181-4" aria-hidden="true" tabindex="-1"></a>;</span></code></pre></div>
<pre><code>Michael 3000
Andy    4500
Justin  3500
Berta   4000
Time taken: 0.099 seconds, Fetched 4 row(s)</code></pre>
<p><strong>Filter observations</strong></p>
<div class="sourceCode" id="cb183"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb183-1"><a href="distributed-systems.html#cb183-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-2"><a href="distributed-systems.html#cb183-2" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="op">*</span> </span>
<span id="cb183-3"><a href="distributed-systems.html#cb183-3" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> json.`examples<span class="op">/</span>src<span class="op">/</span>main<span class="op">/</span>resources<span class="op">/</span>employees.json`</span>
<span id="cb183-4"><a href="distributed-systems.html#cb183-4" aria-hidden="true" tabindex="-1"></a><span class="kw">WHERE</span> salary <span class="op">&lt;</span><span class="dv">4000</span></span>
<span id="cb183-5"><a href="distributed-systems.html#cb183-5" aria-hidden="true" tabindex="-1"></a>;</span></code></pre></div>
<pre><code>Michael 3000
Justin  3500
Time taken: 0.125 seconds, Fetched 2 row(s)</code></pre>
<p><strong>Compute the average salary</strong></p>
<div class="sourceCode" id="cb185"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb185-1"><a href="distributed-systems.html#cb185-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-2"><a href="distributed-systems.html#cb185-2" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="fu">AVG</span>(salary) <span class="kw">AS</span> mean_salary </span>
<span id="cb185-3"><a href="distributed-systems.html#cb185-3" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> json.`examples<span class="op">/</span>src<span class="op">/</span>main<span class="op">/</span>resources<span class="op">/</span>employees.json`;</span></code></pre></div>
<pre><code>3750.0
Time taken: 0.142 seconds, Fetched 1 row(s)</code></pre>
</div>
<div id="spark-with-r-sql" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Spark with R + SQL<a href="distributed-systems.html#spark-with-r-sql" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most conveniently, you can combine the SQL query features of Spark and SQL with running R on Spark. First, initiate the Spark session in RStudio and import the data as a Spark data frame.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="distributed-systems.html#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to install use</span></span>
<span id="cb187-2"><a href="distributed-systems.html#cb187-2" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;cran/SparkR&quot;)</span></span>
<span id="cb187-3"><a href="distributed-systems.html#cb187-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb187-4"><a href="distributed-systems.html#cb187-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SparkR)</span>
<span id="cb187-5"><a href="distributed-systems.html#cb187-5" aria-hidden="true" tabindex="-1"></a><span class="co"># start session</span></span>
<span id="cb187-6"><a href="distributed-systems.html#cb187-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sparkR.session</span>()</span></code></pre></div>
<pre><code>## Java ref type org.apache.spark.sql.SparkSession id 1</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="distributed-systems.html#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read data </span></span>
<span id="cb189-2"><a href="distributed-systems.html#cb189-2" aria-hidden="true" tabindex="-1"></a>flights <span class="ot">&lt;-</span> <span class="fu">read.df</span>(<span class="st">&quot;data/flights.csv&quot;</span>, <span class="at">source =</span> <span class="st">&quot;csv&quot;</span>, <span class="at">header=</span><span class="st">&quot;true&quot;</span>)</span></code></pre></div>
<p>Now we can make the Spark data frame accessible for SQL queries by registering it as a temporary table/view with <code>createOrReplaceTempView()</code> and then run SQL queries on it from within the R session via the <code>sql()</code>-function. <code>sql()</code> will return the results as a Spark data frame (this means the result is also located on the cluster and hardly affects the master node’s memory).</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="distributed-systems.html#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="co"># register the data frame as a table</span></span>
<span id="cb190-2"><a href="distributed-systems.html#cb190-2" aria-hidden="true" tabindex="-1"></a><span class="fu">createOrReplaceTempView</span>(flights, <span class="st">&quot;flights&quot;</span> )</span>
<span id="cb190-3"><a href="distributed-systems.html#cb190-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-4"><a href="distributed-systems.html#cb190-4" aria-hidden="true" tabindex="-1"></a><span class="co"># now run SQL queries on it</span></span>
<span id="cb190-5"><a href="distributed-systems.html#cb190-5" aria-hidden="true" tabindex="-1"></a>query <span class="ot">&lt;-</span> </span>
<span id="cb190-6"><a href="distributed-systems.html#cb190-6" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;SELECT DISTINCT carrier,</span></span>
<span id="cb190-7"><a href="distributed-systems.html#cb190-7" aria-hidden="true" tabindex="-1"></a><span class="st">year,</span></span>
<span id="cb190-8"><a href="distributed-systems.html#cb190-8" aria-hidden="true" tabindex="-1"></a><span class="st">arr_delay,</span></span>
<span id="cb190-9"><a href="distributed-systems.html#cb190-9" aria-hidden="true" tabindex="-1"></a><span class="st">distance</span></span>
<span id="cb190-10"><a href="distributed-systems.html#cb190-10" aria-hidden="true" tabindex="-1"></a><span class="st">FROM flights</span></span>
<span id="cb190-11"><a href="distributed-systems.html#cb190-11" aria-hidden="true" tabindex="-1"></a><span class="st">WHERE 1000 &lt;= distance&quot;</span></span>
<span id="cb190-12"><a href="distributed-systems.html#cb190-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-13"><a href="distributed-systems.html#cb190-13" aria-hidden="true" tabindex="-1"></a>long_flights2 <span class="ot">&lt;-</span> <span class="fu">sql</span>(query)</span>
<span id="cb190-14"><a href="distributed-systems.html#cb190-14" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(long_flights2)</span></code></pre></div>
<pre><code>##   carrier year arr_delay distance
## 1      DL 2013       -30     1089
## 2      UA 2013       -11     1605
## 3      DL 2013       -42     1598
## 4      UA 2013        -5     1585
## 5      AA 2013         6     1389
## 6      UA 2013       -23     1620</code></pre>
</div>
<div id="wrapping-up-2" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Wrapping up<a href="distributed-systems.html#wrapping-up-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>At the core of a vertical scaling strategy are so-called <em>distributed systems</em> – several computers connected in a network to jointly process large amounts of data.</li>
<li>In contrast to standard parallel-computing, the different computing nodes in a distributed system do not share the same physical memory. Each of the nodes/computers in the system has its own CPU, hard disk, and RAM. This architecture requires a different computing paradigm to run the same data analytics job across all nodes (in parallel).</li>
<li>A commonly use paradigm to do this is MapReduce, which is implemented in software called Apache Hadoop.</li>
<li>The core idea of MapReduce is to split a problem/computing task on a large dataset into several components, each of which focuses on a smaller subset of the dataset. The task components are then distributed across the cluster, so that each component is handled by one computer in the network. Finally, each node returns its result to the master node (the computer coordinating all activities in the cluster), where the partial results are combined into the overall result.</li>
<li>A typical example of a MapReduce job is the computation of term frequencies in a large body of text. Here, each node computes the number of occurrences of specific words in a subset of the overall body of text; the individual results are then summed up per unique word.</li>
<li>Apache Hadoop is a collection of open-source software tools to work with massive amounts of data on a distributed system (a network of computers). Part of Hadoop is the Hadoop MapReduce implementation to run MapReduce jobs on a Hadoop cluster.</li>
<li>Apache Spark is an analytics engine for large-scale data processing on local machines or clusters. It improves upon several shortcomings of the previous Hadoop/MapReduce framework, in particular with regard to iterative tasks (such as in machine learning).</li>
</ul>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-sparklyr" class="csl-entry">
Luraschi, Javier, Kevin Kuo, Kevin Ushey, JJ Allaire, Hossein Falaki, Lu Wang, Andy Zhang, Yitao Li, Edgar Ruiz, and The Apache Software Foundation. 2022. <em><span class="nocase">sparklyr: R Interface to Apache Spark</span></em>. <a href="https://spark.rstudio.com/">https://spark.rstudio.com/</a>.
</div>
<div id="ref-SparkR" class="csl-entry">
Venkataraman, Shivaram, Xiangrui Meng, Felix Cheung, and The Apache Software Foundation. 2021. <em><span class="nocase">SparkR: R Front End for Apache Spark</span></em>. <a href="https://CRAN.R-project.org/package=SparkR">https://CRAN.R-project.org/package=SparkR</a>.
</div>
<div id="ref-tidyverse" class="csl-entry">
Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. <span>“Welcome to the <span class="nocase">tidyverse</span>.”</span> <em>Journal of Open Source Software</em> 4 (43): 1686. <a href="https://doi.org/10.21105/joss.01686">https://doi.org/10.21105/joss.01686</a>.
</div>
<div id="ref-devtools" class="csl-entry">
Wickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2022. <em>Devtools: Tools to Make Developing r Packages Easier</em>. <a href="https://CRAN.R-project.org/package=devtools">https://CRAN.R-project.org/package=devtools</a>.
</div>
<div id="ref-Spark" class="csl-entry">
Zaharia, Matei, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, et al. 2016. <span>“<span class="nocase">Apache Spark: A Unified Engine for Big Data Processing</span>.”</span> <em>Commun. ACM</em> 59 (11): 56–65. <a href="https://doi.org/10.1145/2934664">https://doi.org/10.1145/2934664</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="33">
<li id="fn33"><p>See, e.g., <a href="https://commons.wikimedia.org/wiki/File:WordCountFlow.JPG" class="uri">https://commons.wikimedia.org/wiki/File:WordCountFlow.JPG</a> for an illustration of the same example<a href="distributed-systems.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>For a more detailed discussion of what <code>map</code> and <code>reduce</code> <em>actually</em> have to do with MapReduce, see <a href="https://medium.com/@jkff/mapreduce-is-not-functional-programming-39109a4ba7b2" class="uri">https://medium.com/@jkff/mapreduce-is-not-functional-programming-39109a4ba7b2</a>.<a href="distributed-systems.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>More sophisticated programs need to be custom made, written in Java.<a href="distributed-systems.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>See <a href="https://cosminsanda.com/posts/a-compelling-case-for-sparkr/">this blog post</a> for a more detailed comparison and discussion of advantages of either package.<a href="distributed-systems.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>Note that after the installation, the location of Spark is printed to the R console. Alternatively, you can also first install the <code>sparklyr</code> package and then run <code>sparklyr::spark_install()</code> to install Spark. In the data analysis examples later in the book, we will work both with <code>SparkR</code> and <code>sparklyr</code>.<a href="distributed-systems.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p>Simply set the <code>master</code> argument of <code>sparkR.session()</code> to the URL of the Spark master node of the remote cluster. Importantly, the local Spark and Hadoop versions should match the corresponding versions on the remote cluster.<a href="distributed-systems.html#fnref38" class="footnote-back">↩︎</a></p></li>
<li id="fn39"><p>Importantly, this also means that we cannot use SQL commands related to configuring such databases, such as <code>.tables</code> etc. Instead we use SQL commands to directly query data from JSON or CSV files.<a href="distributed-systems.html#fnref39" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hardware-computing-resources.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cloud-computing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
