<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Applied Econometrics with Apache Spark | Big Data Analytics</title>
  <meta name="description" content="A guide to practical big data analytics in R." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Applied Econometrics with Apache Spark | Big Data Analytics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://umatter.github.io/BigData" />
  <meta property="og:image" content="https://umatter.github.io/BigDataimg/cover.png" />
  <meta property="og:description" content="A guide to practical big data analytics in R." />
  <meta name="github-repo" content="umatter/BigData" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Applied Econometrics with Apache Spark | Big Data Analytics" />
  
  <meta name="twitter:description" content="A guide to practical big data analytics in R." />
  <meta name="twitter:image" content="https://umatter.github.io/BigDataimg/cover.png" />

<meta name="author" content="Ulrich Matter" />


<meta name="date" content="2022-02-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bottle-necks-in-local-big-data-analytics.html"/>
<link rel="next" href="appendix-a.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/d3/d3.min.js"></script>
<link href="libs/profvis/profvis.css" rel="stylesheet" />
<script src="libs/profvis/profvis.js"></script>
<script src="libs/profvis/scroll.js"></script>
<link href="libs/highlight/textmate.css" rel="stylesheet" />
<script src="libs/highlight/highlight.js"></script>
<script src="libs/profvis-binding/profvis.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-big-in-big-data"><i class="fa fa-check"></i><b>1.1</b> What is <em>big</em> in “Big Data?”</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#approaches-to-analyzing-big-data"><i class="fa fa-check"></i><b>1.2</b> Approaches to analyzing Big Data</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#content-overview"><i class="fa fa-check"></i><b>1.3</b> Content overview</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#prerequisits"><i class="fa fa-check"></i><b>1.4</b> Prerequisits</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html"><i class="fa fa-check"></i><b>2</b> Software: Programming with (Big) Data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#building-blocks-for-programming-with-big-data"><i class="fa fa-check"></i><b>2.1</b> Building blocks for programming with big data</a></li>
<li class="chapter" data-level="2.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#measuring-r-performance"><i class="fa fa-check"></i><b>2.2</b> Measuring R performance</a></li>
<li class="chapter" data-level="2.3" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#writing-efficient-code"><i class="fa fa-check"></i><b>2.3</b> Writing efficient code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#memory-allocation-and-growing-objects"><i class="fa fa-check"></i><b>2.3.1</b> Memory allocation and growing objects</a></li>
<li class="chapter" data-level="2.3.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#vectorization-in-basic-r-functions"><i class="fa fa-check"></i><b>2.3.2</b> Vectorization in basic R functions</a></li>
<li class="chapter" data-level="2.3.3" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#apply-type-functions-and-vectorization"><i class="fa fa-check"></i><b>2.3.3</b> <code>apply</code>-type functions and vectorization</a></li>
<li class="chapter" data-level="2.3.4" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#avoid-unnecessary-copying"><i class="fa fa-check"></i><b>2.3.4</b> Avoid unnecessary copying</a></li>
<li class="chapter" data-level="2.3.5" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#releasing-memory"><i class="fa fa-check"></i><b>2.3.5</b> Releasing memory</a></li>
<li class="chapter" data-level="2.3.6" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#beyond-r"><i class="fa fa-check"></i><b>2.3.6</b> Beyond R</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#sql-basics"><i class="fa fa-check"></i><b>2.4</b> SQL basics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#first-steps-in-sqlite"><i class="fa fa-check"></i><b>2.4.1</b> First steps in SQL(ite)</a></li>
<li class="chapter" data-level="2.4.2" data-path="software-programming-with-big-data.html"><a href="software-programming-with-big-data.html#joins"><i class="fa fa-check"></i><b>2.4.2</b> Joins</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html"><i class="fa fa-check"></i><b>3</b> Hardware: Computing Resources</a>
<ul>
<li class="chapter" data-level="3.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#components-of-a-standard-computing-environment"><i class="fa fa-check"></i><b>3.1</b> Components of a standard computing environment</a></li>
<li class="chapter" data-level="3.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#mass-storage-and-memory"><i class="fa fa-check"></i><b>3.2</b> Mass Storage and Memory</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#units-of-informationdata-storage"><i class="fa fa-check"></i><b>3.2.1</b> Units of information/data storage</a></li>
<li class="chapter" data-level="3.2.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#combining-ram-and-hard-disk-virtual-memory"><i class="fa fa-check"></i><b>3.2.2</b> Combining RAM and hard-disk: virtual memory</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#computation-cpu"><i class="fa fa-check"></i><b>3.3</b> Computation: CPU</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#parallelization"><i class="fa fa-check"></i><b>3.3.1</b> Parallelization</a></li>
<li class="chapter" data-level="3.3.2" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#gpus-for-scientific-computing"><i class="fa fa-check"></i><b>3.3.2</b> GPUs for Scientific Computing</a></li>
<li class="chapter" data-level="3.3.3" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#gpus-in-r"><i class="fa fa-check"></i><b>3.3.3</b> GPUs in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="hardware-computing-resources.html"><a href="hardware-computing-resources.html#resource-allocation"><i class="fa fa-check"></i><b>3.4</b> Resource allocation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distributed-systems.html"><a href="distributed-systems.html"><i class="fa fa-check"></i><b>4</b> Distributed Systems</a>
<ul>
<li class="chapter" data-level="4.1" data-path="distributed-systems.html"><a href="distributed-systems.html#mapreduce"><i class="fa fa-check"></i><b>4.1</b> MapReduce</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="distributed-systems.html"><a href="distributed-systems.html#mapreduce-concept-illustrated-in-r"><i class="fa fa-check"></i><b>4.1.1</b> Map/Reduce Concept Illustrated in R</a></li>
<li class="chapter" data-level="4.1.2" data-path="distributed-systems.html"><a href="distributed-systems.html#mapper"><i class="fa fa-check"></i><b>4.1.2</b> Mapper</a></li>
<li class="chapter" data-level="4.1.3" data-path="distributed-systems.html"><a href="distributed-systems.html#reducer"><i class="fa fa-check"></i><b>4.1.3</b> Reducer</a></li>
<li class="chapter" data-level="4.1.4" data-path="distributed-systems.html"><a href="distributed-systems.html#simpler-example-compute-the-total-number-of-words"><i class="fa fa-check"></i><b>4.1.4</b> Simpler example: Compute the total number of words</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="distributed-systems.html"><a href="distributed-systems.html#hadoop"><i class="fa fa-check"></i><b>4.2</b> Hadoop</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="distributed-systems.html"><a href="distributed-systems.html#hadoop-word-count"><i class="fa fa-check"></i><b>4.2.1</b> Hadoop Word Count</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cloud-computing.html"><a href="cloud-computing.html"><i class="fa fa-check"></i><b>5</b> Cloud Computing</a>
<ul>
<li class="chapter" data-level="5.1" data-path="cloud-computing.html"><a href="cloud-computing.html#scaling-up-parallel-computing"><i class="fa fa-check"></i><b>5.1</b> Scaling up: parallel computing</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="cloud-computing.html"><a href="cloud-computing.html#setting-up-aws-ec2-with-rrstudio"><i class="fa fa-check"></i><b>5.1.1</b> Setting up AWS EC2 with R/RStudio</a></li>
<li class="chapter" data-level="5.1.2" data-path="cloud-computing.html"><a href="cloud-computing.html#parallelization-with-an-ec2-instance"><i class="fa fa-check"></i><b>5.1.2</b> Parallelization with an EC2 instance</a></li>
<li class="chapter" data-level="5.1.3" data-path="cloud-computing.html"><a href="cloud-computing.html#aws-emr-mapreduce-in-the-cloud"><i class="fa fa-check"></i><b>5.1.3</b> AWS EMR: MapReduce in the cloud</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html"><i class="fa fa-check"></i><b>6</b> Data Storage and Databases</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#big-data-storage"><i class="fa fa-check"></i><b>6.1</b> (Big) Data Storage</a></li>
<li class="chapter" data-level="6.2" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#rdbms-basics"><i class="fa fa-check"></i><b>6.2</b> RDBMS basics</a></li>
<li class="chapter" data-level="6.3" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#row-based-vs-column-based-databases"><i class="fa fa-check"></i><b>6.3</b> Row-based vs column-based databases</a></li>
<li class="chapter" data-level="6.4" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#getting-started-with-rsqlite"><i class="fa fa-check"></i><b>6.4</b> Getting started with RSQLite</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#warm-up-in-sqlite-indices-and-joins"><i class="fa fa-check"></i><b>6.4.1</b> Warm up in SQLite: indices and joins</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#sqlite-from-within-r"><i class="fa fa-check"></i><b>6.5</b> SQLite from within R</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#creating-a-new-database-with-rsqlite"><i class="fa fa-check"></i><b>6.5.1</b> Creating a new database with <code>RSQLite</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#importing-data"><i class="fa fa-check"></i><b>6.5.2</b> Importing data</a></li>
<li class="chapter" data-level="6.5.3" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#issue-queries"><i class="fa fa-check"></i><b>6.5.3</b> Issue queries</a></li>
<li class="chapter" data-level="6.5.4" data-path="data-storage-and-databases.html"><a href="data-storage-and-databases.html#mass-storage-mariadb-on-an-ec2-instance"><i class="fa fa-check"></i><b>6.5.4</b> Mass Storage: MariaDB on an EC2 instance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html"><i class="fa fa-check"></i><b>7</b> Big Data Cleaning and Transformation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#out-of-memory-strategies"><i class="fa fa-check"></i><b>7.1</b> ‘Out-of-memory’ strategies</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#chunking-data-with-the-ff-package"><i class="fa fa-check"></i><b>7.1.1</b> Chunking data with the <code>ff</code>-package</a></li>
<li class="chapter" data-level="7.1.2" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#memory-mapping-with-bigmemory"><i class="fa fa-check"></i><b>7.1.2</b> Memory mapping with <code>bigmemory</code></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#typical-cleaning-tasks"><i class="fa fa-check"></i><b>7.2</b> Typical cleaning tasks</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="big-data-cleaning-and-transformation.html"><a href="big-data-cleaning-and-transformation.html#data-preparation-with-ff"><i class="fa fa-check"></i><b>7.2.1</b> Data Preparation with <code>ff</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html"><i class="fa fa-check"></i><b>8</b> Descriptive Statistics and Aggregation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#data-aggregation-the-split-apply-combine-strategy"><i class="fa fa-check"></i><b>8.1</b> Data aggregation: The ‘split-apply-combine’ strategy</a></li>
<li class="chapter" data-level="8.2" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#data-aggregation-tutorial"><i class="fa fa-check"></i><b>8.2</b> Data aggregation tutorial</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#gathering-and-compilation-of-all-the-raw-data"><i class="fa fa-check"></i><b>8.2.1</b> Gathering and Compilation of all the raw data</a></li>
<li class="chapter" data-level="8.2.2" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#data-aggregation-with-chunked-data-files"><i class="fa fa-check"></i><b>8.2.2</b> Data aggregation with chunked data files</a></li>
<li class="chapter" data-level="8.2.3" data-path="descriptive-statistics-and-aggregation.html"><a href="descriptive-statistics-and-aggregation.html#high-speed-in-memory-data-aggregation-with-data.table"><i class="fa fa-check"></i><b>8.2.3</b> High-speed in-memory data aggregation with <code>data.table</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="big-data-visualization.html"><a href="big-data-visualization.html"><i class="fa fa-check"></i><b>9</b> (Big) Data Visualization</a>
<ul>
<li class="chapter" data-level="9.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#data-set"><i class="fa fa-check"></i><b>9.1</b> Data Set</a></li>
<li class="chapter" data-level="9.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#excursus-modify-and-create-themes"><i class="fa fa-check"></i><b>9.2</b> Excursus: modify and create themes</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#create-your-own-theme-simple-approach"><i class="fa fa-check"></i><b>9.2.1</b> Create your own theme: simple approach</a></li>
<li class="chapter" data-level="9.2.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#implementing-actual-themes-as-functions."><i class="fa fa-check"></i><b>9.2.2</b> Implementing actual themes as functions.</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="big-data-visualization.html"><a href="big-data-visualization.html#visualize-time-and-space"><i class="fa fa-check"></i><b>9.3</b> Visualize Time and Space</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="big-data-visualization.html"><a href="big-data-visualization.html#preparations"><i class="fa fa-check"></i><b>9.3.1</b> Preparations</a></li>
<li class="chapter" data-level="9.3.2" data-path="big-data-visualization.html"><a href="big-data-visualization.html#pick-up-and-drop-off-locations"><i class="fa fa-check"></i><b>9.3.2</b> Pick-up and drop-off locations</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="big-data-visualization.html"><a href="big-data-visualization.html#excursus-change-color-schemes"><i class="fa fa-check"></i><b>9.4</b> Excursus: change color schemes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html"><i class="fa fa-check"></i><b>10</b> Bottle Necks in Local Big Data Analytics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#example-of-computation-time-and-memory-allocation"><i class="fa fa-check"></i><b>10.1</b> Example of Computation Time and Memory Allocation</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#preparation"><i class="fa fa-check"></i><b>10.1.1</b> Preparation</a></li>
<li class="chapter" data-level="10.1.2" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#naïve-approach-ignorant-of-r"><i class="fa fa-check"></i><b>10.1.2</b> Naïve Approach (ignorant of R)</a></li>
<li class="chapter" data-level="10.1.3" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#improvement-1-pre-allocation-of-memory"><i class="fa fa-check"></i><b>10.1.3</b> Improvement 1: Pre-allocation of memory</a></li>
<li class="chapter" data-level="10.1.4" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#improvement-2-exploit-vectorization"><i class="fa fa-check"></i><b>10.1.4</b> Improvement 2: Exploit vectorization</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#case-study-parallel-processing"><i class="fa fa-check"></i><b>10.2</b> Case study: Parallel processing</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#parallelization-with-an-ec2-instance-1"><i class="fa fa-check"></i><b>10.2.1</b> Parallelization with an EC2 instance</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#case-study-memory-allocation"><i class="fa fa-check"></i><b>10.3</b> Case study: Memory allocation</a></li>
<li class="chapter" data-level="10.4" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#big-data-econometrics"><i class="fa fa-check"></i><b>10.4</b> Big Data econometrics</a></li>
<li class="chapter" data-level="10.5" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#example-fast-least-squares-regression"><i class="fa fa-check"></i><b>10.5</b> Example: Fast least squares regression</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#ols-as-a-point-of-reference"><i class="fa fa-check"></i><b>10.5.1</b> OLS as a point of reference</a></li>
<li class="chapter" data-level="10.5.2" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#the-uluru-algorithm-as-an-alternative-to-ols"><i class="fa fa-check"></i><b>10.5.2</b> The Uluru algorithm as an alternative to OLS</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#gpus-and-machine-learning"><i class="fa fa-check"></i><b>10.6</b> GPUs and Machine Learning</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#tensorflowkeras-example-predict-housing-prices"><i class="fa fa-check"></i><b>10.6.1</b> Tensorflow/Keras example: predict housing prices</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="bottle-necks-in-local-big-data-analytics.html"><a href="bottle-necks-in-local-big-data-analytics.html#a-word-of-caution"><i class="fa fa-check"></i><b>10.7</b> A word of caution</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="applied-econometrics-with-apache-spark.html"><a href="applied-econometrics-with-apache-spark.html"><i class="fa fa-check"></i><b>11</b> Applied Econometrics with Apache Spark</a>
<ul>
<li class="chapter" data-level="11.1" data-path="applied-econometrics-with-apache-spark.html"><a href="applied-econometrics-with-apache-spark.html#spark-basics"><i class="fa fa-check"></i><b>11.1</b> Spark basics</a></li>
<li class="chapter" data-level="11.2" data-path="applied-econometrics-with-apache-spark.html"><a href="applied-econometrics-with-apache-spark.html#spark-in-r"><i class="fa fa-check"></i><b>11.2</b> Spark in R</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="applied-econometrics-with-apache-spark.html"><a href="applied-econometrics-with-apache-spark.html#data-import-and-summary-statistics"><i class="fa fa-check"></i><b>11.2.1</b> Data import and summary statistics</a></li>
<li class="chapter" data-level="11.2.2" data-path="applied-econometrics-with-apache-spark.html"><a href="applied-econometrics-with-apache-spark.html#regression-analysis-with-sparklyr"><i class="fa fa-check"></i><b>11.2.2</b> Regression analysis with <code>sparklyr</code></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i><b>A</b> Appendix A</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appendix-a.html"><a href="appendix-a.html#github"><i class="fa fa-check"></i><b>A.1</b> GitHub</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appendix-a.html"><a href="appendix-a.html#initiate-a-new-repository"><i class="fa fa-check"></i><b>A.1.1</b> Initiate a new repository</a></li>
<li class="chapter" data-level="A.1.2" data-path="appendix-a.html"><a href="appendix-a.html#clone-this-courses-repository"><i class="fa fa-check"></i><b>A.1.2</b> Clone this course’s repository</a></li>
<li class="chapter" data-level="A.1.3" data-path="appendix-a.html"><a href="appendix-a.html#fork-this-courses-repository"><i class="fa fa-check"></i><b>A.1.3</b> Fork this course’s repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i><b>B</b> Appendix B</a>
<ul>
<li class="chapter" data-level="B.0.1" data-path="appendix-b.html"><a href="appendix-b.html#data-types-and-memorystorage"><i class="fa fa-check"></i><b>B.0.1</b> Data types and memory/storage</a></li>
<li class="chapter" data-level="B.0.2" data-path="appendix-b.html"><a href="appendix-b.html#data-structures"><i class="fa fa-check"></i><b>B.0.2</b> Data structures</a></li>
<li class="chapter" data-level="B.0.3" data-path="appendix-b.html"><a href="appendix-b.html#vectors-vs-factors-in-r"><i class="fa fa-check"></i><b>B.0.3</b> Vectors vs Factors in R</a></li>
<li class="chapter" data-level="B.0.4" data-path="appendix-b.html"><a href="appendix-b.html#r-tools-to-investigate-structures-and-types"><i class="fa fa-check"></i><b>B.0.4</b> R-tools to investigate structures and types</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://umatter.github.io" target="blank">umatter.github.io</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="applied-econometrics-with-apache-spark" class="section level1" number="11">
<h1><span class="header-section-number">Chapter 11</span> Applied Econometrics with Apache Spark</h1>
<!-- TODO: -->
<!-- - cover http://localhost:4040 briefly -->
<!-- - cover more of sparklyr -->
<div id="spark-basics" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> Spark basics</h2>
<p>Building on the MapReduce model discussed in the previous lecture, <a href="https://spark.apache.org/">Apache Spark</a> is a cluster computing platform particularly made for data analytics. From the technical perspective (in very simple terms), Spark improves some shortcomings of the older <a href="https://hadoop.apache.org/">Hadoop</a> platform, further improving efficiency/speed. More importantly for our purposes, in contrast to Hadoop, Spark is specifically made for analytics tasks and therefore more easily accessible for people with an applied econometrics background but no substantial knowledge in MapReduce cluster computing. In particular, it comes with several high-level operators that make it rather easy to implement analytics tasks. Moreover (in contrast to basic Hadoop), its very easy to use interactively from R, Python, Scala, and SQL. This makes the platform much more accessible and worth the while for empirical economic research, even for relatively simple econometric analyses.</p>
<p>The following figure illustrates the basic components of Spark. The main functionality, including memory management, task scheduling, and the implementation of Spark’s capabilities to handle and manipulate data distributed across many nodes in parallel. Several built-in libraries extend the core implementation, covering specific domains of practical data analytics tasks (querying structured data via SQL, processing streams of data, machine learning, and network/graph analysis). The latter two provide various common functions/algorithms frequently used in data analytics/applied econometrics, such as generalized linear regression, summary statistics, and principal component analysis.</p>
<div class="figure" style="text-align: center"><span id="fig:sparkstack"></span>
<img src="img/spark-stack.png" alt="Basic Spark stack (source: https://spark.apache.org/images/spark-stack.png)" width="60%" />
<p class="caption">
Figure 11.1: Basic Spark stack (source: <a href="https://spark.apache.org/images/spark-stack.png" class="uri">https://spark.apache.org/images/spark-stack.png</a>)
</p>
</div>

<p>At the heart of big data analytics with Spark is the fundamental data structure called ‘resilient distributed dataset’ (RDD). When loading/importing data into Spark, the data is automatically distributed across the cluster in RDDs (~ as distributed collections of elements) and manipulations are then executed in parallel in these RDDs. However, the entire Spark framework also works locally on a simple laptop or desktop computer. This is of great advantage when learning Spark and testing/debugging an analytics script on a small sample of the real dataset.</p>
</div>
<div id="spark-in-r" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Spark in R</h2>
<p>There are two prominent packages to use Spark in connection to R: <code>SparkR</code> and RStudio’s <code>sparklyr</code>, the former is in some ways closer to Spark’s Python API, the latter is closer to the <code>dplyr</code>-type of data handling (and is ‘compatible’ with the ‘<code>tidyverse</code>’).<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> For the very simple introductory examples below, either package could have been used equally well. For the general introduction we focus on <code>SparkR</code> and later have a look at a simple regression example based on <code>sparklyr</code>.</p>
<p>To install and use Spark from the R shell, only a few preparatory steps are needed. The following examples are based on installing/running Spark on a Linux machine with the <code>SparkR</code> package. <code>SparkR</code> depends on Java (version 8). Thus, we first should make sure the right Java version is installed. If several Java versions are installed, we might have to select version 8 manually via the following terminal command (Linux).</p>
<!-- <!-- in Mac OS (after doing this: https://stackoverflow.com/questions/21964709/how-to-set-or-change-the-default-java-jdk-version-on-os-x) -->
<!-- ```{bash eval = FALSE} -->
<!-- cd  -->
<!-- source .bash_profile -->
<!-- j8 -->
<!-- ``` -->
<!-- ```{r} -->
<!-- system("cd -->
<!--        source .bash_profile -->
<!--        j8") -->
<!-- ``` -->
<p>With the right version of Java running, we can install <code>SparkR</code> as usual with <code>install.packages("SparkR")</code>. After installing <code>SparkR</code>, the call <code>SparkR::install.spark()</code> will download and install Apache Spark to a local directory. No we can start an interactive Spark session from within R.</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="applied-econometrics-with-apache-spark.html#cb434-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;SparkR&quot;)</span></span>
<span id="cb434-2"><a href="applied-econometrics-with-apache-spark.html#cb434-2" aria-hidden="true" tabindex="-1"></a><span class="co"># or, if temporarily not available on CRAN:</span></span>
<span id="cb434-3"><a href="applied-econometrics-with-apache-spark.html#cb434-3" aria-hidden="true" tabindex="-1"></a><span class="co">#if (!require(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;)</span></span>
<span id="cb434-4"><a href="applied-econometrics-with-apache-spark.html#cb434-4" aria-hidden="true" tabindex="-1"></a><span class="co">#devtools::install_github(&#39;apache/spark@v2.x.x&#39;, subdir=&#39;R/pkg&#39;) # replace x.x with the version of your spark installation</span></span>
<span id="cb434-5"><a href="applied-econometrics-with-apache-spark.html#cb434-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-6"><a href="applied-econometrics-with-apache-spark.html#cb434-6" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb434-7"><a href="applied-econometrics-with-apache-spark.html#cb434-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SparkR)</span>
<span id="cb434-8"><a href="applied-econometrics-with-apache-spark.html#cb434-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-9"><a href="applied-econometrics-with-apache-spark.html#cb434-9" aria-hidden="true" tabindex="-1"></a><span class="co"># start session</span></span>
<span id="cb434-10"><a href="applied-econometrics-with-apache-spark.html#cb434-10" aria-hidden="true" tabindex="-1"></a><span class="fu">sparkR.session</span>()</span></code></pre></div>
<p>By default this starts a local standalone session (no connection to a cluster computer needed). While the examples below are all intended to run on a local machine, it is straightforward to connect to a remote Spark cluster and run the same examples there.<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a></p>
<div id="data-import-and-summary-statistics" class="section level3" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Data import and summary statistics</h3>
<p>First, we want to have a brief look at how to perform the first few steps of a typical econometric analysis: import data and compute summary statistics. We analyze the already familiar <code>flights.csv</code> dataset. The basic Spark installation provides direct support to import common data formats such as CSV and JSON via the <code>read.df()</code> function (for many additional formats, specific Spark libraries are available). To import<code>flights.csv</code>, we set the <code>source</code>-argument to <code>"csv"</code>.</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="applied-econometrics-with-apache-spark.html#cb435-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import data and create a SparkDataFrame (a distributed collection of data, RDD)</span></span>
<span id="cb435-2"><a href="applied-econometrics-with-apache-spark.html#cb435-2" aria-hidden="true" tabindex="-1"></a>flights <span class="ot">&lt;-</span> <span class="fu">read.df</span>(<span class="at">path=</span><span class="st">&quot;data/flights.csv&quot;</span>, <span class="at">source =</span> <span class="st">&quot;csv&quot;</span>, <span class="at">header=</span><span class="st">&quot;true&quot;</span>)</span>
<span id="cb435-3"><a href="applied-econometrics-with-apache-spark.html#cb435-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb435-4"><a href="applied-econometrics-with-apache-spark.html#cb435-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb435-5"><a href="applied-econometrics-with-apache-spark.html#cb435-5" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect the object</span></span>
<span id="cb435-6"><a href="applied-econometrics-with-apache-spark.html#cb435-6" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(flights)</span></code></pre></div>
<pre><code>## [1] &quot;SparkDataFrame&quot;
## attr(,&quot;package&quot;)
## [1] &quot;SparkR&quot;</code></pre>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="applied-econometrics-with-apache-spark.html#cb437-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(flights)</span></code></pre></div>
<pre><code>##   year month day dep_time sched_dep_time dep_delay
## 1 2013     1   1      517            515         2
## 2 2013     1   1      533            529         4
## 3 2013     1   1      542            540         2
## 4 2013     1   1      544            545        -1
## 5 2013     1   1      554            600        -6
## 6 2013     1   1      554            558        -4
##   arr_time sched_arr_time arr_delay carrier flight
## 1      830            819        11      UA   1545
## 2      850            830        20      UA   1714
## 3      923            850        33      AA   1141
## 4     1004           1022       -18      B6    725
## 5      812            837       -25      DL    461
## 6      740            728        12      UA   1696
##   tailnum origin dest air_time distance hour minute
## 1  N14228    EWR  IAH      227     1400    5     15
## 2  N24211    LGA  IAH      227     1416    5     29
## 3  N619AA    JFK  MIA      160     1089    5     40
## 4  N804JB    JFK  BQN      183     1576    5     45
## 5  N668DN    LGA  ATL      116      762    6      0
## 6  N39463    EWR  ORD      150      719    5     58
##              time_hour
## 1 2013-01-01T10:00:00Z
## 2 2013-01-01T10:00:00Z
## 3 2013-01-01T10:00:00Z
## 4 2013-01-01T10:00:00Z
## 5 2013-01-01T11:00:00Z
## 6 2013-01-01T10:00:00Z</code></pre>
<p>By default, all variables have been imported as type <code>character</code>. For several variables this is, of course, not the optimal data type to compute summary statistics. We thus first have to convert some columns to other data types with the <code>cast</code> function.</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="applied-econometrics-with-apache-spark.html#cb439-1" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>dep_delay <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>dep_delay, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb439-2"><a href="applied-econometrics-with-apache-spark.html#cb439-2" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>dep_time <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>dep_time, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb439-3"><a href="applied-econometrics-with-apache-spark.html#cb439-3" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>arr_time <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>arr_time, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb439-4"><a href="applied-econometrics-with-apache-spark.html#cb439-4" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>arr_delay <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>arr_delay, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb439-5"><a href="applied-econometrics-with-apache-spark.html#cb439-5" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>air_time <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>air_time, <span class="st">&quot;double&quot;</span>)</span>
<span id="cb439-6"><a href="applied-econometrics-with-apache-spark.html#cb439-6" aria-hidden="true" tabindex="-1"></a>flights<span class="sc">$</span>distance <span class="ot">&lt;-</span> <span class="fu">cast</span>(flights<span class="sc">$</span>distance, <span class="st">&quot;double&quot;</span>)</span></code></pre></div>
<p>Suppose we only want to compute average arrival delays per carrier for flights with a distance over 1000 miles. Variable selection and filtering of observations is implemented in <code>select()</code> and <code>filter()</code> (as in the <code>dplyr</code> package).</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="applied-econometrics-with-apache-spark.html#cb440-1" aria-hidden="true" tabindex="-1"></a><span class="co"># filter</span></span>
<span id="cb440-2"><a href="applied-econometrics-with-apache-spark.html#cb440-2" aria-hidden="true" tabindex="-1"></a>long_flights <span class="ot">&lt;-</span> <span class="fu">select</span>(flights, <span class="st">&quot;carrier&quot;</span>, <span class="st">&quot;year&quot;</span>, <span class="st">&quot;arr_delay&quot;</span>, <span class="st">&quot;distance&quot;</span>)</span>
<span id="cb440-3"><a href="applied-econometrics-with-apache-spark.html#cb440-3" aria-hidden="true" tabindex="-1"></a>long_flights <span class="ot">&lt;-</span> <span class="fu">filter</span>(long_flights, long_flights<span class="sc">$</span>distance <span class="sc">&gt;=</span> <span class="dv">1000</span>)</span>
<span id="cb440-4"><a href="applied-econometrics-with-apache-spark.html#cb440-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(long_flights)</span></code></pre></div>
<pre><code>##   carrier year arr_delay distance
## 1      UA 2013        11     1400
## 2      UA 2013        20     1416
## 3      AA 2013        33     1089
## 4      B6 2013       -18     1576
## 5      B6 2013        19     1065
## 6      B6 2013        -2     1028</code></pre>
<p>Now we summarize the arrival delays for the subset of long flights by carrier. This is the ‘split-apply-combine’ approach applied in <code>SparkR</code>.</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="applied-econometrics-with-apache-spark.html#cb442-1" aria-hidden="true" tabindex="-1"></a><span class="co"># aggregation: mean delay per carrier</span></span>
<span id="cb442-2"><a href="applied-econometrics-with-apache-spark.html#cb442-2" aria-hidden="true" tabindex="-1"></a>long_flights_delays<span class="ot">&lt;-</span> <span class="fu">summarize</span>(<span class="fu">groupBy</span>(long_flights, long_flights<span class="sc">$</span>carrier),</span>
<span id="cb442-3"><a href="applied-econometrics-with-apache-spark.html#cb442-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">avg_delay =</span> <span class="fu">mean</span>(long_flights<span class="sc">$</span>arr_delay))</span>
<span id="cb442-4"><a href="applied-econometrics-with-apache-spark.html#cb442-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(long_flights_delays)</span></code></pre></div>
<pre><code>##   carrier avg_delay
## 1      UA    3.2622
## 2      AA    0.4958
## 3      EV   15.6876
## 4      B6    9.0364
## 5      DL   -0.2394
## 6      OO   -2.0000</code></pre>
<p>Finally, we want to convert the result back into a usual <code>data.frame</code> (loaded in our current R session) in order to further process the summary statistics (output to LaTex table, plot, etc.). Note that as in the previous aggregation exercises with the <code>ff</code> package, the computed summary statistics (in the form of a table/df) are obviously much smaller than the raw data. However, note that converting a <code>SparkDataFrame</code> back into a native R object generally means all the data stored in the RDDs constituting the <code>SparkDataFrame</code> object are loaded into local RAM. Hence, when working with actual big data on a Spark cluster, this type of operation can quickly overflow local RAM.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="applied-econometrics-with-apache-spark.html#cb444-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert result back into native R object</span></span>
<span id="cb444-2"><a href="applied-econometrics-with-apache-spark.html#cb444-2" aria-hidden="true" tabindex="-1"></a>delays <span class="ot">&lt;-</span> <span class="fu">collect</span>(long_flights_delays)</span>
<span id="cb444-3"><a href="applied-econometrics-with-apache-spark.html#cb444-3" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(delays)</span></code></pre></div>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="applied-econometrics-with-apache-spark.html#cb446-1" aria-hidden="true" tabindex="-1"></a>delays</span></code></pre></div>
<pre><code>##    carrier avg_delay
## 1       UA    3.2622
## 2       AA    0.4958
## 3       EV   15.6876
## 4       B6    9.0364
## 5       DL   -0.2394
## 6       OO   -2.0000
## 7       F9   21.9207
## 8       US    0.5567
## 9       MQ    8.2331
## 10      HA   -6.9152
## 11      AS   -9.9309
## 12      VX    1.7645
## 13      WN    9.0842
## 14      9E    6.6730</code></pre>
</div>
<div id="regression-analysis-with-sparklyr" class="section level3" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Regression analysis with <code>sparklyr</code></h3>
<p>Suppose we want to conduct a correlation study of what factors are associated with more or less arrival delay. Spark provides via its built-in ‘MLib’ library several high-level functions to conduct regression analyses. When calling these functions via <code>sparklyr</code> (or <code>SparkR</code>), their usage is actually very similar to the usual R packages/functions commonly used to run regressions in R.</p>
<p>As a simple point of reference, we first estimate a linear model with the usual R approach (all computed in the R environment). First, we load the data as a common <code>data.table</code>. We could also convert a copy of the entire <code>SparkDataFrame</code> object to a <code>data.frame</code> or <code>data.table</code> and get essentially the same outcome. However, collecting the data from the RDD structure would take much longer than parsing the csv with <code>fread</code>. In addition, we only import the first 300 rows. Running regression analysis with relatively large datasets in Spark on a small local machine might fail or be rather slow.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a></p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="applied-econometrics-with-apache-spark.html#cb448-1" aria-hidden="true" tabindex="-1"></a><span class="co"># flights_r &lt;- collect(flights) # very slow!</span></span>
<span id="cb448-2"><a href="applied-econometrics-with-apache-spark.html#cb448-2" aria-hidden="true" tabindex="-1"></a>flights_r <span class="ot">&lt;-</span> data.table<span class="sc">::</span><span class="fu">fread</span>(<span class="st">&quot;data/flights.csv&quot;</span>, <span class="at">nrows =</span> <span class="dv">300</span>) </span></code></pre></div>
<p>Now we run a simple linear regression (OLS) and show the summary output.</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="applied-econometrics-with-apache-spark.html#cb449-1" aria-hidden="true" tabindex="-1"></a><span class="co"># specify the linear model</span></span>
<span id="cb449-2"><a href="applied-econometrics-with-apache-spark.html#cb449-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> arr_delay <span class="sc">~</span> dep_delay <span class="sc">+</span> distance</span>
<span id="cb449-3"><a href="applied-econometrics-with-apache-spark.html#cb449-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model with ols</span></span>
<span id="cb449-4"><a href="applied-econometrics-with-apache-spark.html#cb449-4" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(model1, flights_r)</span>
<span id="cb449-5"><a href="applied-econometrics-with-apache-spark.html#cb449-5" aria-hidden="true" tabindex="-1"></a><span class="co"># compute t-tests etc.</span></span>
<span id="cb449-6"><a href="applied-econometrics-with-apache-spark.html#cb449-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = model1, data = flights_r)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -42.39  -9.96  -1.91   9.87  48.02 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.182662   1.676560   -0.11     0.91    
## dep_delay    0.989553   0.017282   57.26   &lt;2e-16 ***
## distance     0.000114   0.001239    0.09     0.93    
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.5 on 297 degrees of freedom
## Multiple R-squared:  0.917,  Adjusted R-squared:  0.917 
## F-statistic: 1.65e+03 on 2 and 297 DF,  p-value: &lt;2e-16</code></pre>
<p>Now we aim to compute essentially the same model estimate in <code>sparklyr</code>.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> In order to use Spark via the <code>sparklyr</code> package we need to first load the package and establish a connection with Spark (similar to <code>SparkR::sparkR.session()</code>).</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="applied-econometrics-with-apache-spark.html#cb451-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sparklyr)</span>
<span id="cb451-2"><a href="applied-econometrics-with-apache-spark.html#cb451-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb451-3"><a href="applied-econometrics-with-apache-spark.html#cb451-3" aria-hidden="true" tabindex="-1"></a><span class="co"># connect with default configuration</span></span>
<span id="cb451-4"><a href="applied-econometrics-with-apache-spark.html#cb451-4" aria-hidden="true" tabindex="-1"></a>sc <span class="ot">&lt;-</span> <span class="fu">spark_connect</span>(<span class="at">master =</span> <span class="st">&quot;local&quot;</span>, </span>
<span id="cb451-5"><a href="applied-econometrics-with-apache-spark.html#cb451-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">version =</span> <span class="st">&quot;2.4.5&quot;</span>)</span></code></pre></div>
<p>We then copy the data.table <code>flights_r</code> (previously loaded into our R session) to Spark. Again, working on a normal laptop this seems trivial, but the exact same command would allow us (when connected with Spark on a cluster computer in the cloud) to properly load and distribute the data.table on the cluster. Finally, we then fit the model with <code>ml_linear_regression()</code> and compute</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="applied-econometrics-with-apache-spark.html#cb452-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data to spark</span></span>
<span id="cb452-2"><a href="applied-econometrics-with-apache-spark.html#cb452-2" aria-hidden="true" tabindex="-1"></a>flights3 <span class="ot">&lt;-</span> <span class="fu">copy_to</span>(sc, flights_r, <span class="st">&quot;flights3&quot;</span>)</span>
<span id="cb452-3"><a href="applied-econometrics-with-apache-spark.html#cb452-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb452-4"><a href="applied-econometrics-with-apache-spark.html#cb452-4" aria-hidden="true" tabindex="-1"></a>fit1_spark <span class="ot">&lt;-</span> <span class="fu">ml_linear_regression</span>(flights3, <span class="at">formula =</span> model1)</span>
<span id="cb452-5"><a href="applied-econometrics-with-apache-spark.html#cb452-5" aria-hidden="true" tabindex="-1"></a><span class="co"># compute summary stats</span></span>
<span id="cb452-6"><a href="applied-econometrics-with-apache-spark.html#cb452-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1_spark)</span></code></pre></div>
<pre><code>## Deviance Residuals:
##    Min     1Q Median     3Q    Max 
## -42.39  -9.96  -1.91   9.87  48.02 
## 
## Coefficients:
## (Intercept)   dep_delay    distance 
##   -0.182662    0.989553    0.000114 
## 
## R-Squared: 0.9172
## Root Mean Squared Error: 15.42</code></pre>
<p>Alternatively, we can do essentially the same with the <code>SparkR</code> package:</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="applied-econometrics-with-apache-spark.html#cb454-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create SparkDataFrame</span></span>
<span id="cb454-2"><a href="applied-econometrics-with-apache-spark.html#cb454-2" aria-hidden="true" tabindex="-1"></a>flights3 <span class="ot">&lt;-</span> <span class="fu">createDataFrame</span>(flights_r)</span>
<span id="cb454-3"><a href="applied-econometrics-with-apache-spark.html#cb454-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb454-4"><a href="applied-econometrics-with-apache-spark.html#cb454-4" aria-hidden="true" tabindex="-1"></a>fit2_spark <span class="ot">&lt;-</span> <span class="fu">spark.glm</span>( <span class="at">formula =</span> model1, <span class="at">data =</span> flights3 , <span class="at">family=</span><span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb454-5"><a href="applied-econometrics-with-apache-spark.html#cb454-5" aria-hidden="true" tabindex="-1"></a><span class="co"># compute t-tests etc.</span></span>
<span id="cb454-6"><a href="applied-econometrics-with-apache-spark.html#cb454-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2_spark)</span></code></pre></div>

</div>
</div>
</div>



<div class="footnotes">
<hr />
<ol start="33">
<li id="fn33"><p>See <a href="https://cosminsanda.com/posts/a-compelling-case-for-sparkr/">this blog post</a> for a more detailed comparison and discussion of advantages of either package.<a href="applied-econometrics-with-apache-spark.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>Simply set the <code>master</code> argument of <code>sparkR.session()</code> to the URL of the Spark master node of the remote cluster. Importantly, the local Spark and Hadoop versions should match the corresponding versions on the remote cluster.<a href="applied-econometrics-with-apache-spark.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>Again, it is important to keep in mind that running Spark on a small local machine is only optimal for learning and testing code (based on relatively small samples). The whole framework is not optimized to be run on a small machine but for cluster computers.<a href="applied-econometrics-with-apache-spark.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>Most regression models commonly used in traditional applied econometrics are in some form provided in <code>sparklyr</code> or <code>SparkR</code>. See the package documentations for more details.<a href="applied-econometrics-with-apache-spark.html#fnref36" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bottle-necks-in-local-big-data-analytics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix-a.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bigdata.pdf", "bigdata.html", "bigdata.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
