# Descriptive Statistics and Aggregation


## Data aggregation: The 'split-apply-combine' strategy
\index{Split-Apply-Combine}

The 'split-apply-combine'\index{Split-Apply-Combine} strategy plays an important role in many data analysis tasks, ranging from data preparation to summary statistics and model-fitting.^[Moreover, 'split-apply-combine' is closely related to a core strategy of Big Data Analytics with distributed systems\index{Distributed System} (MapReduce).] The strategy can be defined as "break up a problem into manageable pieces, operate on each piece independently, and then put all the pieces back together." [@wickham_2011, p. 1]

Many R users are familiar with the basic concept of split-apply-combine implemented in the `plyr` package\index{plyr Package} intended for normal in-memory operations (dataset fits into RAM). Here, we explore the options for split-apply-combine approaches to large datasets that do not fit into RAM. 



## Data aggregation with chunked data files
\index{ff Package}
In this tutorial we explore the world of New York's famous Yellow Cabs. In a first step, we will focus on the `ff`-based approach to employ parts of the hard disk as 'virtual memory'. This means, that all of the examples are easily scalable without risking too much memory pressure. Given the size of the entire TLC database (over 200GB), we will only use one million taxi trip records.^[Note that the code examples below could also be run based on the entire TLC database (provided that there is enough hard-disk space available). But, creating the `ff` chunked file structure for a 200GB CSV\index{CSV (Comma Separated Values)} would take hours or even days.]



**Data import**

First, we read the raw taxi trip records into R with the `ff` package\index{ff Package}.

```{r message=FALSE}
# load packages
library(ff)
library(ffbase)

# set up the ff directory (for data file chunks)
if (!dir.exists("fftaxi")){
     system("mkdir fftaxi")
}
options(fftempdir = "fftaxi")

# import the first one million observations
taxi <- read.table.ffdf(file = "data/tlc_trips.csv",
                        sep = ",",
                        header = TRUE,
                        next.rows = 100000,
                        # colClasses= col_classes,
                        nrows = 1000000
                        )

```

Following the data documentation provided by TLC, we give the columns of our dataset more meaningful names and remove the empty columns (some covariates are only collected in later years).

When inspecting the factor variables of the dataset, we notice that some of the values are not standardized/normalized, and the resulting factor levels are, therefore, somewhat ambiguous. We should clean this before getting into data aggregation tasks. Note the `ff`-specific syntax needed to recode the factor.

```{r}
# inspect the factor levels
levels(taxi$Payment_Type)
# recode them
levels(taxi$Payment_Type) <- tolower(levels(taxi$Payment_Type))
taxi$Payment_Type <- ff(taxi$Payment_Type,
                        levels = unique(levels(taxi$Payment_Type)),
                        ramclass = "factor")
# check result
levels(taxi$Payment_Type)

```





**Aggregation with split-apply-combine**

First, we will have a look at whether trips paid with credit card tend to involve lower tip amounts than trips paid in cash. In order to do so, we create a table that shows the average amount of tip paid for each payment-type category. 

In simple words, this means we first split the dataset into subsets, each of which contains all observations belonging to a distinct payment type. Then, we compute the arithmetic mean of the tip-column of each of these subsets. Finally, we combine all of these results into one table (i.e., the split-apply-combine strategy). When working with `ff`, the `ffdfply()`\index{ffdply()} function in combination with the `doBy` package\index{doBy Package} [@doBy] provides a user-friendly implementation of split-apply-combine\index{Split-Apply-Combine} types of tasks. 

```{r ffdfdply, linewidth=40, message=FALSE, warning=FALSE}

# load packages
library(doBy)

# split-apply-combine procedure on data file chunks
tip_pcategory <- ffdfdply(taxi,
                          split = taxi$Payment_Type,
                          BATCHBYTES = 100000000,
                          FUN = function(x) {
                               summaryBy(Tip_Amt~Payment_Type,
                                         data = x,
                                         FUN = mean,
                                         na.rm = TRUE)})
```

Note how the output describes the procedure step by step. Now we can have a look at the resulting summary statistic in the form of a `data.frame`.

```{r}
as.data.frame(tip_pcategory)
```

The result contradicts our initial hypothesis. However, the comparison is a little flawed. If trips paid by credit card also tend to be longer, the result is not too surprising. We should thus look at the share of tip (or percentage), given the overall amount paid for the trip.

We add an additional variable `percent_tip` and then repeat the aggregation exercise for this variable.

```{r ffdfdply2, linewidth=40, message=FALSE, warning=FALSE}
# add additional column with the share of tip
taxi$percent_tip <- (taxi$Tip_Amt/taxi$Total_Amt)*100

# recompute the aggregate stats
tip_pcategory <- ffdfdply(taxi,
                          split = taxi$Payment_Type,
                          BATCHBYTES = 100000000,
                          FUN = function(x) {
                             # note the difference here
                               summaryBy(percent_tip~Payment_Type, 
                                         data = x,
                                         FUN = mean,
                                         na.rm = TRUE)})
# show result as data frame
as.data.frame(tip_pcategory)
```


**Cross-tabulation of `ff` vectors**
\index{Cross-Tabulation}

Also in relative terms, trips paid by credit card tend to be tipped more. However, are there actually many trips paid by credit card? In order to figure this out, we count the number of trips per payment type by applying the `table.ff`\index{table.ff()} function provided in `ffbase`\index{ffbase Package}. 

```{r}
table.ff(taxi$Payment_Type)
```

So trips paid in cash are much more frequent than trips paid by credit card. Again using the `table.ff`\index{table.ff()} function, we investigate what factors might be correlated with payment types. First, we have a look at whether payment type is associated with the number of passengers in a trip.

```{r}
# select the subset of observations only containing trips paid by
# credit card or cash
taxi_sub <- subset.ffdf(taxi, Payment_Type=="credit" | Payment_Type == "cash")
taxi_sub$Payment_Type <- ff(taxi_sub$Payment_Type,
                        levels = c("credit", "cash"),
                        ramclass = "factor")

# compute the cross tabulation
crosstab <- table.ff(taxi_sub$Passenger_Count,
                     taxi_sub$Payment_Type
                     )
# add names to the margins
names(dimnames(crosstab)) <- c("Passenger count", "Payment type")
# show result
crosstab
```

From the raw numbers it is hard to see whether there are significant differences between the categories cash and credit. We therefore use a visualization technique called a 'mosaic plot' (provided in the `vcd` package\index{vcd Package}; see @vcd, @strucplot, and @shading) to visualize the cross-tabulation.

```{r warning=FALSE, message=FALSE, out.width="75%", fig.align='center'}
# install.packages(vcd)
# load package for mosaic plot
library(vcd)

# generate a mosaic plot
mosaic(crosstab, shade = TRUE)
```

The plot suggests that trips involving more than one passenger tend to be paid by cash rather than by credit card.




## High-speed in-memory data aggregation with `arrow`
\index{arrow Package}

For large datasets that (at least in part) fit into RAM, the `arrow` package\index{arrow Package} again provides an attractive alternative to `ff`.

**Data import**

We use the already familiar `read_csv_arrow()` to import the same first million observations from the taxi trips records.

```{r warning=FALSE, message=FALSE}
# load packages
library(arrow)
library(dplyr)

# read the CSV file 
taxi <- read_csv_arrow("data/tlc_trips.csv", 
                       as_data_frame = FALSE)

```

__Data preparation and 'split-apply-combine'__

We prepare/clean the data as in the `ff`-approach above. 

As `arrow`\index{arrow Package} builds on a `dplyr`\index{dplyr Package} back-end, basic computations can be easily done through the common `dplyr` syntax. Note, however, that not all of the `dplyr` functions are covered in `arrow`\index{arrow Package} (as of the writing of this book).^[If a `dplyr`-like function is not implemented in `arrow`, the `arrow`\index{arrow Package} data object is automatically pulled into R (meaning fully into RAM) and then processed there directly via native `dplyr`\index{dplyr Package}. Such a situation might crash your R session due to a lack of RAM\index{Random Access Memory (RAM)}.]


```{r}

# clean the categorical variable; aggregate by group
taxi <- 
   taxi %>% 
   mutate(Payment_Type = tolower(Payment_Type))
```


```{r}
taxi_summary <- 
   taxi %>%
   mutate(percent_tip = (Tip_Amt/Total_Amt)*100 ) %>% 
   group_by(Payment_Type) %>% 
   summarize(avg_percent_tip = mean(percent_tip)) %>% 
   collect() 
```


Similarly, we can use `data.table`'s `dcast()` for cross-tabulation-like operations.

```{r warning=FALSE, message=FALSE}
library(tidyr)

# compute the frequencies; pull result into R
ct <- taxi %>%
   filter(Payment_Type %in% c("credit", "cash")) %>%
   group_by(Passenger_Count, Payment_Type) %>%
   summarize(n=n())%>%
     collect()

# present as cross-tabulation
pivot_wider(data=ct, 
            names_from="Passenger_Count",
            values_from = "n")

```






## High-speed in-memory data aggregation with `data.table`

\index{data.table Package}

For large datasets that still fit into RAM, the `data.table` package\index{data.table Package} [@data.table] provides very fast and elegant functions to compute aggregate statistics.

__Data import__

We use the already familiar `fread()`\index{fread()} to import the same first million observations from the taxi trip records.

```{r warning=FALSE, message=FALSE}
# load packages
library(data.table)

# import data into RAM (needs around 200MB)
taxi <- fread("data/tlc_trips.csv",
              nrows = 1000000)

```

__Data preparation and `data.table` syntax for 'split-apply-combine'__

We prepare/clean the data as in the `ff` approach above.

```{r}
# clean the factor levels
taxi$Payment_Type <- tolower(taxi$Payment_Type)
taxi$Payment_Type <- factor(taxi$Payment_Type,
                            levels = unique(taxi$Payment_Type))     

```

Note the simpler syntax of essentially doing the same thing, but all in-memory.

__`data.table`-syntax for 'split-apply-combine' operations__

With the `[]`-syntax we index/subset the usual `data.frame` objects in R. When working with `data.table`s, much more can be done in the step of 'sub-setting' the frame.^[See https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html for a detailed introduction to the syntax.]

For example, we can directly compute on columns.

```{r}
taxi[, mean(Tip_Amt/Total_Amt)]
```

Moreover, in the same step, we can 'split' the rows *by* specific groups and apply the function to each subset.

```{r}
taxi[, .(percent_tip = mean((Tip_Amt/Total_Amt)*100)), by = Payment_Type]
```

Similarly, we can use `data.table`'s `dcast()`\index{dcast()} for cross-tabulation-like operations.

```{r}
dcast(taxi[Payment_Type %in% c("credit", "cash")],
      Passenger_Count~Payment_Type, 
      fun.aggregate = length,
      value.var = "vendor_name")
```




```{r echo = FALSE, message=FALSE, warning=FALSE }
# housekeeping
#gc()
system("rm -r fftaxi")
```



## Wrapping up

 - Similar to the MapReduce idea in the context of distributed systems, the *split-apply-combine*\index{Split-Apply-Combine} approach is key in many Big Data aggregation procedures on normal machines (laptop/desktop computers). The idea is to split the overall data into subsets based on a categorical variable, apply a function (e.g., mean) on each subset, and then combine the results into one object. Thus, the approach allows for parallelization\index{Parallelization} and working on separate data chunks.
 - As computing descriptive statistics on various subsets of a large dataset can be very memory-intensive, it is recommended to use out-of-memory strategies\index{Out-Of-Memory Strategy}, lazy evaluation\index{Lazy Evaluation}, or a classical SQL-database approach for this. 
 - There are several options available such as `ffdply`\index{ffdply()}, running on chunked datasets; and `arrow` with `group_by()`\index{group\_by()}.
 

